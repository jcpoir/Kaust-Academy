{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd7a5d4",
   "metadata": {},
   "source": [
    "## ðŸª Kaust Academy: Classifying Math Probems\n",
    "Goal: to use my NLP background to produce a top solution by f1 for kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0f8a6",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b79b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "!python3 -m spacy download en_core_web_lg\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter as count\n",
    "\n",
    "from sklearn.metrics import f1_score as f1, accuracy_score as accuracy\n",
    "import pickle as pkl\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "!pip install lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46c8c6",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e239bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run(command, shell = False):\n",
    "  ''' run the specified command via subprocess.run '''\n",
    "  if not shell: command = command.split(\" \")\n",
    "  process = subprocess.Popen(command, shell = shell) \n",
    "  process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc6a27",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv(\"train-data/train.csv\")\n",
    "df_te = pd.read_csv(\"train-data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56edb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc8d1d7",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41bf45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "max_n_gram = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72de7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bad_pos = set(\n",
    "    [\"PUNCT\", \"SPACE\"]\n",
    ")\n",
    "\n",
    "replacements = {\n",
    "    \"$\" : \"\",\n",
    "    \"+\" : \" + \",\n",
    "    \"-\" : \" - \",\n",
    "    \"*\" : \" * \",\n",
    "    \"=\" : \" = \",\n",
    "    \"^\" : \" ^ \",\n",
    "    \"\\cos\" : \" \\cos \",\n",
    "    \"\\sin\" : \" \\sin \",\n",
    "    r\"\\tan\" : r\" \\tan \",\n",
    "    \"\\sqrt\" : \" \\sqrt \",\n",
    "    \"{\" : \" { \",\n",
    "    \"}\" : \" } \",\n",
    "    \"\\\\\" : \" \",\n",
    "    \")\" : \" \",\n",
    "    \"(\" : \" \"\n",
    "}\n",
    "\n",
    "regex_replacements = {\n",
    "    \"[\\d.,]+\" : \" NUMX \"\n",
    "}\n",
    "\n",
    "def preprocess(df_tr, chunk_size = 20):\n",
    "    ''' encodes the sentences in df_tr as bag-of-words tf-idf features '''\n",
    "\n",
    "    X = pd.DataFrame()\n",
    "    _len = len(df_tr)\n",
    "\n",
    "    for i,(txt,label) in tqdm(enumerate(zip(df_tr.Question, df_tr.label))):\n",
    "\n",
    "        for k,v in replacements.items():\n",
    "            txt = txt.replace(k,v)\n",
    "        for k,v in regex_replacements.items():\n",
    "            txt = re.sub(k,v,txt)\n",
    "\n",
    "        doc = nlp(txt)\n",
    "\n",
    "        toks = []\n",
    "        for tok in doc:\n",
    "\n",
    "            is_stop = tok.is_stop\n",
    "            is_bad_pos = tok.pos_ in bad_pos\n",
    "\n",
    "            if is_bad_pos: continue\n",
    "\n",
    "            toks.append(tok.lemma_)\n",
    "\n",
    "        def get_n_grams(toks, n, sep = \" \"):\n",
    "            ''' for a list of one-gram tokens, returns all n-grams '''\n",
    "\n",
    "            out = []\n",
    "            _len = len(toks)\n",
    "            for i in range(_len - n - 1):\n",
    "                out.append(sep.join(toks[i:i+n]))\n",
    "\n",
    "            return out\n",
    "\n",
    "        n_grams = toks[:]\n",
    "        for n in range(2, max_n_gram+1):\n",
    "            n_grams += get_n_grams(toks, n=n)\n",
    "\n",
    "        term_freq = count(n_grams)\n",
    "\n",
    "        # append to the dataframe\n",
    "        row = pd.DataFrame()\n",
    "        row[\"label\"] = [label]\n",
    "        for k in term_freq:\n",
    "            row[k] = [term_freq[k]]\n",
    "\n",
    "        X = pd.concat((row,X))\n",
    "        \n",
    "        # output chunks to improve runtime efficiency\n",
    "        i += 1\n",
    "        if i % chunk_size == 0:\n",
    "            X.to_csv(f\"temp-data/{i}.csv\", index = False)\n",
    "            X = pd.DataFrame()\n",
    "\n",
    "    # read and merge chunks\n",
    "    for i in tqdm(range(chunk_size,_len+1,chunk_size)):\n",
    "        df_sub = pd.read_csv(f\"temp-data/{i}.csv\")\n",
    "        X = pd.concat((X, df_sub))\n",
    "\n",
    "    # erase all chunks\n",
    "    run(\"rm -r temp-data\")\n",
    "    run(\"mkdir temp-data\")\n",
    "    \n",
    "    X.fillna(0, inplace = True)\n",
    "\n",
    "    y, X = X.label, X.drop(\"label\", axis = 1)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "process_data = False\n",
    "if process_data:\n",
    "    chunk_size = 500\n",
    "    X, y = pd.DataFrame(), pd.DataFrame()\n",
    "    for start_idx in range(0, 10000, chunk_size):\n",
    "        end_idx = start_idx + chunk_size\n",
    "        print(f\"idx: {start_idx}-{end_idx}\")\n",
    "        X_sub, y_sub = preprocess(df_tr.iloc[start_idx: end_idx])\n",
    "        X, y = pd.concat((X, X_sub)), pd.concat((y, y_sub))\n",
    "\n",
    "    X.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e68971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data as a sparse matrix\n",
    "max_n_gram = 3\n",
    "\n",
    "save_data = False\n",
    "if save_data:   \n",
    "    from scipy.sparse import csr_matrix\n",
    "    X_sparse  = csr_matrix(X.values)\n",
    "    with open(f\"processed-data/X{max_n_gram}.pkl\", \"wb\") as f:\n",
    "        pkl.dump(X_sparse, f)\n",
    "    with open(f\"processed-data/cols{max_n_gram}.pkl\", \"wb\") as f:\n",
    "        pkl.dump(X.columns, f)\n",
    "\n",
    "load_data = True\n",
    "if load_data:\n",
    "    with open(f\"processed-data/X{max_n_gram}.pkl\", \"rb\") as f:\n",
    "        X_sparse = pkl.load(f)\n",
    "    with open(f\"processed-data/cols{max_n_gram}.pkl\", \"rb\") as f:\n",
    "        columns = pkl.load(f)\n",
    "\n",
    "    X = pd.DataFrame.sparse.from_spmatrix(X_sparse)\n",
    "    X.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb9fb106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 277113)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4469a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minibatch = 10000\n",
    "X_sub, y_sub = X.iloc[:minibatch], df_tr.label.iloc[:minibatch]\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "_len = len(X_sub)\n",
    "n_folds = 5\n",
    "partition_size = _len // n_folds\n",
    "\n",
    "ovr_metrics = {}\n",
    "metrics = [\"acc\", \"f1\"]\n",
    "for metric in metrics:\n",
    "    ovr_metrics[metric] = 0\n",
    "\n",
    "for start_idx in range(0, _len, partition_size):\n",
    "\n",
    "    # partition data into train, validation sets\n",
    "    end_idx = start_idx + partition_size\n",
    "    end_idx = min((end_idx,_len))\n",
    "    X_v, y_v = X_sub.iloc[start_idx:end_idx], y_sub.iloc[start_idx:end_idx]\n",
    "    X_t = pd.concat((X_sub.iloc[:start_idx], X_sub.iloc[end_idx:]))\n",
    "    y_t = pd.concat((y_sub.iloc[:start_idx], y_sub.iloc[end_idx:]))\n",
    "\n",
    "    # train the model and render predictions\n",
    "    model.fit(X_t, y_t)\n",
    "    y_pred_t = model.predict(X_t)\n",
    "    y_pred = model.predict(X_v)\n",
    "\n",
    "    # score the model\n",
    "    score = f1(y_v, y_pred, average = \"micro\")\n",
    "    acc = accuracy(y_v, y_pred)\n",
    "\n",
    "    score_t = f1(y_t, y_pred_t, average = \"micro\")\n",
    "\n",
    "    ovr_metrics[\"f1\"] = score\n",
    "    ovr_metrics[\"acc\"] = acc\n",
    "    ovr_metrics[\"train_f1\"] = score_t\n",
    "\n",
    "    break\n",
    "\n",
    "# save the model with metrics\n",
    "model.fit(X_sub,y_sub)\n",
    "acc, score = round(ovr_metrics[\"acc\"],3), round(ovr_metrics[\"f1\"],3)\n",
    "\n",
    "# identify model architecture via class membership\n",
    "arch = \"other\"\n",
    "architectures = {\n",
    "    \"lr\" : LogisticRegression,\n",
    "    \"ridge\" : RidgeClassifier,\n",
    "    \"lgb\" : LGBMClassifier\n",
    "}\n",
    "for k,v in architectures.items():\n",
    "    if isinstance(model, v):\n",
    "        arch = k\n",
    "\n",
    "with open(f\"models/{arch}-{score}.pkl\", \"wb\") as f:\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(ovr_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef92843b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((end_idx,_len))\n\u001b[1;32m     22\u001b[0m X_v, y_v \u001b[38;5;241m=\u001b[39m X_sub\u001b[38;5;241m.\u001b[39miloc[start_idx:end_idx], y_sub\u001b[38;5;241m.\u001b[39miloc[start_idx:end_idx]\n\u001b[0;32m---> 23\u001b[0m X_t \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m y_t \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat((y_sub\u001b[38;5;241m.\u001b[39miloc[:start_idx], y_sub\u001b[38;5;241m.\u001b[39miloc[end_idx:]))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# train the model and render predictions\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/concat.py:189\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/concat.py:466\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, copy)\u001b[0m\n\u001b[1;32m    463\u001b[0m has_none_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(unit\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m join_units)\n\u001b[1;32m    464\u001b[0m upcasted_na \u001b[38;5;241m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[0;32m--> 466\u001b[0m to_concat \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mju\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reindexed_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempty_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupcasted_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcasted_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mju\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjoin_units\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat):\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special case not needed if all EAs used HybridBlocks\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# error: No overload variant of \"__getitem__\" of \"ExtensionArray\" matches\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# argument type \"Tuple[int, slice]\"\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    477\u001b[0m         t\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m t[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat\n\u001b[1;32m    481\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/concat.py:467\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    463\u001b[0m has_none_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(unit\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m join_units)\n\u001b[1;32m    464\u001b[0m upcasted_na \u001b[38;5;241m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[1;32m    466\u001b[0m to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 467\u001b[0m     \u001b[43mju\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reindexed_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempty_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupcasted_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupcasted_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units\n\u001b[1;32m    469\u001b[0m ]\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat):\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special case not needed if all EAs used HybridBlocks\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# error: No overload variant of \"__getitem__\" of \"ExtensionArray\" matches\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# argument type \"Tuple[int, slice]\"\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    477\u001b[0m         t\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(t\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m t[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m to_concat\n\u001b[1;32m    481\u001b[0m     ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/concat.py:440\u001b[0m, in \u001b[0;36mJoinUnit.get_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m upcasted_na\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_valid_na_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mempty_dtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;66;03m# note: always holds when self.block.dtype.kind == \"V\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m         blk_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m blk_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;66;03m# we want to avoid filling with np.nan if we are\u001b[39;00m\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# using None; we already know that we are all\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# nulls\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/concat.py:362\u001b[0m, in \u001b[0;36mJoinUnit._is_valid_na_for\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_valid_na_for\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: DtypeObj) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    Check that we are all-NA of a type/dtype that is compatible with this dtype.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    Augments `self.is_na` with an additional check of the type of NA values.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_na\u001b[49m:\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     blk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock\n",
      "File \u001b[0;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/concat.py:415\u001b[0m, in \u001b[0;36mJoinUnit.is_na\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     val \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isna(val):\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;66;03m# ideally isna_all would do this short-circuiting\u001b[39;00m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(isna_all(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "minibatch = 100\n",
    "X_sub, y_sub = X, df_tr.label[:10000]\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "n_folds = 5\n",
    "_len = len(X_sub)\n",
    "partition_size = _len // n_folds\n",
    "\n",
    "ovr_metrics = {}\n",
    "metrics = [\"acc\", \"f1\"]\n",
    "for metric in metrics:\n",
    "    ovr_metrics[metric] = 0\n",
    "\n",
    "for start_idx in range(0, _len, partition_size):\n",
    "\n",
    "    # partition data into train, validation sets\n",
    "    end_idx = start_idx + partition_size\n",
    "    end_idx = min((end_idx,_len))\n",
    "    X_v, y_v = X_sub.iloc[start_idx:end_idx], y_sub.iloc[start_idx:end_idx]\n",
    "    X_t = pd.concat((X_sub.iloc[:start_idx], X_sub.iloc[end_idx:]))\n",
    "    y_t = pd.concat((y_sub.iloc[:start_idx], y_sub.iloc[end_idx:]))\n",
    "\n",
    "    # train the model and render predictions\n",
    "    model.fit(X_t, y_t)\n",
    "    y_pred = model.predict(X_v)\n",
    "\n",
    "    # score the model\n",
    "    score = f1(y_v, y_pred, average = \"micro\")\n",
    "    acc = accuracy(y_v, y_pred)\n",
    "\n",
    "    print(score)\n",
    "    \n",
    "    ovr_metrics[\"f1\"] += score / n_folds\n",
    "    ovr_metrics[\"acc\"] += acc / n_folds\n",
    "\n",
    "# save the model with metrics\n",
    "model.fit(X_sub,y_sub)\n",
    "acc, score = round(ovr_metrics[\"acc\"],3), round(ovr_metrics[\"f1\"],3)\n",
    "\n",
    "# identify model architecture via class membership\n",
    "arch = \"other\"\n",
    "architectures = {\n",
    "    \"lr\" : LogisticRegression,\n",
    "    \"ridge\" : RidgeClassifier\n",
    "}\n",
    "for k,v in architectures.items():\n",
    "    if isinstance(model, v):\n",
    "        arch = k\n",
    "\n",
    "with open(f\"models/{arch}-f1={score}+acc={acc}\", \"wb\") as f:\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(ovr_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
