{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "860280ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hf-token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "\n",
    "LF = \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "65c1ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(torch.mps.is_available())\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ab890ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lschlessinger/bert-finetuned-math-prob-classification\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Load in float16 to fit memory\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,   # Important for Mac memory fitting\n",
    "    device_map=\"mps:0\",\n",
    "    trust_remote_code=True,\n",
    "    offload_folder = \"offload\",\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# customnize model for this task\n",
    "model.classifier = nn.Linear(768, 8, bias = True, device = \"mps:0\", dtype = torch.float16) # device = mps\n",
    "\n",
    "# # freeze all but the final layer of weights\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18f6263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(outs):\n",
    "\n",
    "    cls_ref = {\n",
    "        0 : \"Algebra\",\n",
    "        1 : \"Counting & Probability\",\n",
    "        2 : \"Geometry\",\n",
    "        3 : \"Intermediate Algebra\",\n",
    "        4 : \"Number Theory\",\n",
    "        5 : \"Prealgebra\",\n",
    "        6 : \"Precalculus\"\n",
    "    }\n",
    "\n",
    "    outs = res[\"logits\"].squeeze(0)\n",
    "    cls_idx = outs.argmax(axis = 0)\n",
    "    cls_conf = outs.max()\n",
    "    cls = cls_ref[int(cls_idx)]\n",
    "\n",
    "    print(f\"cls: {cls}, conf: {cls_conf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f83f4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data load, train-test split\n",
    "\n",
    "df = pd.read_csv(\"train-data/train.csv\")\n",
    "df_te = pd.read_csv(\"train-data/test.csv\").drop(\"id\", axis = 1)\n",
    "train_size = 2073\n",
    "df_tr, df_val = df.iloc[:train_size], df.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39c37974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, df_val, batch_size = 30, subset = 1000):\n",
    "    ''' tests the model on a given test dataset '''\n",
    "\n",
    "    print(\"Validating...\")\n",
    "\n",
    "    torch.mps.empty_cache()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        y = df_val.label.to_numpy()\n",
    "        avg_list = []\n",
    "\n",
    "        pbar = tqdm(range(0, len(df_val), batch_size))\n",
    "\n",
    "        for start_idx in pbar: \n",
    "\n",
    "            if start_idx >= subset: break\n",
    "\n",
    "            end_idx = min((start_idx + batch_size, len(df_val)))\n",
    "            df = df_val.iloc[start_idx:end_idx]\n",
    "            questions = df.Question.tolist()\n",
    "\n",
    "            tokens = tokenizer(questions, padding = True, truncation = True, return_tensors = \"pt\") # .to(\"mps:0\")\n",
    "            res = np.array(model(**tokens).logits.argmax(axis = 1).cpu())\n",
    "\n",
    "            sub_avg = (res == y[start_idx:end_idx]).mean()\n",
    "            avg_list.append(sub_avg)\n",
    "\n",
    "            pbar.set_description(f\"score: {round(np.mean(avg_list),3)}\")\n",
    "        \n",
    "# test_model(model, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ed8ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_to_dummies(y):\n",
    "    ''' converts answers in the form [1,8,3,2,3...] to \n",
    "    [[0,1,0,...0],[0,0,...,0,1]] (one-hot encoded) '''\n",
    "\n",
    "    out = torch.zeros((len(y),8), device = \"mps:0\", dtype = torch.float16)\n",
    "\n",
    "    # for i,yi in enumerate(y):\n",
    "    #     out[i,yi] = 1\n",
    "\n",
    "    return torch.tensor(y, device = \"mps:0\", dtype = torch.float16) # device = \"mps:0\"\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "def train_model(model, df_tr, optimizer, loss_fn):\n",
    "    ''' performs one training step on model with the specified input data '''\n",
    "    global y_pred, y\n",
    "\n",
    "    # torch.mps.empty_cache()\n",
    "\n",
    "    ## reformat input data\n",
    "    questions, answers = df_tr.Question.tolist(), df_tr.label.to_numpy()\n",
    "    y = y_to_dummies(answers)\n",
    "\n",
    "    ## render predictions\n",
    "    tokens = tokenizer(questions, padding = True, truncation = True, return_tensors = \"pt\").to(\"mps:0\")\n",
    "    tokens[\"input_ids\"]\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(**tokens).logits\n",
    "    y_pred = softmax(y_pred)\n",
    "\n",
    "    y, y_pred = y, y_pred\n",
    "\n",
    "    ## loss & backprop & optim\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # generate training metrics\n",
    "    pred_labels = np.array(y_pred.argmax(axis = 1).cpu())\n",
    "    avg = (pred_labels == answers).mean()\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86f715f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== epoch 0 ==\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/208 [00:00<?, ?it/s]/var/folders/v2/b9ylmqgs5nx8s6khtkwdyk2w0000gn/T/ipykernel_1591/3223579398.py:39: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  pred_labels = np.array(y_pred.argmax(axis = 1).cpu())\n",
      "train acc: 0.1:   0%|          | 1/208 [00:00<01:28,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc: 0.2:   1%|          | 2/208 [00:00<00:56,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc: 0.225:   2%|▏         | 4/208 [00:01<00:52,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc: 0.217:   3%|▎         | 6/208 [00:01<00:43,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc: 0.238:   4%|▍         | 8/208 [00:01<00:40,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc: 0.23:   5%|▍         | 10/208 [00:02<00:41,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc: 0.217:   6%|▌         | 12/208 [00:03<00:48,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n",
      "tensor(nan, device='mps:0', dtype=torch.float16, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train acc: 0.208:   6%|▋         | 13/208 [00:03<00:53,  3.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m end_idx = \u001b[38;5;28mmin\u001b[39m((start_idx + batch_size), \u001b[38;5;28mlen\u001b[39m(df_tr))\n\u001b[32m     17\u001b[39m df_tr_sub = df_tr.iloc[start_idx:end_idx]\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_tr_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m accs.append(acc)\n\u001b[32m     21\u001b[39m pbar.set_description(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(np.mean(accs),\u001b[32m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, df_tr, optimizer, loss_fn)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m## loss & backprop & optim\u001b[39;00m\n\u001b[32m     33\u001b[39m loss = loss_fn(y_pred, y)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[32m     35\u001b[39m loss.backward()\n\u001b[32m     36\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/_tensor.py:590\u001b[39m, in \u001b[36mTensor.__repr__\u001b[39m\u001b[34m(self, tensor_contents)\u001b[39m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    587\u001b[39m         Tensor.\u001b[34m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents=tensor_contents\n\u001b[32m    588\u001b[39m     )\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tensor_str\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/_tensor_str.py:710\u001b[39m, in \u001b[36m_str\u001b[39m\u001b[34m(self, tensor_contents)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(), torch.utils._python_dispatch._disable_current_modes():\n\u001b[32m    709\u001b[39m     guard = torch._C._DisableFuncTorch()  \u001b[38;5;66;03m# noqa: F841\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/_tensor_str.py:631\u001b[39m, in \u001b[36m_str_intern\u001b[39m\u001b[34m(inp, tensor_contents)\u001b[39m\n\u001b[32m    629\u001b[39m                     tensor_str = _tensor_str(\u001b[38;5;28mself\u001b[39m.to_dense(), indent)\n\u001b[32m    630\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m                     tensor_str = \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layout != torch.strided:\n\u001b[32m    634\u001b[39m     suffixes.append(\u001b[33m\"\u001b[39m\u001b[33mlayout=\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.layout))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/_tensor_str.py:363\u001b[39m, in \u001b[36m_tensor_str\u001b[39m\u001b[34m(self, indent)\u001b[39m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[32m    360\u001b[39m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     formatter = \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/_tensor_str.py:145\u001b[39m, in \u001b[36m_Formatter.__init__\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    142\u001b[39m         \u001b[38;5;28mself\u001b[39m.max_width = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     nonzero_finite_vals = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m&\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m.\u001b[49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals.numel() == \u001b[32m0\u001b[39m:\n\u001b[32m    150\u001b[39m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## training loop\n",
    "batch_size = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "n_epochs = 10\n",
    "for ep in range(n_epochs):\n",
    "    print(f\"\\n== epoch {ep} ==\\n\")\n",
    "\n",
    "    pbar = tqdm(range(0, len(df_tr), batch_size))\n",
    "    accs = []\n",
    "    for start_idx in pbar:\n",
    "\n",
    "        # subset training data into minibatches\n",
    "        end_idx = min((start_idx + batch_size), len(df_tr))\n",
    "        df_tr_sub = df_tr.iloc[start_idx:end_idx]\n",
    "\n",
    "        acc = train_model(model, df_tr_sub, optimizer, loss_fn)\n",
    "        accs.append(acc)\n",
    "        pbar.set_description(f\"train acc: {round(np.mean(accs),3)}\")\n",
    "\n",
    "    # test_model(model, df_val)\n",
    "    print([p for p in model.classifier.parameters()][0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a44f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss_fn(y_pred,y).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b4784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0106, -0.0171, -0.0295,  0.0323,  0.0349], device='mps:0',\n",
       "       dtype=torch.float16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in model.classifier.parameters()][0][0][:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
