{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd7a5d4",
   "metadata": {},
   "source": [
    "## ðŸª Kaust Academy: Classifying Math Probems\n",
    "Goal: to use my NLP background to produce a top solution by f1 for kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0f8a6",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b79b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "!python3 -m spacy download en_core_web_lg\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter as count\n",
    "\n",
    "from sklearn.metrics import f1_score as f1, accuracy_score as accuracy\n",
    "import pickle as pkl\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "!pip install lightgbm\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46c8c6",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e239bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run(command, shell = False):\n",
    "  ''' run the specified command via subprocess.run '''\n",
    "  if not shell: command = command.split(\" \")\n",
    "  process = subprocess.Popen(command, shell = shell) \n",
    "  process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc6a27",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdd1bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv(\"train-data/train.csv\")\n",
    "df_te = pd.read_csv(\"train-data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56edb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994d30d",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf59f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 (1) Problems A, B, and C of example 3 can all be written as Lv=wwhere L:V\u0000!W; (read this as Lmaps the set of vectors Vto the set of vectors W). For each case write down the sets VandWwhere the vectors vandw come from.\n",
      "1075 (2) Torque is a measure of \\rotational force\". It is a vector whose direction is the (preferred) axis of rotation. Upon applying a force Fon an object at pointrthe torque is the cross product r\u0002F= : 301.5 Review Problems 31 Remember that the cross product of two 3-vectors is given by 0 @x y z1 A\u00020 @x0 y0 z01 A:=0 @yz0\u0000zy0 zx0\u0000xz0 xy0\u0000yx01 A: Indeed, 3-vectors are special, usually vectors an only be added, not multiplied. Lets nd the force F(a vector) one must apply to a wrench lying along the vectorr=0 @1 1 01 Aft, to produce a torque0 @0 0 11 Aft lb: (a) Find a solution by writing out this equation with F=0 @a b c1 A. (Hint: Guess and check that a solution with a= 0 exists). (b) Add0 @1 1 01 Ato your solution and check that the result is a solution. (c) Give a physics explanation of why there can be two solutions, and argue that there are, in fact, in nitely many solutions. (d) Set up a system of three linear equations with the three compo- nents ofFas the variables which describes this situation. What happens if you try to solve these equations by substitution?\n",
      "478 (3) The function P(t) gives gas prices (in units of dollars per gallon) as a function of tthe year (in A.D. or C.E.), and g(t) is the gas consumption rate measured in gallons per year by a driver as a function of their age. The function gis certainly di erent for di erent people. Assuming a lifetime is 100 years, what function gives the total amount spent on gas during the lifetime of an individual born in an arbitrary year t? Is the operator that maps gto this function linear?\n",
      "508 (4) The di erential equation (DE) d dtf= 2f 3132 What is Linear Algebra? says that the rate of change of fis proportional to f. It describes exponential growth because the exponential function f(t) =f(0)e2t satis es the DE for any number f(0). The number 2 in the DE is called the constant of proportionality. A similar DE d dtf=2 tf has a time-dependent \\constant of proportionality\". (a) Do you think that the second DE describes exponential growth? (b) Write both DEs in the form Df= 0 withDa linear operator.\n",
      "1392 (5) Pablo is a nutritionist who knows that oranges always have twice as much sugar as apples. When considering the sugar intake of schoolchil- dren eating a barrel of fruit, he represents the barrel like so: sugarfruit (s;f) Find a linear operator relating Pablo's representation to the \\everyday\" representation in terms of the number of apples and number of oranges. Write your answer as a matrix. Hint: Let\u0015represent the amount of sugar in each apple. Hint 321.5 Review Problems 33 6.Matrix Multiplication: LetMandNbe matrices M=\u0012a b c d\u0013 andN=\u0012e f g h\u0013 ; andvthe vector v=\u0012x y\u0013 : If we rst apply Nand thenMtovwe obtain the vector MNv . (a) Show that the composition of matrices MN is also a linear oper- ator. (b) Write out the components of the matrix product MN in terms of the components of Mand the components of N.Hint: use the general rule for multiplying a 2-vector by a 2 \u00022 matrix. (c) Try to answer the following common question, \\Is there any sense in which these rules for matrix multiplication are unavoidable, or are they just a notation that could be replaced by some other notation?\" (d) Generalize your multiplication rule to 3 \u00023 matrices. 7.Diagonal matrices: A matrixMcan be thought of as an array of num- bersmi j, known as matrix entries, or matrix components, where iandj index row and column numbers, respectively. Let M=\u00121 2 3 4\u0013 =\u0000 mi j\u0001 : Computem1 1,m1 2,m2 1andm2\n",
      "790 (6) The matrix entries mi iwhose row and column numbers are the same are called the diagonal ofM. Matrix entries mi jwithi6=jare called o -diagonal . How many diagonal entries does an n\u0002nmatrix have? How many o -diagonal entries does an n\u0002nmatrix have? If all the o -diagonal entries of a matrix vanish, we say that the matrix is diagonal. Let D=\u0012\u00150 0\u0016\u0013 andD0=\u0012\u001500 0\u00160\u0013 : 3334 What is Linear Algebra? Are these matrices diagonal and why? Use the rule you found in prob- lem 6 to compute the matrix products DD0andD0D. What do you observe? Do you think the same property holds for arbitrary matrices? What about products where only one of the matrices is diagonal? (p.s. Diagonal matrices play a special role in in the study of matrices in linear algebra. Keep an eye out for this special role.)\n",
      "613 (7) Find the linear operator that takes in vectors from n-space and gives out vectors from n-space in such a way that (a) whatever you put in, you get exactly the same thing out as what you put in. Show that it is unique. Can you write this operator as a matrix? (b) whatever you put in, you get exactly the same thing out as when you put something else in. Show that it is unique. Can you write this operator as a matrix? Hint: To show something is unique, it is usually best to begin by pre- tending that it isn't, and then showing that this leads to a nonsensical conclusion. In mathspeak{ proof by contradiction .\n",
      "884 (8) Consider the set S=f\u0003;?;#g. It contains just 3 elements, and has no ordering;f\u0003;?;#g=f#;?;\u0003getc. (In fact the same is true for f1;2;3g=f2;3;1getc, although we could make this an ordered set using 3>2>1.) (i) Invent a function with domain f\u0003;?;#gand codomain R. (Re- member that the domain of a function is the set of all its allowed inputs and the codomain (ortarget space ) is the set where the outputs can live. A function is speci ed by assigning exactly one codomain element to each element of the domain.) (ii) Choose an ordering on f\u0003;?;#g, and then use it to write your function from part (i) as a triple of numbers. (iii) Choose a new ordering on f\u0003;?;#gand then write your function from part (i) as a triple of numbers. 341.5 Review Problems 35 (iv) Your answers for parts (ii) and (iii) are di erent yet represent the same function { explain! 3536 What is Linear Algebra? 36\n",
      "592 (9) A particular solution is3 2x2whilexand 1 are homogeneous solutions. The solution set is f3 2x2+ax+c1 :a;b2Rg. You can imagine similar di erential equations with more homogeneous solutions. You need to become very adept at reading o solutions sets of linear systems from the RREF of their augmented matrix; it is a basic skill for linear algebra, and we will continue using it up to the last page of the book! Worked examples of Gaussian elimination 2.2 Review Problems Webwork:Reading problems 1 , 2 Augmented matrix 6 2\u00022 systems 7, 8, 9, 10, 11, 12 3\u00022 systems 13, 14 3\u00023 systems 15, 16, 17\n",
      "341 (10) State whether the following augmented matrices are in RREF and com- pute their solution sets. 0 BB@1 0 0 0 3 1 0 1 0 0 1 2 0 0 1 0 1 3 0 0 0 1 2 01 CCA; 0 BB@1 1 0 1 0 1 0 0 0 1 2 0 2 0 0 0 0 0 1 3 0 0 0 0 0 0 0 01 CCA; 482.2 Review Problems 49 0 BBBB@1 1 0 1 0 1 0 1 0 0 1 2 0 2 0 \u00001 0 0 0 0 1 3 0 1 0 0 0 0 0 2 0 \u00002 0 0 0 0 0 0 1 11 CCCCA:\n",
      "293 (11) Solve the following linear system: 2x1+ 5x2\u00008x3+ 2x4+ 2x5= 0 6x1+ 2x2\u000010x3+ 6x4+ 8x5= 6 3x1+ 6x2+ 2x3+ 3x4+ 5x5= 6 3x1+ 1x2\u00005x3+ 3x4+ 4x5= 3 6x1+ 7x2\u00003x3+ 6x4+ 9x5= 9 Be sure to set your work out carefully with equivalence signs \u0018between each step, labeled by the row operations you performed.\n",
      "534 (12) Check that the following two matrices are row-equivalent: \u00121 4 7 10 2 9 6 0\u0013 and\u00120\u00001 8 20 4 18 12 0\u0013 : Now remove the third column from each matrix, and show that the resulting two matrices (shown below) are row-equivalent: \u00121 4 10 2 9 0\u0013 and\u00120\u0000120 4 18 0\u0013 : Now remove the fourth column from each of the original two matri- ces, and show that the resulting two matrices, viewed as augmented matrices (shown below) are row-equivalent: \u00121 4 7 2 9 6\u0013 and\u00120\u000018 4 18 12\u0013 : Explain why row-equivalence is never a ected by removing columns.\n",
      "322 (13) Check that the system of equations corresponding to the augmented matrix 0 @1 4 10 3 13 9 4 17 201 A 4950 Systems of Linear Equations has no solutions. If you remove one of the rows of this matrix, does the new matrix have any solutions? In general, can row equivalence be a ected by removing rows? Explain why or why not.\n",
      "178 (14) Explain why the linear system has no solutions: 0 @1 0 3 1 0 1 2 4 0 0 0 61 A For which values of kdoes the system below have a solution? x\u00003y = 6 x + 3z=\u00003 2x+ky+ (3\u0000k)z= 1 Hint\n",
      "211 (15) Show that the RREF of a matrix is unique. (Hint: Consider what happens if the same augmented matrix had two di erent RREFs. Try to see what happens if you removed columns from these two RREF augmented matrices.)\n",
      "618 (16) Another method for solving linear systems is to use row operations to bring the augmented matrix to Row Echelon Form (REF as opposed to RREF). In REF, the pivots are not necessarily set to one, and we only require that all entries left of the pivots are zero, not necessarily entries above a pivot. Provide a counterexample to show that row echelon form is not unique. Once a system is in row echelon form, it can be solved by \\back substi- tution.\" Write the following row echelon matrix as a system of equa- tions, then solve the system using back-substitution. 0 @2 3 1 6 0 1 1 2 0 0 3 31 A 502.2 Review Problems 51\n",
      "125 (17) Show that this pair of augmented matrices are row equivalent, assuming ad\u0000bc6= 0: a be c df! \u0018 1 0de\u0000bf ad\u0000bc 0 1af\u0000ce ad\u0000bc!\n",
      "367 (18) Consider the augmented matrix: \u00122\u000013 \u00006 3 1\u0013 : Give a geometric reason why the associated system of equations has no solution. (Hint, plot the three vectors given by the columns of this augmented matrix in the plane.) Given a general augmented matrix \u0012a be c df\u0013 ; can you nd a condition on the numbers a;b;c anddthat corresponds to the geometric condition you found?\n",
      "391 (19) A relation\u0018on a set of objects Uis an equivalence relation if the following three properties are satis ed: â€¢Re exive: For any x2U, we havex\u0018x. â€¢Symmetric: For any x;y2U, ifx\u0018ytheny\u0018x. â€¢Transitive: For any x;yandz2U, ifx\u0018yandy\u0018zthenx\u0018z. Show that row equivalence of matrices is an example of an equivalence relation. (For a discussion of equivalence relations, see Homework 0, Problem 4) Hint\n",
      "300 (20) Equivalence of augmented matrices does not come from equality of their solution sets. Rather, we de ne two matrices to be equivalent if one can be obtained from the other by elementary row operations. Find a pair of augmented matrices that are not row equivalent but do have the same solution set. 51\n",
      "193 (21) While performing Gaussian elimination on these augmented matrices write the full system of equations describing the new rows in terms of the old rows above each equivalence symbol as in Example\n",
      "145 (23) Solve the vector equation by applying ERO matrices to each side of the equation to perform elimination. Show each matrix explicitly as in Example\n",
      "182 (25) Solve this vector equation by nding the inverse of the matrix through (MjI)\u0018(IjM\u00001) and then applying M\u00001to both sides of the equation. 0 @2 1 1 1 1 1 1 1 21 A0 @x y z1 A=0 @9 6 71 A\n",
      "100 (26) Follow the method of Examples 29 and 30 to nd the LUandLDU factorization of 0 @3 3 6 3 5 2 6 2 51 A:\n",
      "258 (27) Multiple matrix equations with the same matrix can be solved simul- taneously. (a) Solve both systems by performing elimination on just one aug- mented matrix. 0 @2\u00001\u00001 \u00001 1 1 1\u00001 01 A0 @x y z1 A=0 @0 1 01 A;0 @2\u00001\u00001 \u00001 1 1 1\u00001 01 A0 @a b c1 A=0 @2 1 11 A 62\n",
      "142 (28) Write down examples of augmented matrices corresponding to each of the ve types of solution sets for systems of equations with three unknowns.\n",
      "249 (29) Invent a simple linear system that has multiple solutions. Use the stan- dard approach for solving linear systems and a non-standard approach to obtain di erent descriptions of the solution set. Is the solution set di erent with di erent approaches?\n",
      "1120 (30) Let M=0 BBBB@a1 1a1 2\u0001\u0001\u0001a1 k a2 1a2 2\u0001\u0001\u0001a2 k ......... ar 1ar 2\u0001\u0001\u0001ar k1 CCCCAandx=0 BBBB@x1 x2 ... xk1 CCCCA: Note:x2does not denote the square of the column vector x. Instead x1,x2,x3,etc... , denote di erent variables (the components of x); the superscript is an index. Although confusing at rst, this nota- tion was invented by Albert Einstein who noticed that quantities like a2 1x1+a2 2x2\u0001\u0001\u0001+a2 kxk=:Pk j=1a2 jxj, can be written unambiguously asa2 jxj. This is called Einstein summation notation. The most im- portant thing to remember is that the index jis a dummy variable, 682.6 Review Problems 69 so thata2 jxj\u0011a2 ixi; this is called \\relabeling dummy indices\". When dealing with products of sums, you must remember to introduce a new dummy for each term; i.e.,aixibiyi=P iaixibiyidoes notequal aixibjyj=\u0000P iaixi\u0001\u0000P jbjyj\u0001 . Use Einstein summation notation to propose a rule for Mx so that Mx= 0 is equivalent to the linear system a1 1x1+a1 2x2\u0001\u0001\u0001+a1 kxk= 0 a2 1x1+a2 2x2\u0001\u0001\u0001+a2 kxk= 0 ............ ar 1x1+ar 2x2\u0001\u0001\u0001+ar kxk= 0 Show that your rule for multiplying a matrix by a vector obeys the linearity property.\n",
      "250 (31) The standard basis vector eiis a column vector with a one in the ith row, and zeroes everywhere else. Using the rule for multiplying a matrix times a vector in problem 3, nd a simple rule for multiplying Mei, whereMis the general matrix de ned there.\n",
      "155 (32) IfAis a non-linear operator, can the solutions to Ax=bstill be written as \\general solution=particular solution + homogeneous solutions\"? Provide examples.\n",
      "206 (33) Find a system of equations whose solution set is the walls of a 1 \u00021\u00021 cube. (Hint: You may need to restrict the ranges of the variables; could your equations be linear?) 6970 Systems of Linear Equations 70\n",
      "263 (34) Maximize f(x;y) = 2x+ 3ysubject to the constraints x\u00150; y\u00150; x + 2y\u00142;2x+y\u00142; by (a) sketching the region in the xy-plane de ned by the constraints and then checking the values of fat its corners; and, (b) the simplex algorithm ( hint: introduce slack variables).\n",
      "868 (35) Conoil operates two wells (well A and well B) in southern Grease (a small Mediterranean country). You have been employed to gure out how many barrels of oil they should pump from each well to maximize 803.5 Review Problems 81 their pro t (all of which goes to shareholders, not operating costs). The quality of oil from well A is better than from well B, so is worth 50% more per barrel. The Greasy government cares about the environment and will not allow Conoil to pump in total more than 6 million barrels per year. Well A costs twice as much as well B to operate. Conoil's yearly operating budget is only su\u000ecient to pump at most 10 million barrels from well B per year. Using both a graphical method and then (as a double check) Dantzig's algorithm, determine how many barrels Conoil should pump from each well to maximize their pro ts. 8182 The Simplex Method 82\n",
      "847 (36) When he was young, Captain Conundrum mowed lawns on weekends to help pay his college tuition bills. He charged his customers according to the size of their lawns at a rate of 5 Â¢per square foot and meticulously kept a record of the areas of their lawns in an ordered list: A= (200;300;50;50;100;100;200;500;1000;100): He also listed the number of times he mowed each lawn in a given year, for the year 1988 that ordered list was f= (20;1;2;4;1;5;2;1;10;6): (a) Pretend that Aandfare vectors and compute Af. (b) What quantity does the dot product Afmeasure? (c) How much did Captain Conundrum earn from mowing lawns in 1988? Write an expression for this amount in terms of the vectors Aandf. (d) Suppose Captain Conundrum charged di erent customers di er- ent rates. How could you modify the expression in part 1c to compute the Captain's earnings?\n",
      "449 (37) (2) Find the angle between the diagonal of the unit square in R2and one of the coordinate axes. (3) Find the angle between the diagonal of the unit cube in R3and one of the coordinate axes. (n) Find the angle between the diagonal of the unit (hyper)-cube in Rnand one of the coordinate axes. 9798 Vectors in Space, n-Vectors (1) What is the limit as n!1 of the angle between the diagonal of the unit (hyper)-cube in Rnand one of the coordinate axes?\n",
      "249 (38) Consider the matrix M=\u0012cos\u0012sin\u0012 \u0000sin\u0012cos\u0012\u0013 and the vector X=\u0012x y\u0013 . (a) Sketch XandMX inR2for several values of Xand\u0012. (b) ComputejjMXjj jjXjjfor arbitrary values of Xand\u0012. (c) Explain your result for (b) and describe the action of Mgeomet- rically.\n",
      "520 (39) (Lorentzian Strangeness). For this problem, consider Rnwith the Lorentzian inner product de ned in example 53 above. (a) Find a non-zero vector in two-dimensional Lorentzian space-time with zero length. (b) Find and sketch the collection of all vectors in two-dimensional Lorentzian space-time with zero length. (c) Find and sketch the collection of all vectors in three-dimensional Lorentzian space-time with zero length. (d) Replace the word \\zero\" with the word \\one\" in the previous two parts. The Story of Your Life\n",
      "87 (40) Create a system of equations whose solution set is a 99 dimensional hyperplane in R101.\n",
      "303 (41) Recall that a plane in R3can be described by the equation n\u00010 @x y z1 A=n\u0001p 984.5 Review Problems 99 where the vector plabels a given point on the plane and nis a vector normal to the plane. Let NandPbe vectors in R101and X=0 BBB@x1 x2 ... x1011 CCCA: What kind of geometric object does N\u0001X=N\u0001Pdescribe?\n",
      "289 (42) Let u=0 BBBBB@1 1 1 ... 11 CCCCCAandv=0 BBBBB@1 2 3 ... 1011 CCCCCA Find the projection of vontouand the projection of uontov. (Hint: Remember that two vectors uandvde ne a plane, so rst work out how to project one vector onto another in a plane. The picture from Section 14.4 could help.)\n",
      "150 (43) If the solution set to the equation A(x) =bis the set of vectors whose tips lie on the paraboloid z=x2+y2, then what can you say about the function A?\n",
      "283 (44) Find a system of equations whose solution set is 8 >>< >>:0 BB@1 1 2 01 CCA+c10 BB@\u00001 \u00001 0 11 CCA+c20 BB@0 0 \u00001 \u000031 CCA c1;c22R9 >>= >>;: Give a general procedure for going from a parametric description of a hyperplane to a system of equations with that hyperplane as a solution set.\n",
      "154 (45) IfAis a linear operator and both vandcv(for any real number c) are solutions to Ax=b, then what can you say about b? 99100 Vectors in Space, n-Vectors 100\n",
      "140 (46) Check that\u001a\u0012x y\u0013 x;y2R\u001b =R2(with the usual addition and scalar multiplication) satis es all of the parts in the de nition of a vector space.\n",
      "302 (47) (a) Check that the complex numbers C=fx+iyji2=\u00001;x;y2Rg, satisfy all of the parts in the de nition of a vector space over C. Make sure you state carefully what your rules for vector addition and scalar multiplication are. (b) What would happen if you used Ras the base eld (try comparing to problem 1).\n",
      "438 (48) (a) Consider the set of convergent sequences, with the same addi- tion and scalar multiplication that we de ned for the space of sequences: V=n fjf:N!R;lim n!1f(n)2Ro \u001aRN: Is this still a vector space? Explain why or why not. (b) Now consider the set of divergent sequences, with the same addi- tion and scalar multiplication as before: V=n fjf:N!R;lim n!1f(n) does not exist or is \u00061o \u001aRN: Is this a vector space? Explain why or why not.\n",
      "247 (49) Consider the set of 2 \u00024 matrices: V=\u001a\u0012a b c d e f g h\u0013 a;b;c;d;e;f;g;h2C\u001b 109110 Vector Spaces Propose de nitions for addition and scalar multiplication in V. Identify the zero vector in V, and check that every matrix in Vhas an additive inverse.\n",
      "463 (50) LetPR 3be the set of polynomials with real coe\u000ecients of degree three or less. (a) Propose a de nition of addition and scalar multiplication to make PR 3a vector space. (b) Identify the zero vector, and nd the additive inverse for the vector \u00003\u00002x+x2. (c) Show that PR 3is not a vector space over C. Propose a small change to the de nition of PR 3to make it a vector space over C. (Hint: Every little symbol in the the instructions for par (c) is important!) Hint\n",
      "97 (51) LetV=fx2Rjx>0g=:R+. Forx;y2Vand\u00152R, de ne y=xy; \u0015 x=x\u0015: Show that ( V; ;R) is a vector space.\n",
      "235 (52) The component in the ith row and jth column of a matrix can be labeledmi j. In this sense a matrix is a function of a pair of integers. For what set Sis the set of 2\u00022 matrices the same as the set RS? Generalize to other size matrices.\n",
      "210 (53) Show that any function in Rf\u0003;?;#gcan be written as a sum of multiples of the functions e\u0003;e?;e#de ned by e\u0003(k) =8 < :1; k=\u0003 0; k=? 0; k= #; e?(k) =8 < :0; k=\u0003 1; k=? 0; k= #; e#(k) =8 < :0; k=\u0003 0; k=? 1; k= #:\n",
      "174 (54) LetVbe a vector space and Sany set. Show that the set VSof all functionsS!Vis a vector space. Hint: rst decide upon a rule for adding functions whose outputs are vectors. 110\n",
      "335 (55) Show that the pair of conditions: \u001aL(u+v) =L(u) +L(v) L(cv) =cL(v)(1) (valid for all vectors u;vand any scalar c) is equivalent to the single condition: L(ru+sv) =rL(u) +sL(v); (2) (for all vectors u;vand any scalars rands). Your answer should have two parts. Show that (1) )(2), and then show that (2) )(1). 1186.5 Review Problems 119\n",
      "290 (56) Iffis a linear function of one variable, then how many points on the graph of the function are needed to specify the function? Give an explicit expression for fin terms of these points. (You might want to look up the de nition of a graph before you make any assumptions about the function.)\n",
      "176 (57) (a) Ifp\u00121 2\u0013 = 1 andp\u00122 4\u0013 = 3 is it possible that pis a linear function? (b) IfQ(x2) =x3andQ(2x2) =x4is it possible that Qis a linear function from polynomials to polynomials?\n",
      "81 (58) Iffis a linear function such that f\u00121 2\u0013 = 0;andf\u00122 3\u0013 = 1; then what is f\u0012x y\u0013 ?\n",
      "269 (59) LetPnbe the space of polynomials of degree nor less in the variable t. SupposeLis a linear transformation from P2!P3such thatL(1) = 4, L(t) =t3, andL(t2) =t\u00001. (a) FindL(1 +t+ 2t2). (b) FindL(a+bt+ct2). (c) Find all values a;b;c such thatL(a+bt+ct2) = 1 + 3t+ 2t3. Hint\n",
      "137 (60) Show that the operator Ithat maps fto the functionIfde ned byIf(x) :=Rx 0f(t)dtis a linear operator on the space of continuous functions.\n",
      "307 (61) Letz2C. Recall that z=x+iyfor somex;y2R, and we can form the complex conjugate ofzby takingz=x\u0000iy. The function c:R2!R2 which sends ( x;y)7!(x;\u0000y) agrees with complex conjugation. (a) Show that cis a linear map over R(i.e.scalars in R). (b) Show that zis not linear over C. 119120 Linear Transformations 120\n",
      "809 (62) A door factory can buy supplies in two kinds of packages, fandg. The packagefcontains 3 slabs of wood, 4 fasteners, and 6 brackets. The packagegcontains 5 fasteners, 3 brackets, and 7 slabs of wood. (a) Explain how to view the packages fandgas functions and list their inputs and outputs. 129130 Matrices (b) Choose an ordering for the 3 kinds of supplies and use this to rewritefandgas elements of R3. (c) LetLbe a manufacturing process that takes as inputs supply packages and outputs two products (doors, and door frames). Ex- plain how it can be viewed as a function mapping one vector space into another. (d) Assuming that Lis linear and Lfis 1 door and 2 frames, and Lg is 3 doors and 1 frame, nd a matrix for L. Be sure to specify the basis vectors you used, both for the input and output vector space.\n",
      "528 (63) You are designing a simple keyboard synthesizer with two keys. If you push the rst key with intensity athen the speaker moves in time as asin(t). If you push the second key with intensity bthen the speaker moves in time as bsin(2t). If the keys are pressed simultaneously, (a) describe the set of all sounds that come out of your synthesizer. (Hint: Sounds can be \\added\".) (b) Graph the function\u00123 5\u0013 2Rf1;2g. (c) LetB= (sin(t);sin(2t)). Explain why\u00123 5\u0013 Bis not in Rf1;2gbut is still a function. (d) Graph the function\u00123 5\u0013 B.\n",
      "667 (64) (a) Find the matrix ford dxacting on the vector space Vof polynomi- als of degree 2 or less in the ordered basis B= (x2;x;1) (b) Use the matrix from part (a) to rewrite the di erential equation d dxp(x) =xas a matrix equation. Find all solutions of the matrix equation. Translate them into elements of V. 1307.2 Review Problems 131 (c) Find the matrix ford dxacting on the vector space Vin the ordered basisB0= (x2+x;x2\u0000x;1). (d) Use the matrix from part (c) to rewrite the di erential equation d dxp(x) =xas a matrix equation. Find all solutions of the matrix equation. Translate them into elements of V. (e) Compare and contrast your results from parts (b) and (d).\n",
      "242 (65) Find the \\matrix\" ford dxacting on the vector space of all power series in the ordered basis (1 ;x;x2;x3;:::). Use this matrix to nd all power series solutions to the di erential equationd dxf(x) =x.Hint: your \\matrix\" may not have nite size.\n",
      "100 (66) Find the matrix ford2 dx2acting onfc1cos(x) +c2sin(x)jc1;c22Rgin the ordered basis (cos( x);sin(x)).\n",
      "162 (67) Find the matrix ford dxacting onfc1cosh(x) +c2sinh(x)jc1;c22Rgin the ordered basis (cosh(x);sinh(x)) and in the ordered basis (cosh(x) + sinh(x);cosh(x)\u0000sinh(x)):\n",
      "256 (68) LetB= (1;x;x2) be an ordered basis for V=fa0+a1x+a2x2ja0;a1;a22Rg; and letB0= (x3;x2;x;1) be an ordered basis for W=fa0+a1x+a2x2+a3x3ja0;a1;a2;a32Rg; Find the matrix for the operator I:V!Wde ned by Ip(x) =Zx 1p(t)dt relative to these bases. 131132 Matrices\n",
      "1242 (69) This exercise is meant to show you a generalization of the procedure you learned long ago for nding the function mx+bgiven two points on its graph. It will also show you a way to think of matrices as members of a much bigger class of arrays of numbers. Find the (a) constant function f:R!Rwhose graph contains (2 ;3). (b) linear function h:R!Rwhose graph contains (5 ;4). (c) rst order polynomial function g:R!Rwhose graph contains (1;2) and (3;3). (d) second order polynomial function p:R!Rwhose graph contains (1;0), (3;0) and (5;0). (e) second order polynomial function q:R!Rwhose graph contains (1;1), (3;2) and (5;7). (f) second order homogeneous polynomial function r:R!Rwhose graph contains (3 ;2). (g) number of points required to specify a third order polynomial R!R. (h) number of points required to specify a third order homogeneous polynomial R!R. (i) number of points required to specify a n-th order polynomial R! R. (j) number of points required to specify a n-th order homogeneous polynomial R!R. (k) rst order polynomial function F:R2!Rwhose graph contains\u0012\u00120 0\u0013 ;1\u0013 ,\u0012\u00120 1\u0013 ;2\u0013 ,\u0012\u00121 0\u0013 ;3\u0013 , and\u0012\u00121 1\u0013 ;4\u0013 . (l) homogeneous rst order polynomial function H:R2!Rwhose graph contains\u0012\u00120 1\u0013 ;2\u0013 ,\u0012\u00121 0\u0013 ;3\u0013 , and\u0012\u00121 1\u0013 ;4\u0013 . 132\n",
      "560 (70) Compute the following matrix products 0 B@1 2 1 4 5 2 7 8 21 CA0 B@\u000024 3\u00001 3 2\u00005 32 3 \u00001 2\u000011 CA;\u0000 1 2 3 4 5\u00010 BBBBB@1 2 3 4 51 CCCCCA; 0 BBBBB@1 2 3 4 51 CCCCCA\u0000 1 2 3 4 5\u0001 ;0 B@1 2 1 4 5 2 7 8 21 CA0 B@\u000024 3\u00001 3 2\u00005 32 3 \u00001 2\u000011 CA0 B@1 2 1 4 5 2 7 8 21 CA; 1467.4 Review Problems 147 \u0000 x y z\u00010 @2 1 1 1 2 1 1 1 21 A0 B@x y z1 CA;0 BBBBB@2 1 2 1 2 0 2 1 2 1 0 1 2 1 2 0 2 1 2 1 0 0 0 0 21 CCCCCA0 BBBBB@1 2 1 2 1 0 1 2 1 2 0 2 1 2 1 0 1 2 1 2 0 0 0 0 11 CCCCCA; 0 B@\u000024 3\u00001 3 2\u00005 32 3 \u00001 2\u000011 CA0 B@42 3\u00002 3 65 3\u00002 3 12\u000016 310 31 CA0 B@1 2 1 4 5 2 7 8 21 CA:\n",
      "969 (71) Let's prove the theorem ( MN)T=NTMT. Note: the following is a common technique for proving matrix identities. (a) LetM= (mi j) and letN= (ni j). Write out a few of the entries of each matrix in the form given at the beginning of section 7.3. (b) Multiply out MN and write out a few of its entries in the same form as in part (a). In terms of the entries of Mand the entries ofN, what is the entry in row iand column jofMN? (c) Take the transpose ( MN)Tand write out a few of its entries in the same form as in part (a). In terms of the entries of Mand the entries ofN, what is the entry in row iand column jof (MN)T? (d) Take the transposes NTandMTand write out a few of their entries in the same form as in part (a). (e) Multiply out NTMTand write out a few of its entries in the same form as in part a. In terms of the entries of Mand the entries of N, what is the entry in row iand column jofNTMT? (f) Show that the answers you got in parts (c) and (e) are the same.\n",
      "248 (72) (a) Let A=\u00121 2 0 3\u00001 4\u0013 . FindAATandATAand their traces. (b) LetMbe anym\u0002nmatrix. Show that MTMandMMTare symmetric. (Hint: use the result of the previous problem.) What are their sizes? What is the relationship between their traces? 147148 Matrices\n",
      "103 (73) Letx=0 B@x1 ... xn1 CAandy=0 B@y1 ... yn1 CAbe column vectors. Show that the dot product xy=xTI y. Hint\n",
      "258 (74) Above, we showed that leftmultiplication by an r\u0002smatrixNwas a linear transformation Ms kN\u0000!Mr k. Show that right multiplication by ak\u0002mmatrixRis a linear transformation Ms kR\u0000!Ms m. In other words, show that right matrix multiplication obeys linearity. Hint\n",
      "523 (75) Let the Vbe a vector space where B= (v1;v2) is an ordered basis. Suppose L:Vlinear\u0000\u0000\u0000!V and L(v1) =v1+v2; L (v2) = 2v1+v2: Compute the matrix of Lin the basis Band then compute the trace of this matrix. Suppose that ad\u0000bc6= 0 and consider now the new basis B0= (av1+bv2;cv1+dv2): Compute the matrix of Lin the basis B0. Compute the trace of this matrix. What do you nd? What do you conclude about the trace of a matrix? Does it make sense to talk about the \\trace of a linear transformation\" without reference to any bases?\n",
      "230 (76) Explain what happens to a matrix when: (a) You multiply it on the left by a diagonal matrix. (b) You multiply it on the right by a diagonal matrix. 1487.4 Review Problems 149 Give a few simple examples before you start explaining.\n",
      "83 (77) Compute exp( A) for the following matrices: â€¢A=\u0012\u00150 0\u0015\u0013 â€¢A=\u00121\u0015 0 1\u0013 â€¢A=\u00120\u0015 0 0\u0013 Hint\n",
      "265 (78) LetM=0 BBBBBBBBBB@1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 3 1 0 0 0 0 0 0 0 31 CCCCCCCCCCA. DivideMinto named blocks, with one block the 4 \u00024 identity matrix, and then multiply blocks to computeM2.\n",
      "228 (79) A matrix Ais called anti-symmetric (or skew-symmetric) if AT=\u0000A. Show that for every n\u0002nmatrixM, we can write M=A+Swhere Ais an anti-symmetric matrix and Sis a symmetric matrix. Hint: What kind of matrix is M+MT? How about M\u0000MT?\n",
      "169 (80) An example of an operation which is not associative is the cross prod- uct. (a) Give a simple example of three vectors from 3-space u;v;w such thatu\u0002(v\u0002w)6= (u\u0002v)\u0002w. 149\n",
      "2043 (81) Therefore, we can apply all of the linear algebra we have learned thus far to matrices with Z2entries. A matrix with entries in Z2is sometimes called a bit matrix . Example 950 @1 0 1 0 1 1 1 1 11 Ais an invertible matrix over Z2; 0 @1 0 1 0 1 1 1 1 11 A\u00001 =0 @0 1 1 1 0 1 1 1 11 A: This can be easily veri ed by multiplying: 0 @1 0 1 0 1 1 1 1 11 A0 @0 1 1 1 0 1 1 1 11 A=0 @1 0 0 0 1 0 0 0 11 A Application: Cryptography A very simple way to hide information is to use a sub- stitution cipher, in which the alphabet is permuted and each letter in a message is systematically exchanged for another. For example, the ROT-13 cypher just exchanges a letter with the letter thirteen places before or after it in the alphabet. For example, HELLO becomes URYYB. Applying the algorithm again decodes the message, turning URYYB back into HELLO. Substitution ciphers are easy to break, but the basic idea can be extended to create cryptographic systems that are practically uncrackable. For example, a one-time pad is a system that uses a di erent substitution for each letter in the message. So long as a particular set of substitutions is not used on more than one message, the one-time pad is unbreakable. English characters are often stored in computers in the ASCII format. In ASCII, a single character is represented by a string of eight bits, which we can consider as a vector in Z8 2(which is like vectors in R8, where the entries are zeros and ones). One way to create a substitution cipher, then, is to choose an 8\u00028invertible bit matrix M, and multiply each letter of the message by M. Then to decode the message, each string of eight characters would be multiplied by M\u00001. To make the message a bit tougher to decode, one could consider pairs (or longer sequences) of letters as a single vector in Z16 2(or a higher-dimensional space), and then use an appropriately-sized invertible matrix. For more on cryptography, see \\The Code Book,\" by Simon Singh (1999, Doubleday). 7.6 Review Problems Webwork: Reading Problems 6 , 7 155156 Matrices\n",
      "169 (82) Find formulas for the inverses of the following matrices, when they are not singular: (a)0 @1a b 0 1c 0 0 11 A (b)0 @a b c 0d e 0 0f1 A When are these matrices singular?\n",
      "135 (83) Write down all 2 \u00022 bit matrices and decide which of them are singular. For those which are not singular, pair them with their inverse.\n",
      "1354 (84) LetMbe a square matrix. Explain why the following statements are equivalent: (a)MX =Vhas a unique solution for every column vector V. (b)Mis non-singular. Hint: In general for problems like this, think about the key words: First, suppose that there is some column vector Vsuch that the equa- tionMX =Vhas two distinct solutions. Show that Mmust be sin- gular; that is, show that Mcan have no inverse. Next, suppose that there is some column vector Vsuch that the equa- tionMX =Vhas no solutions. Show that Mmust be singular. Finally, suppose that Mis non-singular. Show that no matter what the column vector Vis, there is a unique solution to MX =V: Hint 4.Left and Right Inverses: So far we have only talked about inverses of square matrices. This problem will explore the notion of a left and right inverse for a matrix that is not square. Let A=\u00120 1 1 1 1 0\u0013 1567.6 Review Problems 157 (a) Compute: i.AAT, ii.\u0000 AAT\u0001\u00001, iii.B:=AT\u0000 AAT\u0001\u00001 (b) Show that the matrix Babove is a right inverse forA,i.e., verify that AB=I: (c) IsBAde ned? (Why or why not?) (d) LetAbe ann\u0002mmatrix with n > m . Suggest a formula for a left inverse Csuch that CA=I Hint: you may assume that ATAhas an inverse. (e) Test your proposal for a left inverse for the simple example A=\u00121 2\u0013 ; (f) True or false: Left and right inverses are unique. If false give a counterexample. Hint\n",
      "379 (85) Show that if the range (remember that the range of a function is the set of all its outputs, not the codomain) of a 3 \u00023 matrixM(viewed as a function R3!R3) is a plane then one of the columns is a sum of multiples of the other columns. Show that this relationship is preserved under EROs. Show, further, that the solutions to Mx= 0 describe this relationship between the columns.\n",
      "104 (86) IfMandNare square matrices of the same size such that M\u00001exists andN\u00001does not exist, does ( MN)\u00001exist?\n",
      "79 (87) IfMis a square matrix which is not invertible, is eMinvertible? 157158 Matrices\n",
      "258 (88) Elementary Column Operations (ECOs) can be de ned in the same 3 types as EROs. Describe the 3 kinds of ECOs. Show that if maximal elimination using ECOs is performed on a square matrix and a column of zeros is obtained then that matrix is not invertible. 158\n",
      "244 (89) Consider the linear system: x1=v1 l2 1x1+x2=v2 ...... ln 1x1+ln 2x2+\u0001\u0001\u0001+xn=vn (i) Findx1. (ii) Findx2. (iii) Findx3. 1667.8 Review Problems 167 (k) Try to nd a formula or recursive method for nding xk. Don't worry about simplifying your answer.\n",
      "226 (90) LetM=\u0012X Y Z W\u0013 be a square n\u0002nblock matrix with Winvertible. i:IfWhasrrows, what size are X,Y, andZ? ii:Find aUDL decomposition for M. In other words, ll in the stars in the following equation: \u0012X Y Z W\u0013 =\u0012I\u0003 0I\u0013\u0012\u00030 0\u0003\u0013\u0012I0 \u0003I\u0013\n",
      "160 (91) Show that if Mis a square matrix which is not invertible then either the matrix matrix Uor the matrix Lin the LU-decomposition M=LU has a zero on it's diagonal.\n",
      "93 (92) Describe what upper and lower triangular matrices do to the unit hy- percube in their domain.\n",
      "325 (93) In chapter 3 we saw that, since in general row exchange matrices are necessary to achieve upper triangular form, LDPU factorization is the complete decomposition of an invertible matrix into EROs of various kinds. Suggest a procedure for using LDPU decompositions to solve linear systems that generalizes the procedure above.\n",
      "98 (94) Is there a reason to prefer LUdecomposition to ULdecomposition, or is the order just a convention?\n",
      "138 (95) IfMis invertible then what are the LU; LDU; andLDPU decompo- sitions ofMTin terms of the decompositions for M? Can you do the same forM\u00001?\n",
      "85 (96) Argue that if Mis symmetric then L=UTin theLDU decomposition ofM. 167168 Matrices 168\n",
      "185 (97) Let M=0 B@m1 1m1 2m1 3 m2 1m2 2m2 3 m3 1m3 2m3 31 CA: 1828.3 Review Problems 183 Use row operations to put Minto row echelon form . For simplicity, assume that m1 16= 06=m1 1m2 2\u0000m2 1m1\n",
      "125 (98) Prove that Mis non-singular if and only if: m1 1m2 2m3 3\u0000m1 1m2 3m3 2+m1 2m2 3m3 1\u0000m1 2m2 1m3 3+m1 3m2 1m3 2\u0000m1 3m2 2m3 16= 0\n",
      "359 (99) (a) What does the matrix E1 2=\u00120 1 1 0\u0013 do toM=\u0012a b d c\u0013 under left multiplication? What about right multiplication? (b) Find elementary matrices R1(\u0015) andR2(\u0015) that respectively mul- tiply rows 1 and 2 of Mby\u0015but otherwise leave Mthe same under left multiplication. (c) Find a matrix S1 2(\u0015) that adds a multiple \u0015of row 2 to row 1 under left multiplication.\n",
      "462 (100) Let ^\u001bdenote the permutation obtained from \u001bby transposing the rst two outputs, i.e.^\u001b(1) =\u001b(2) and ^\u001b(2) =\u001b(1). Suppose the function f:f1;2;3;4g!R. Write out explicitly the following two sums: X \u001bf\u0000 \u001b(s)\u0001 andX \u001bf\u0000 ^\u001b(s)\u0001 : What do you observe? Now write a brief explanation why the following equality holdsX \u001bF(\u001b) =X \u001bF(^\u001b); where the domain of the function Fis the set of all permutations of n objects and ^ \u001bis related to \u001bby swapping a given pair of objects.\n",
      "143 (101) LetMbe a matrix and Si jMthe same matrix with rows iandj switched. Explain every line of the series of equations proving that detM=\u0000det(Si jM).\n",
      "89 (102) LetM0be the matrix obtained from Mby swapping two columns i andj. Show that det M0=\u0000detM.\n",
      "267 (103) The scalar triple product of three vectors u;v;w fromR3isu\u0001(v\u0002w). Show that this product is the same as the determinant of the matrix whose columns are u;v;w (in that order). What happens to the scalar triple product when the factors are permuted? 183184 Determinants\n",
      "110 (104) Show that if Mis a 3\u00023 matrix whose third row is a sum of multiples of the other rows ( R3=aR2+bR1) then detM=\n",
      "85 (105) Show that the same is true if one of the columns is a sum of multiples of the others.\n",
      "225 (106) Calculate the determinant below by factoring the matrix into elemen- tary matrices times simpler matrices and using the trick det(M) = det(E\u00001EM) = det(E\u00001) det(EM): Explicitly show each ERO matrix. det0 @2 1 0 4 3 1 2 2 21 A\n",
      "131 (107) LetM=\u0012a b c d\u0013 andN=\u0012x y z w\u0013 . Compute the following: (a) detM. (b) detN. (c) det(MN). (d) detMdetN. (e) det(M\u00001) assuming ad\u0000bc6=\n",
      "130 (108) (f) det(MT) (g) det(M+N)\u0000(detM+ detN). Is the determinant a linear trans- formation from square matrices to real numbers? Explain.\n",
      "99 (109) Suppose M=\u0012a b c d\u0013 is invertible. Write Mas a product of elemen- tary row matrices times RREF( M).\n",
      "164 (110) Find the inverses of each of the elementary matrices, Ei j;Ri(\u0015);Si j(\u0015). Make sure to show that the elementary matrix times its inverse is ac- tually the identity.\n",
      "469 (111) Letei jdenote the matrix with a 1 in the i-th row and j-th column and 0's everywhere else, and let Abe an arbitrary 2\u00022 matrix. Com- pute det(A+tI2). What is the rst order term (the t1term)? Can you 1848.3 Review Problems 185 express your results in terms of tr( A)? What about the rst order term in det(A+tIn) for any arbitrary n\u0002nmatrixAin terms of tr( A)? Note that the result of det( A+tI2) is a polynomial in the variable t known as the characteristic polynomial .\n",
      "477 (112) (Directional) Derivative of the determinant: Notice that det: Mn n!R(where Mn nis the vector space of all n\u0002n matrices) det is a function of n2variables so we can take directional derivatives of det. LetAbe an arbitrary n\u0002nmatrix, and for all iandjcompute the following: (a) lim t!0det(I2+tei j)\u0000det(I2) t (b) lim t!0det(I3+tei j)\u0000det(I3) t (c) lim t!0det(In+tei j)\u0000det(In) t (d) lim t!0det(In+At)\u0000det(In) t Note, these are the directional derivative in the ei jandAdirections.\n",
      "1655 (113) How many functions are in the set ff:f1;:::;ng!f 1;:::;ngjf\u00001existsg? What about the set f1;:::;ngf1;:::;ng? Which of these two sets correspond to the set of all permutations of n objects? 185186 Determinants 8.4 Properties of the Determinant We now know that the determinant of a matrix is non-zero if and only if that matrix is invertible. We also know that the determinant is a multiplicative function, in the sense that det( MN) = detMdetN. Now we will devise some methods for calculating the determinant. Recall that: detM=X \u001bsgn(\u001b)m1 \u001b(1)m2 \u001b(2)\u0001\u0001\u0001mn \u001b(n): Aminor of ann\u0002nmatrixMis the determinant of any square matrix obtained from Mby deleting one row and one column. In particular, any entrymi jof a square matrix Mis associated to a minor obtained by deleting theith row and jth column of M. It is possible to write the determinant of a matrix in terms of its minors as follows: detM=X \u001bsgn(\u001b)m1 \u001b(1)m2 \u001b(2)\u0001\u0001\u0001mn \u001b(n) =m1 1X =\u001b1sgn(=\u001b1)m2 =\u001b1(2)\u0001\u0001\u0001mn =\u001b1(n) +m1 2X =\u001b2sgn(=\u001b2)m2 =\u001b2(1)m3 =\u001b2(3)\u0001\u0001\u0001mn =\u001b2(n) +m1 3X =\u001b3sgn(=\u001b3)m2 =\u001b3(1)m3 =\u001b3(2)m4 =\u001b3(4)\u0001\u0001\u0001mn =\u001b3(n) +\u0001\u0001\u0001 Here the symbols =\u001bkrefers to the permutation \u001bwith the input kremoved. The summand on the j'th line of the above formula looks like the determinant of the minor obtained by removing the rst and j'th column of M. However we still need to replace sum of =\u001bjby a sum over permutations of column numbers of the matrix entries of this minor. This costs a minus sign whenever j\u00001 is odd. In other words, to expand by minors we pick an entry m1 jof the rst row, then add ( \u00001)j\u00001times the determinant of the matrix with row i and column jdeleted. An example will probably help: 186\n",
      "87 (114) Find the determinant via expanding by minors. 0 BB@2 1 3 7 6 1 4 4 2 1 8 0 1 0 2 01 CCA\n",
      "163 (115) Even ifMis not a square matrix, both MMTandMTMare square. Is it true that det( MMT) = det(MTM) for all matrices M? How about tr(MMT) = tr(MTM)? 193194 Determinants\n",
      "336 (116) Let\u001b\u00001denote the inverse permutation of \u001b. Suppose the function f:f1;2;3;4g!R. Write out explicitly the following two sums: X \u001bf\u0000 \u001b(s)\u0001 andX \u001bf\u0000 \u001b\u00001(s)\u0001 : What do you observe? Now write a brief explanation why the following equality holdsX \u001bF(\u001b) =X \u001bF(\u001b\u00001); where the domain of the function Fis the set of all permutations of n objects.\n",
      "165 (117) Suppose M=LUis an LU decomposition. Explain how you would e\u000eciently compute det Min this case. How does this decomposition allow you to easily see if Mis invertible?\n",
      "906 (118) In computer science, the complexity of an algorithm is (roughly) com- puted by counting the number of times a given operation is performed. Suppose adding or subtracting any two numbers takes aseconds, and multiplying two numbers takes mseconds. Then, for example, com- puting 2\u00016\u00005 would take a+mseconds. (a) How many additions and multiplications does it take to compute the determinant of a general 2 \u00022 matrix? (b) Write a formula for the number of additions and multiplications it takes to compute the determinant of a general n\u0002nmatrix using the de nition of the determinant as a sum over permutations. Assume that nding and multiplying by the sign of a permutation is free. (c) How many additions and multiplications does it take to compute the determinant of a general 3 \u00023 matrix using expansion by minors? Assuming m= 2a, is this faster than computing the determinant from the de nition? Hint 194\n",
      "111 (120) LetUandWbe subspaces of V. Are: (a)U[W (b)U\\W also subspaces? Explain why or why not. Draw examples in R3. Hint\n",
      "90 (121) LetL:R3!R3where L(x;y;z ) = (x+ 2y+z;2x+y+z;0): Find kerL, imLand the eigenspaces R3 \u00001,R3\n",
      "75 (122) Your answers should be subsets of R3. Express them using span notation. 202\n",
      "558 (123) LetBnbe the space of n\u00021 bit-valued matrices ( i.e., column vectors) over the eld Z2. Remember that this means that the coe\u000ecients in any linear combination can be only 0 or 1, with rules for adding and multiplying coe\u000ecients given here. (a) How many di erent vectors are there in Bn? (b) Find a collection Sof vectors that span B3and are linearly inde- pendent. In other words, nd a basis of B3. (c) Write each other vector in B3as a linear combination of the vectors in the setSthat you chose. (d) Would it be possible to span B3with only two vectors? Hint\n",
      "280 (124) Leteibe the vector in Rnwith a 1 in the ith position and 0's in every other position. Let vbe an arbitrary vector in Rn. (a) Show that the collection fe1;:::;engis linearly independent. (b) Demonstrate that v=Pn i=1(vei)ei. (c) The spanfe1;:::;engis the same as what vector space?\n",
      "526 (125) Consider the ordered set of vectors from R3 0 @0 @1 2 31 A;0 @2 4 61 A;0 @1 0 11 A;0 @1 4 51 A1 A (a) Determine if the set is linearly independent by using the vectors as the columns of a matrix Mand nding RREF( M). 21010.4 Review Problems 211 (b) If possible, write each vector as a linear combination of the pre- ceding ones. (c) Remove the vectors which can be expressed as linear combinations of the preceding vectors to form a linearly independent ordered set. (Every vector in your set set should be from the given set.)\n",
      "773 (126) Gaussian elimination is a useful tool to gure out whether a set of vectors spans a vector space and if they are linearly independent. Consider a matrix Mmade from an ordered set of column vectors (v1;v2;:::;vm)\u001aRnand the three cases listed below: (a) RREF( M) is the identity matrix. (b) RREF( M) has a row of zeros. (c) Neither case (a) or (b) apply. First give an explicit example for each case, state whether the col- umn vectors you use are linearly independent or spanning in each case. Then, in general, determine whether ( v1;v2;:::;vm) are linearly inde- pendent and/or spanning Rnin each of the three cases. If they are linearly dependent, does RREF( M) tell you which vectors could be removed to yield an independent set of vectors? 211212 Linear Independence 212\n",
      "294 (127) (a) Draw the collection of all unit vectors in R2. (b) LetSx=\u001a\u00121 0\u0013 ;x\u001b , wherexis a unit vector in R2. For which x isSxa basis of R2? (c) Sketch all unit vectors in R3. (d) For which x2R3isSx=8 < :0 @1 0 01 A;0 @0 1 01 A;x9 = ;a basis for R3. (e) Discuss the generalization of the above to Rn.\n",
      "520 (128) LetBnbe the vector space of column vectors with bit entries 0 ;1. Write down every basis for B1andB2. How many bases are there for B3? B4? Can you make a conjecture for the number of bases for Bn? 221222 Basis and Dimension (Hint: You can build up a basis for Bnby choosing one vector at a time, such that the vector you choose is not in the span of the previous vectors you've chosen. How many vectors are in the span of any one vector? Any two vectors? How many vectors are in the span of any k vectors, for k\u0014n?) Hint\n",
      "560 (129) Suppose that Vis ann-dimensional vector space. (a) Show that any nlinearly independent vectors in Vform a basis. (Hint: Letfw1;:::;wmgbe a collection of nlinearly independent vectors inV, and letfv1;:::;vngbe a basis for V. Apply the method of Lemma 11.0.2 to these two sets of vectors.) (b) Show that any set of nvectors inVwhich span Vforms a basis forV. (Hint: Suppose that you have a set of nvectors which span Vbut do not form a basis. What must be true about them? How could you get a basis from this set? Use Corollary 11.0.3 to derive a contradiction.)\n",
      "404 (130) LetS=fv1;:::;vngbe a subset of a vector space V. Show that if every vectorwinVcan be expressed uniquely as a linear combination of vec- tors inS, thenSis a basis of V. In other words: suppose that for every vectorwinV, there is exactly one set of constants c1;:::;cnso that c1v1+\u0001\u0001\u0001+cnvn=w. Show that this means that the set Sis linearly independent and spans V. (This is the converse to theorem 11.0.1.)\n",
      "402 (131) Vectors are objects that you can add together; show that the set of all linear transformations mapping R3!Ris itself a vector space. Find a basis for this vector space. Do you think your proof could be modi ed to work for linear transformations Rn!R? For RN!Rm? For RR? Hint: Represent R3as column vectors, and argue that a linear trans- formationT:R3!Ris just a row vector. 22211.3 Review Problems 223\n",
      "439 (132) LetSndenote the vector space of all n\u0002nsymmetric matrices; Sn:=fM:Rn!RnjM=MTg: LetAndenote the vector space of all n\u0002nanti-symmetric matrices; An=fM:Rn!RnjM=\u0000MTg: (a) Find a basis for S3. (b) Find a basis for A3. (c) Can you nd a basis for Sn? ForAn? Hint: Describe it in terms of combinations of the matrices Fi j which have a 1 in the i-th row and the j-th column and 0 every- where else. Note that fFi jj1\u0014i\u0014r;1\u0014j\u0014kgis a basis for Mr k.\n",
      "287 (133) Give the matrix of the linear transformation Lwith respect to the input and output bases BandB0listed below: (a)L:V!WwhereB= (v1;:::;vn) is a basis for VandB0= (L(v1);:::;L (vn)) is a basis for W. (b)L:V!VwhereB=B0= (v1;:::;vn) andL(vi) =\u0015ivifor all 1\u0014i\u0014n. 223224 Basis and Dimension 224\n",
      "240 (134) Try to nd more solutions to the vibrating string problem @2y=@t2= @2y=@x2using the ansatz y(x;t) = sin(!t)f(x): What equation must f(x) obey? Can you write this as an eigenvector equation? Suppose that the string has length Landf(0) =f(L) =\n",
      "176 (136) LetM=\u00122 1 0 2\u0013 . Find all eigenvalues of M. DoesMhave two linearly independent eigenvectors? Is there a basis in which the matrix of Mis diagonal? ( I.e., canMbe diagonalized?)\n",
      "425 (137) Consider L:R2!R2with L\u0012x y\u0013 =\u0012xcos\u0012+ysin\u0012 \u0000xsin\u0012+ycos\u0012\u0013 : (a) Write the matrix of Lin the basis\u00121 0\u0013 ;\u00120 1\u0013 . (b) When\u00126= 0, explain how Lacts on the plane. Draw a picture. (c) Do you expect Lto have invariant directions? (Consider also special values of \u0012.) (d) Try to nd real eigenvalues for Lby solving the equation L(v) =\u0015v: (e) Are there complex eigenvalues for L, assuming that i=p\u00001 exists? 23812.4 Review Problems 239\n",
      "383 (138) LetLbe the linear transformation L:R3!R3given by L0 @x y z1 A=0 @x+y x+z y+z1 A: Leteibe the vector with a one in the ith position and zeros in all other positions. (a) FindLeifor eachi= 1;2;3. (b) Given a matrix M=0 @m1 1m1 2m1 3 m2 1m2 2m2 3 m3 1m3 2m3 31 A, what can you say about Meifor eachi? (c) Find a 3\u00023 matrixMrepresenting L. (d) Find the eigenvectors and eigenvalues of M:\n",
      "227 (139) LetAbe a matrix with eigenvector vwith eigenvalue \u0015. Show that vis also an eigenvector for A2and nd the corresponding eigenvalue. How about forAnwheren2N? Suppose that Ais invertible. Show that v is also an eigenvector for A\u00001.\n",
      "304 (140) A projection is a linear operator Psuch thatP2=P. Letvbe an eigenvector with eigenvalue \u0015for a projection P, what are all possible values of\u0015? Show that every projection Phas at least one eigenvector. Note that every complex matrix has at least 1 eigenvector, but you need to prove the above for any eld.\n",
      "220 (141) Explain why the characteristic polynomial of an n\u0002nmatrix has de- green. Make your explanation easy to read by starting with some simple examples, and then use properties of the determinant to give a general explanation.\n",
      "1046 (142) Compute the characteristic polynomial PM(\u0015) of the matrix M=\u0012a b c d\u0013 : Now, since we can evaluate polynomials on square matrices, we can plugMinto its characteristic polynomial and nd the matrixPM(M). 239240 Eigenvalues and Eigenvectors What do you nd from this computation? Does something similar hold for 3\u00023 matrices? (Try assuming that the matrix of Mis diagonal to answer this.) 9.Discrete dynamical system. LetMbe the matrix given by M=\u00123 2 2 3\u0013 : Given any vector v(0) =\u0012x(0) y(0)\u0013 ;we can create an in nite sequence of vectorsv(1);v(2);v(3);and so on using the rule: v(t+ 1) =Mv(t) for all natural numbers t: (This is known as a discrete dynamical system whose initial condition isv(0):) (a) Find all eigenvectors and eigenvalues of M: (b) Find all vectors v(0) such that v(0) =v(1) =v(2) =v(3) =\u0001\u0001\u0001 (Such a vector is known as a xed point of the dynamical system.) (c) Find all vectors v(0) such that v(0);v(1);v(2);v(3);:::all point in the same direction. (Any such vector describes an invariant curve of the dynamical system.) Hint 240\n",
      "375 (143) LetPn(t) be the vector space of polynomials of degree nor less, and d dt:Pn(t)!Pn(t) be the derivative operator. Find the matrix of d dtin the ordered bases E= (1;t;:::;tn) for the domain and F= 24813.4 Review Problems 249 (tn;:::;;t; 1) for the codomain. Determine if this derivative operator is diagonalizable. Recall from Chapter 6 that the derivative operator is linear .\n",
      "762 (144) When writing a matrix for a linear transformation, we have seen that the choice of basis matters. In fact, even the order of the basis matters! (a) Write all possible reorderings of the standard basis ( e1;e2;e3) forR3. (b) Write each change of basis matrix between the standard basis and each of its reorderings. Make as many observations as you can about these matrices. what are their entries? Do you notice anything about how many of each type of entry appears in each row and column? What are their determinants? (Note: These matrices are known as permutation matrices .) (c) GivenL:R3!R3is linear and L0 @x y z1 A=0 @2y\u0000z 3x 2z+x+y1 A write the matrix MforLin the standard basis, and two reorder- ings of the standard basis. How are these matrices related?\n",
      "444 (145) Let X=f~;|;g; Y =f\u0003;?g: Write down two di erent ordered bases, S;S0andT;T0respectively, for each of the vector spaces RXandRY. Find the change of basis matricesPandQthat map these bases to one another. Now consider the map `:Y!X; where`(\u0003) =~and`(?) =. Show that `can be used to de ne a linear transformation L:RX!RY. Compute the matrices Mand M0ofLin the bases S;T and thenS0;T0. Use your change of basis matricesPandQto check that M0=Q\u00001MP.\n",
      "157 (146) Recall that tr MN = trNM. Use this fact to show that the trace of a square matrix Mdoes not depend on the basis you used to compute M. 249250 Diagonalization\n",
      "80 (147) When is the 2\u00022 matrix\u0012a b c d\u0013 diagonalizable? Include examples in your answer.\n",
      "939 (148) Show that similarity of matrices is an equivalence relation . (The de - nition of an equivalence relation is given in the background WeBWorK set.) 7.Jordan form â€¢Can the matrix\u0012\u00151 0\u0015\u0013 be diagonalized? Either diagonalize it or explain why this is impossible. â€¢Can the matrix0 @\u00151 0 0\u00151 0 0\u00151 Abe diagonalized? Either diagonalize it or explain why this is impossible. â€¢Can then\u0002nmatrix0 BBBBBBB@\u00151 0\u0001\u0001\u0001 0 0 0\u00151\u0001\u0001\u0001 0 0 0 0\u0015\u0001\u0001\u0001 0 0 .................. 0 0 0\u0001\u0001\u0001\u00151 0 0\u0001\u0001\u0001 0\u00151 CCCCCCCAbe diagonalized? Either diagonalize it or explain why this is impossible. Note: It turns out that every matrix is similar to a block ma- trix whose diagonal blocks look like diagonal matrices or the ones above and whose o -diagonal blocks are all zero. This is called theJordan form of the matrix and a (maximal) block that looks like 0 BBBBB@\u00151 0\u0001\u0001\u0001 0 0\u00151 0 ......... \u00151 0 0 0 \u00151 CCCCCA is called a Jordann-cell or a Jordan block wherenis the size of the block.\n",
      "534 (149) LetAandBbe commuting matrices ( i.e.,AB=BA) and suppose thatAhas an eigenvector vwith eigenvalue \u0015. 25013.4 Review Problems 251 (a) Show that Bvis also an eigenvector of Awith eigenvalue \u0015. (b) Additionally suppose that Ais diagonalizable with distinct eigen- values. What is the dimension of each eigenspace of A? (c) Show that vis also an eigenvector of B. (d) Explain why this shows that AandBcan be simultaneously diago- nalized (i.e.there is an ordered basis in which both their matrices are diagonal). 251252 Diagonalization 252\n",
      "360 (150) LetD=\u0012\u001510 0\u00152\u0013 . (a) WriteDin terms of the vectors e1ande2, and their transposes. (b) Suppose P=\u0012a b c d\u0013 is invertible. Show that Dis similar to M=1 ad\u0000bc\u0012\u00151ad\u0000\u00152bc\u0000(\u00151\u0000\u00152)ab (\u00151\u0000\u00152)cd\u0000\u00151bc+\u00152ad\u0013 : 27214.7 Review Problems 273 (c) Suppose the vectors\u0000 a;b\u0001 and\u0000 c;d\u0001 are orthogonal. What can you say about Min this case? (Hint: think about what MTis equal to.)\n",
      "205 (151) Suppose S=fv1;:::;vngis an orthogonal (not orthonormal) basis forRn. Then we can write any vector vasv=P icivifor some constantsci. Find a formula for the constants ciin terms of vand the vectors inS. Hint\n",
      "385 (152) Letu;vbe linearly independent vectors in R3, andP= spanfu;vgbe the plane spanned by uandv. (a) Is the vector v?:=v\u0000u\u0001v u\u0001uuin the plane P? (b) What is the (cosine of the) angle between v?andu? (c) How can you nd a third vector perpendicular to both uandv?? (d) Construct an orthonormal basis for R3fromuandv. (e) Test your abstract formul\u001a starting with u=\u0000 1;2;0\u0001 andv=\u0000 0;1;1\u0001 : Hint\n",
      "842 (153) Find an orthonormal basis for R4which includes (1 ;1;1;1) using the following procedure: (a) Pick a vector perpendicular to the vector v1=0 BB@1 1 1 11 CCA 273274 Orthonormal Bases and Complements from the solution set of the matrix equation vT 1x= 0: Pick the vector v2obtained from the standard Gaussian elimina- tion procedure which is the coe\u000ecient of x2. (b) Pick a vector perpendicular to both v1andv2from the solutions set of the matrix equation \u0012vT 1 vT 2\u0013 x= 0: Pick the vector v3obtained from the standard Gaussian elimina- tion procedure with x3as the coe\u000ecient. (c) Pick a vector perpendicular to v1;v2;andv3from the solution set of the matrix equation 0 B@vT 1 vT 2 vT 31 CAx= 0: Pick the vector v4obtained from the standard Gaussian elimina- tion procedure with x3as the coe\u000ecient. (d) Normalize the four vectors obtained above.\n",
      "152 (154) Use the inner product f\u0001g:=Z1 0f(x)g(x)dx on the vector space V= spanf1;x;x2;x3gto perform the Gram-Schmidt procedure on the set of vectors f1;x;x2;x3g.\n",
      "281 (155) Use the inner product f\u0001g:=Z2\u0019 0f(x)g(x)dx on the vector space V= spanfsin(x);sin(2x);sin(3x)gto perform the Gram-Schmidt procedure on the set of vectors fsin(x);sin(2x);sin(3x)g. Try to build an orthonormal basis for the vector space spanfsin(nx)jn2Ng: 27414.7 Review Problems 275\n",
      "434 (156) (a) Show that if Qis an orthogonal n\u0002nmatrix, then uv= (Qu)(Qv); for anyu;v2Rn. That is,Qpreserves the inner product. (b) DoesQpreserve the outer product? (c) If the set of vectors fu1;:::;ungis orthonormal and f\u00151;\u0001\u0001\u0001;\u0015ng is a set of numbers, then what are the eigenvalues and eigenvectors of the matrix M=Pn i=1\u0015iuiuT i? (d) How would the eigenvectors and eigenvalues of this matrix change if we replacedfu1;:::;ungbyfQu1;:::;Qu ng?\n",
      "225 (157) Carefully write out the Gram-Schmidt procedure for the set of vectors 8 < :0 @1 1 11 A;0 @1 \u00001 11 A;0 @1 1 \u000011 A9 = ;: Is it possible to rescale the second vector obtained in the procedure to a vector with integer components?\n",
      "270 (158) (a) Suppose uandvare linearly independent. Show that uandv? are also linearly independent. Explain why fu;v?gis a basis for spanfu;vg. Hint (b) Repeat the previous problem, but with three independent vectors u;v;w wherev?andw?are as de ned by the Gram-Schmidt procedure.\n",
      "56 (159) Find the QRfactorization of M=0 @1 0 2 \u00001 2 0 \u00001\u00002 21 A:\n",
      "123 (160) Given any three vectors u;v;w , when dov?orw?of the Gram{Schmidt procedure vanish? 275276 Orthonormal Bases and Complements\n",
      "81 (161) ForUa subspace of W, use the subspace theorem to check that U?is a subspace of W.\n",
      "283 (162) LetSnandAnde ne the space of n\u0002nsymmetric and anti-symmetric matrices, respectively. These are subspaces of the vector space Mn nof alln\u0002nmatrices. What is dim Mn n, dimSn, and dimAn? Show that Mn n=Sn+An. De ne an inner product on square matrices M\u0001N= trMN: IsA? n=Sn? IsMn n=SAn?\n",
      "226 (163) The vector space V= spanfsin(t);sin(2t);sin(3t);sin(3t)ghas an inner product: f\u0001g:=Z2\u0019 0f(t)g(t)dt: Find the orthogonal compliment to U= spanfsin(t) + sin(2t)ginV. Express sin( t)\u0000sin(2t) as the sum of vectors from UandU?. 276\n",
      "579 (164) Consider an arbitrary matrix M:Rm!Rn: (a) Argue that Mx= 0 if only if xis perpendicular to all columns ofMT. (b) Argue that Mx= 0 if only if xis perpendicular to all of the linear combinations of the columns of MT. (c) Argue that ker Mis perpendicular to ran MT. (d) Argue further Rm= kerranMT. (e) Argue analogously that Rn= kerMranM. The equations in the last two parts describe how a linear transforma- tionM:Rm!Rndetermines orthogonal decompositions of both it's domain and target. This result sometimes goes by the humble name The Fundamental Theorem of Linear Algebra .\n",
      "507 (165) LetL:V!Wbe a linear transformation. Show that ker L=f0Vgif and only if Lis one-to-one: (a)(Trivial kernel)injective.) Suppose that ker L=f0Vg. Show thatLis one-to-one. Think about methods of proof{does a proof by contradiction, a proof by induction, or a direct proof seem most appropriate? (b)(Injective)trivial kernel.) Now suppose that Lis one-to-one. Show that ker L=f0Vg. That is, show that 0 Vis in kerL, and then show that there are no other vectors in ker L. 299300 Kernel, Range, Nullity, Rank Hint\n",
      "107 (166) Letfv1;:::;vngbe a basis for VandL:V!Wis a linear function. Carefully explain why L(V) = spanfLv1;:::;Lvng:\n",
      "482 (167) Suppose L:R4!R3whose matrix Min the standard basis is row equivalent to the following matrix: 0 @1 0 0\u00001 0 1 0 1 0 0 1 11 A= RREF(M)\u0018M: (a)Explain why the rst three columns of the original matrix Mform a basis forL(R4). (b)Find and describe an algorithm ( i.e., a general procedure) for computing a basis for L(Rn) whenL:Rn!Rm. (c)Useyour algorithm to nd a basis for L(R4) whenL:R4!R3is the linear transformation whose matrix Min the standard basis is 0 @2 1 1 4 0 1 0 5 4 1 1 61 A:\n",
      "255 (168) Claim: Iffv1;:::;vngis a basis for ker L, whereL:V!W, then it is always possible to extend this set to a basis for V. Choose some simple yet non-trivial linear transformations with non- trivial kernels and verify the above claim for those transformations.\n",
      "619 (169) LetPn(x) be the space of polynomials in xof degree less than or equal ton, and consider the derivative operator d dx:Pn(x)!Pn(x): 30016.4 Review Problems 301 Find the dimension of the kernel and image of this operator. What happens if the target space is changed to Pn\u00001(x) orPn+1(x)? Now consider P2(x;y), the space of polynomials of degree two or less inxandy. (Recall how degree is counted; xyis degree two, yis degree one andx2yis degree three, for example.) Let L:=@ @x+@ @y:P2(x;y)!P2(x;y): (For example, L(xy) =@ @x(xy) +@ @y(xy) =y+x.) Find a basis for the kernel ofL. Verify the dimension formula in this case.\n",
      "597 (170) Lets demonstrate some ways the dimension formula can break down if a vector space is in nite dimensional. (a) Let R[x] be the vector space of all polynomials in the variable x with real coe\u000ecients. Let D=d dxbe the usual derivative operator. Show that the range of DisR[x]. What is ker D? Hint: Use the basis fxnjn2Ng. (b) LetL:R[x]!R[x] be the linear map L(p(x)) =xp(x): What is the kernel and range of M? (c) LetVbe an in nite dimensional vector space and L:V!Vbe a linear operator. Suppose that dim ker L<1, show that dim L(V) is in nite. Also show that when dim L(V)<1that dim ker Lis in nite.\n",
      "1692 (171) This question will answer the question, \\If I choose a bit vector at random , what is the probability that it lies in the span of some other vectors?\" i:Given a collection Sofkbit vectors in B3, consider the bit ma- trixMwhose columns are the vectors in S. Show that Sis linearly independent if and only if the kernel of Mis trivial, namely the set kerM=fv2B3jMv= 0gcontains only the zero vector. 301302 Kernel, Range, Nullity, Rank ii:Give some method for choosing a random bit vector vinB3. Sup- poseSis a collection of 2 linearly independent bit vectors in B3. How can we tell whether S[fvgis linearly independent? Do you think it is likely or unlikely that S[fvgis linearly independent? Explain your reasoning. iii:IfPis the characteristic polynomial of a 3 \u00023 bit matrix, what must the degree of Pbe? Given that each coe\u000ecient must be either 0 or 1, how many possibilities are there for P? How many of these possible characteristic polynomials have 0 as a root? If M is a 3\u00023 bit matrix chosen at random, what is the probability that it has 0 as an eigenvalue? (Assume that you are choosing a random matrixMin such a way as to make each characteristic polynomial equally likely.) What is the probability that the columns of M form a basis for B3? (Hint: what is the relationship between the kernel ofMand its eigenvalues?) Note: We could ask the same question for real vectors: If I choose a real vector at random, what is the probability that it lies in the span of some other vectors? In fact, once we write down a reasonable way of choosing a random real vector, if I choose a real vector in Rnat random, the probability that it lies in the span of n\u00001 other real vectors is zero! 302\n",
      "204 (172) LetL:U!Vbe a linear transformation. Suppose v2L(U) and you have found a vector upsthat obeys L(ups) =v. Explain why you need to compute ker Lto describe the solution set of the linear system L(u) =v. Hint\n",
      "281 (173) Suppose that Mis anm\u0002nmatrix with trivial kernel. Show that for any vectors uandvinRm: â€¢uTMTMv=vTMTMu. 31217.3 Review Problems 313 â€¢vTMTMv\u00150. In case you are concerned (you don't need to be) and for future reference, the notation v\u00150 means each component vi\u00150. â€¢IfvTMTMv= 0, thenv=\n",
      "67 (175) Rewrite the Gram-Schmidt algorithm in terms of projection matrices.\n",
      "140 (176) Show that if v1;:::;vkare linearly independent that the matrix M= (v1\u0001\u0001\u0001vk) is not necessarily invertible but the matrix MTMis invert- ible.\n",
      "243 (177) Write out the singular value decomposition theorem of a 3 \u00021, a 3\u00022, and a 3\u00023 symmetric matrix. Make it so that none of the components of your matrices are zero but your computations are simple. Explain why you choose the matrices you choose.\n",
      "267 (178) Find the best polynomial approximation to a solution to the di erential equationd dxf=x+x2by considering the derivative to have domain and codomain span f1;x;x2g. (Hint: Begin by de ning bases for the domain and codomain.) 313314 Least squares and Singular Values 314\n",
      "251 (179) Solve the following linear system. Write the solution set in vector form. Check your solution. Write one particular solution and one homogeneous solution, if they exist. What does the solution set look like geometrically? x+ 3y = 4 x\u00002y+z= 1 2x+y+z= 5\n",
      "498 (180) Consider the system of equations 8 >>>>>< >>>>>:x\u0000z+ 2w=\u00001 x+y+z\u0000w= 2 \u0000y\u00002z+ 3w=\u00003 5x+ 2y\u0000z+ 4w= 1 (a) Write an augmented matrix for this system. (b) Use elementary row operations to nd its reduced row echelon form. (c) Write the solution set for the system in the form S=fX0+X i\u0016iYi:\u0016i2Rg: 321322 Sample First Midterm (d) What are the vectors X0andYicalled andwhich matrix equations do they solve? (e) Check separately that X0and eachYisolve the matrix systems you claimed they solved in part (d).\n",
      "87 (181) Use row operations to invert the matrix 0 BB@1 2 3 4 2 4 7 11 3 7 14 25 4 11 25 501 CCA\n",
      "108 (182) LetM=\u00122 1 3\u00001\u0013 . Calculate MTM\u00001. IsMsymmetric? What is the trace of the transpose of f(M), wheref(x) =x2\u00001?\n",
      "365 (183) In this problem Mis the matrix M=\u0012cos\u0012sin\u0012 \u0000sin\u0012cos\u0012\u0013 andXis the vector X=\u0012x y\u0013 : Calculate all possible dot products between the vectors XandMX. Com- pute the lengths of XandMX. What is the angle between the vectors MX andX. Draw a picture of these vectors in the plane. For what values of \u0012 do you expect equality in the triangle and Cauchy{Schwartz inequalities?\n",
      "212 (184) LetMbe the matrix 0 BBBBBB@1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 11 CCCCCCA Find a formula for Mkfor any positive integer power k. Try some simple examples like k= 2;3 if confused.\n",
      "208 (185) What does it mean for a function to be linear? Check that integration is a linear function from VtoV, whereV=ff:R!Rjfis integrablegis a vector space over Rwith usual addition and scalar multiplication. 322323\n",
      "1282 (186) What are the four main things we need to de ne for a vector space? Which of the following is a vector space over R? For those that are not vector spaces, modify one part of the de nition to make it into a vector space. (a)V=f2\u00022 matrices with entries in Rg, usual matrix addition, and k\u0001\u0012a b c d\u0013 =\u0012ka b kc d\u0013 fork2R. (b)V=fpolynomials with complex coe\u000ecients of degree \u00143g, with usual addition and scalar multiplication of polynomials. (c)V=fvectors in R3with at least one entry containing a 1 g, with usual addition and scalar multiplication. 9.Subspaces: IfVis a vector space, we say that Uis asubspace ofVwhen the setUis also a vector space, using the vector addition and scalar multiplica- tion rules of the vector space V. (Remember that U\u001aVsays that \\Uis a subset ofV\",i.e., all elements of Uare also elements of V. The symbol8 means \\for all\" and 2means \\is an element of\".) Explain why additive closure ( u+w2U8u;v2U) and multiplicative closure (r:u2U8r2R,u2V) ensure that (i) the zero vector 0 2Uand (ii) everyu2Uhas an additive inverse. In fact it su\u000eces to check closure under addition and scalar multiplication to verify that Uis a vector space. Check whether the following choices of U are vector spaces: (a)U=8 < :0 @x y 01 A:x;y2R9 = ; (b)U=8 < :0 @1 0 z1 A:z2R9 = ;\n",
      "846 (187) Find an LU decomposition for the matrix 0 BB@1 1\u00001 2 1 3 2 2 \u00001\u00003\u00004 6 0 4 7\u000021 CCA 323324 Sample First Midterm Use your result to solve the system 8 >>>< >>>:x+y\u0000z+ 2w= 7 x+ 3y+ 2z+ 2w= 6 \u0000x\u00003y\u00004z+ 6w= 12 4y+ 7z\u00002w=\u00007 Solutions 1.As an additional exercise, write out the row operations above the \u0018signs below. 0 B@1 3 0 4 1\u00002 1 1 2 1 1 51 CA\u00180 B@1 3 0 4 0\u00005 1\u00003 0\u00005 1\u000031 CA\u00180 B@1 03 511 5 0 1\u00001 53 5 0 0 0 01 CA: Solution set is 8 < :0 @x y z1 A=0 @11 5 3 5 01 A+\u00160 @\u00003 51 5 11 A:\u00162R9 = ;: Geometrically this represents a line in R3through the point0 @11 5 3 5 01 Arunning parallel to the vector0 B@\u00003 5 1 5 11 CA. The vector0 @11 5 3 5 01 Aisaparticular solution and0 @\u00003 51 5 11 Aisahomogeneous solution. As a double check note that 0 @1 3 0 1\u00002 1 2 1 11 A0 @11 53 5 01 A=0 @4 1 51 Aand0 @1 3 0 1\u00002 1 2 1 11 A0 @\u00003 51 5 11 A=0 @0 0 01 A: 324325\n",
      "1517 (188) (a) The augmented matrix 0 BBB@1 0\u00001 2\u00001 1 1 1\u00001 2 0\u00001\u00002 3\u00003 5 2\u00001 4 11 CCCA encodes the system of equations. (b)Again, write out the row operations as an additional exercise. The above augmented matrix is row equivalent to 0 BBB@1 0\u00001 2\u00001 0 1 2\u00003 3 0\u00001\u00002 3\u00003 0 2 4\u00006 61 CCCA\u00180 BBB@1 0\u00001 2\u00001 0 1 2\u00003 3 0 0 0 0 0 0 0 0 0 01 CCCA which is in reduced row echelon form. (c) Solution set is 8 >>< >>:X=0 BB@\u00001 3 0 01 CCA+\u001610 BB@1 \u00002 1 01 CCA+\u001620 BB@\u00002 3 0 11 CCA:\u00161;\u001622R9 >>= >>;: (d) The vector X0=0 BB@\u00001 3 0 01 CCAisaparticular solution and the vectors Y1=0 BB@1 \u00002 1 01 CCAandY2=0 BB@\u00002 3 0 11 CCA are homogeneous solutions. They obey MX =V ; MY 1= 0 =MY 2: where M=0 BB@1 0\u00001 2 1 1 1\u00001 0\u00001\u00002 3 5 2\u00001 41 CCAandV=0 BB@\u00001 2 \u00003 11 CCA: 325326 Sample First Midterm (e) This amounts to explicitly performing the matrix manipulations MX\u0000V; MY 1;andMY 2 to verify that they are all zero vectors. 3.As usual, be sure to write out the row operations above the \u0018's so your work can be easily checked. 0 BB@1 2 3 4 1 0 0 0 2 4 7 11 0 1 0 0 3 7 14 25 0 0 1 0 4 11 25 50 0 0 0 11 CCA \u00180 BB@1 2 3 4 1 0 0 0 0 0 1 3\u00002 1 0 0 0 1 5 13\u00003 0 1 0 0 3 13 34\u00004 0 0 11 CCA \u00180 BB@1 0\u00007\u000022 7 0\u00002 0 0 1 5 13 \u00003 0 1 0 0 0 1 3 \u00002 1 0 0 0 0\u00002\u00005 5 0\u00003 11 CCA \u00180 BB@1 0 0\u00001\u00007 7\u00002 0 0 1 0\u00002 7\u00005 1 0 0 0 1 3\u00002 1 0 0 0 0 0 1 1 2\u00003 11 CCA \u00180 BB@1 0 0 0\u00006 9\u00005 1 0 1 0 0 9\u00001\u00005 2 0 0 1 0\u00005\u00005 9\u00003 0 0 0 1 1 2\u00003 11 CCA: Check 0 BB@1 2 3 4 2 4 7 11 3 7 14 25 4 11 25 501 CCA0 BB@\u00006 9\u00005 1 9\u00001\u00005 2 \u00005\u00005 9\u00003 1 2\u00003 11 CCA=0 BB@1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 11 CCA:\n",
      "214 (189) MTM\u00001=\u00122 3 1\u00001\u0013 1 51 5 3 5\u00002 5! =\u001211 5\u00004 5 \u00002 53 5\u0013 : 326327 SinceMTM\u000016=I, it follows MT6=MsoMisnotsymmetric. Finally trf(M)T= trf(M) = tr(M2\u0000I) = tr\u00122 1 3\u00001\u0013\u00122 1 3\u00001\u0013 \u0000trI = (2\u00012 + 1\u00013) + (3\u00011 + (\u00001)\u0001(\u00001))\u00002 = 9:\n",
      "639 (190) First X(MX) =XTMX =\u0000 x y\u0001\u0012cos\u0012sin\u0012 \u0000sin\u0012cos\u0012\u0013\u0012x y\u0013 =\u0000 x y\u0001\u0012xcos\u0012+ysin\u0012 \u0000xsin\u0012+ycos\u0012\u0013 = (x2+y2) cos\u0012: NowjjXjj=p XX=p x2+y2and (MX)(MX) =XMTMX. But MTM=\u0012cos\u0012\u0000sin\u0012 sin\u0012 cos\u0012\u0013\u0012cos\u0012sin\u0012 \u0000sin\u0012cos\u0012\u0013 =\u0012cos2\u0012+ sin2\u0012 0 0 cos2\u0012+ sin2\u0012\u0013 =I: HencejjMXjj=jjXjj=p x2+y2. Thus the cosine of the angle between X andMX is given by X(MX) jjXjjjjMXjj=(x2+y2) cos\u0012p x2+y2p x2+y2= cos\u0012: In other words, the angle is \u0012OR\u0000\u0012. You should draw two pictures, one where the angle between XandMX is\u0012, the other where it is \u0000\u0012. For Cauchy{Schwartz,jX(MX)j jjXjjjjMXjj=jcos\u0012j= 1 when \u0012= 0;\u0019. For the triangle equality MX =XachievesjjX+MXjj=jjXjj+jjMXjj, which requires\u0012=\n",
      "367 (191) 6. This is a block matrix problem. Notice the that matrix Mis really just M=\u0012I I 0I\u0013 , whereIand 0 are the 3\u00023 identity zero matrices, respectively. But M2=\u0012I I 0I\u0013\u0012I I 0I\u0013 =\u0012I2I 0I\u0013 and M3=\u0012I I 0I\u0013\u0012I2I 0I\u0013 =\u0012I3I 0I\u0013 327328 Sample First Midterm so,Mk=\u0012I kI 0I\u0013 , or explicitly Mk=0 BBBBBB@1 0 0k0 0 0 1 0 0 k0 0 0 1 0 0 k 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 11 CCCCCCA:\n",
      "429 (192) We can call a function f:V\u0000!Wlinear if the setsVandWare vector spaces and fobeys f( u+ v) = f(u) + f(v); for allu;v2Vand ; 2R. Now, integration is a linear transformation from the space Vof all inte- grable functions (don't be confused between the de nition of a linear func- tion above, and integrable functions f(x) which here are the vectors in V) to the real numbers R, becauseR1 \u00001( f(x) + g(x))dx= R1 \u00001f(x)dx+ R1 \u00001g(x)dx.\n",
      "1020 (193) The four main ingredients are (i) a set Vof vectors, (ii) a number eld K (usuallyK=R), (iii) a rule for adding vectors (vector addition) and (iv) a way to multiply vectors by a number to produce a new vector (scalar multiplication). There are, of course, ten rules that these four ingredients must obey. (a) This is not a vector space. Notice that distributivity of scalar multi- plication requires 2 u= (1 + 1)u=u+ufor any vector ubut 2\u0001\u0012a b c d\u0013 =\u00122a b 2c d\u0013 which does notequal \u0012a b c d\u0013 +\u0012a b c d\u0013 =\u00122a2b 2c2d\u0013 : This could be repaired by taking k\u0001\u0012a b c d\u0013 =\u0012ka kb kc kd\u0013 : 328329 (b) This is a vector space. Although, the question does not ask you to, it is a useful exercise to verify that all ten vector space rules are satis ed. (c) This is not a vector space for many reasons. An easy one is that (1;\u00001;0) and (\u00001;1;0) are both in the space, but their sum (0 ;0;0) is not ( i.e., additive closure fails). The easiest way to repair this would be to drop the requirement that there be at least one entry equaling\n",
      "394 (194) 9. (i) Thanks to multiplicative closure, if u2U, so is (\u00001)\u0001u. But (\u00001)\u0001u+u= (\u00001)\u0001u+1\u0001u= (\u00001+1)\u0001u= 0:u= 0 (at each step in this chain of equalities we have used the fact that Vis a vector space and therefore can use its vector space rules). In particular, this means that the zero vector of Vis inUand is its zero vector also. (ii) Also, in V, for eachuthere is an element \u0000u such thatu+ (\u0000u) =\n",
      "1527 (195) But by additive close, ( \u0000u) must also be in U, thus everyu2Uhas an additive inverse. (a) This is a vector space. First we check additive closure: let0 @x y 01 Aand 0 @z w 01 Abe arbitrary vectors in U. But since0 @x y 01 A+0 @z w 01 A=0 @x+z y+w 01 A, so is their sum (because vectors in Uare those whose third component vanishes). Multiplicative closure is similar: for any 2R, 0 @x y 01 A= 0 @ x y 01 A, which also has no third component, so is in U. (b) This is not a vector space for various reasons. A simple one is that u=0 @1 0 z1 Ais inUbut the vector u+u=0 @2 0 2z1 Ais not inU(it has a 2 in the rst component, but vectors in Ualways have a 1 there). 10.0 BB@1 1\u00001 2 1 3 2 2 \u00001\u00003\u00004 6 0 4 7\u000021 CCA=0 BB@1 0 0 0 1 1 0 0 \u00001 0 1 0 0 0 0 11 CCA0 BB@1 1\u00001 2 0 2 3 0 0\u00002\u00005 8 0 4 7\u000021 CCA 329330 Sample First Midterm =0 BB@1 0 0 0 1 1 0 0 \u00001\u00001 1 0 0 2 0 11 CCA0 BB@1 1\u00001 2 0 2 3 0 0 0\u00002 8 0 0 1\u000021 CCA =0 BB@1 0 0 0 1 1 0 0 \u00001\u00001 1 0 0 2\u00001 211 CCA0 BB@1 1\u00001 2 0 2 3 0 0 0\u00002 8 0 0 0 21 CCA: To solveMX =VusingM=LUwe rst solve LW=Vwhose augmented matrix reads 0 BB@1 0 0 0 7 1 1 0 0 6 \u00001\u00001 1 0 12 0 2\u00001 21\u000071 CCA\u00180 BB@1 0 0 0 7 0 1 0 0\u00001 0 0 1 0 18 0 2\u00001 21\u000071 CCA \u00180 BB@1 0 0 0 7 0 1 0 0\u00001 0 0 1 0 18 0 0 0 1 41 CCA; from which we can read o W. Now we compute Xby solvingUX=W with the augmented matrix 0 BB@1 1\u00001 2 7 0 2 3 0\u00001 0 0\u00002 8 18 0 0 0 2 41 CCA\u00180 BB@1 1\u00001 2 7 0 2 3 0\u00001 0 0\u00002 0 2 0 0 0 1 21 CCA \u00180 BB@1 1\u00001 2 7 0 2 0 0 2 0 0 1 0\u00001 0 0 0 1 21 CCA\u00180 BB@1 0 0 0 1 0 1 0 0 1 0 0 1 0\u00001 0 0 0 1 21 CCA: Sox= 1,y= 1,z=\u00001 andw=\n",
      "317 (196) 330E Sample Second Midterm Here are some worked problems typical for what you might expect on a second midterm examination. 1.Determinants: The determinant det Mof a 2\u00022 matrixM=\u0012a b c d\u0013 is de ned by detM=ad\u0000bc: (a) For which values of det MdoesMhave an inverse? (b) Write down all 2 \u00022 bit matrices with determinant\n",
      "102 (197) (Remember bits are either 0 or 1 and 1 + 1 = 0.) (c) Write down all 2 \u00022 bit matrices with determinant\n",
      "148 (198) (d) Use one of the above examples to show why the following statement is FALSE. Square matrices with the same determinant are always row equivalent.\n",
      "213 (199) Let A=0 BB@1 1 1 2 2 3 4 5 61 CCA: Compute det A. Find all solutions to (i) AX= 0 and (ii) AX=0 @1 2 31 Afor 331332 Sample Second Midterm the vectorX2R3. Find, but do not solve, the characteristic polynomial of A.\n",
      "556 (200) LetMbe any 2\u00022 matrix. Show detM=\u00001 2trM2+1 2(trM)2: 4.The permanent: LetM= (Mi j) be ann\u0002nmatrix. An operation producing a single number from Msimilar to the determinant is the \\permanent\" permM=X \u001bM1 \u001b(1)M2 \u001b(2)\u0001\u0001\u0001Mn \u001b(n): For example perm\u0012a b c d\u0013 =ad+bc: Calculate perm0 @1 2 3 4 5 6 7 8 91 A: What do you think would happen to the permanent of an n\u0002nmatrixM if (include a brief explanation with each answer): (a) You multiplied Mby a number \u0015. (b) You multiplied a row of Mby a number \u0015. (c) You took the transpose of M. (d) You swapped two rows of M.\n",
      "116 (201) LetXbe ann\u00021 matrix subject to XTX= (1); and de ne H=I\u00002XXT; (whereIis then\u0002nidentity matrix). Show H=HT=H\u00001: 332333\n",
      "291 (202) Suppose \u0015is an eigenvalue of the matrix Mwith associated eigenvector v. Isvan eigenvector of Mk(wherekis any positive integer)? If so, what would the associated eigenvalue be? Now suppose that the matrix Nisnilpotent ,i.e. Nk= 0 for some integer k\u00152. Show that 0 is the only eigenvalue of N.\n",
      "1236 (203) LetM= 3\u00005 1\u00003! . Compute M12. (Hint: 212= 4096.) 8.The Cayley Hamilton Theorem : Calculate the characteristic polynomial PM(\u0015) of the matrix M=\u0012a b c d\u0013 . Now compute the matrix polynomial PM(M). What do you observe? Now suppose the n\u0002nmatrixAis \\similar\" to a diagonal matrix D, in other words A=P\u00001DP for some invertible matrix PandDis a matrix with values \u00151,\u00152;:::\u0015n along its diagonal. Show that the two matrix polynomials PA(A) andPA(D) are similar ( i.e.PA(A) =P\u00001PA(D)P). Finally, compute PA(D), what can you say about PA(A)? 9.De ne what it means for a set Uto be a subspace of a vector space V. Now letUandWbe non-trivial subspaces of V. Are the following also subspaces? (Remember that [means \\union\" and \\means \\intersection\".) (a)U[W (b)U\\W In each case draw examples in R3that justify your answers. If you answered \\yes\" to either part also give a general explanation why this is the case. 10.De ne what it means for a set of vectors fv1;v2;:::;vngto (i) be linearly independent, (ii) span a vector space Vand (iii) be a basis for a vector spaceV. Consider the following vectors in R3 u=0 @\u00001 \u00004 31 A; v =0 @4 5 01 A; w =0 @10 7 h+ 31 A: For which values of hisfu;v;wga basis for R3? 333334 Sample Second Midterm Solutions\n",
      "774 (205) (b) Unit determinant bit matrices: \u00121 0 0 1\u0013 ;\u00121 1 0 1\u0013 ;\u00121 0 1 1\u0013 ;\u00120 1 1 0\u0013 ;\u00121 1 1 0\u0013 ;\u00120 1 1 1\u0013 : (c) Bit matrices with vanishing determinant: \u00120 0 0 0\u0013 ;\u00121 0 0 0\u0013 ;\u00120 1 0 0\u0013 ;\u00120 0 1 0\u0013 ;\u00120 0 0 1\u0013 ; \u00121 1 0 0\u0013 ;\u00120 0 1 1\u0013 ;\u00121 0 1 0\u0013 ;\u00120 1 0 1\u0013 ;\u00121 1 1 1\u0013 : As a check, count that the total number of 2\u00022bit matrices is 2(number of entries)= 24= 16 . (d) To disprove this statement, we just need to nd a single counterexam- ple. All the unit determinant examples above are actually row equiva- lent to the identity matrix, so focus on the bit matrices with vanishing determinant. Then notice (for example), that \u00121 1 0 0\u0013 \u0018=\u00120 0 0 0\u0013 : So we have found a pair of matrices that are not row equivalent but do have the same determinant. It follows that the statement is false.\n",
      "123 (206) detA= 1:(2:6\u00003:5)\u00001:(2:6\u00003:4) + 1:(2:5\u00002:4) =\u00001: (i) Since det A6= 0, the homogeneous system AX= 0 only has the solution X=\n",
      "317 (207) (ii) It is e\u000ecient to compute the adjoint adjA=0 @\u00003 0 2 \u00001 2\u00001 1\u00001 01 AT =0 @\u00003\u00001 1 0 2\u00001 2\u00001 01 A Hence A\u00001=0 @3 1\u00001 0\u00002 1 \u00002 1 01 A: 334335 Thus X=0 @3 1\u00001 0\u00002 1 \u00002 1 01 A0 @1 2 31 A=0 @2 \u00001 01 A: Finally, PA(\u0015) =\u0000det0 @1\u0000\u0015 1 1 2 2\u0000\u0015 3 4 5 6\u0000\u00151 A =\u0000h (1\u0000\u0015)[(2\u0000\u0015)(6\u0000\u0015)\u000015]\u0000[2:(6\u0000\u0015)\u000012] + [10\u00004:(2\u0000\u0015)]i =\u00153\u00009\u00152\u0000\u0015+ 1:\n",
      "170 (208) CallM=\u0012a b c d\u0013 . Then detM=ad\u0000bc, yet \u00001 2trM2+1 2(trM)2=\u00001 2tr\u0012a2+bc\u0003 \u0003bc+d2\u0013 \u00001 2(a+d)2 =\u00001 2(a2+ 2bc+d2) +1 2(a2+ 2ad+d2) =ad\u0000bc; which is what we were asked to show.\n",
      "1231 (209) perm0 @1 2 3 4 5 6 7 8 91 A= 1\u0001(5\u00019 + 6\u00018) + 2\u0001(4\u00019 + 6\u00017) + 3\u0001(4\u00018 + 5\u00017) = 450: (a) Multiplying Mby\u0015replaces every matrix element Mi \u001b(j)in the formula for the permanent by \u0015Mi \u001b(j), and therefore produces an overall factor \u0015n. (b) Multiplying the ithrow by\u0015replacesMi \u001b(j)in the formula for the permanent by \u0015Mi \u001b(j). Therefore the permanent is multiplied by an overall factor \u0015. (c) The permanent of a matrix transposed equals the permanent of the original matrix, because in the formula for the permanent this amounts to summing over permutations of rows rather than columns. But we could then sort the product M\u001b(1) 1M\u001b(2) 2:::M\u001b(n) nback into its original order using the inverse permutation \u001b\u00001. But summing over permuta- tions is equivalent to summing over inverse permutations, and therefore the permanent is unchanged. 335336 Sample Second Midterm (d) Swapping two rows also leaves the permanent unchanged. The argu- ment is almost the same as in the previous part, except that we need only reshu\u000fe two matrix elements Mj \u001b(i)andMi \u001b(j)(in the case where rowsiandjwere swapped). Then we use the fact that summing over all permutations \u001bor over all permutations e\u001bobtained by swapping a pair in\u001bare equivalent operations.\n",
      "276 (210) Firstly, lets call (1) = 1 (the 1 \u00021 identity matrix). Then we calculate HT= (I\u00002XXT)T=IT\u00002(XXT)T=I\u00002(XT)TXT=I\u00002XXT=H; which demonstrates the rst equality. Now we compute H2= (I\u00002XXT)(I\u00002XXT) =I\u00004XXT+ 4XXTXXT =I\u00004XXT+ 4X(XTX)XT=I\u00004XXT+ 4X:1:XT=I: So, sinceHH =I, we haveH\u00001=H.\n",
      "350 (211) We know Mv=\u0015v. Hence M2v=MMv =M\u0015v =\u0015Mv =\u00152v; and similarly Mkv=\u0015Mk\u00001v=:::=\u0015kv: Sovis an eigenvector of Mkwith eigenvalue \u0015k. Now let us assume vis an eigenvector of the nilpotent matrix Nwith eigen- value\u0015. Then from above Nkv=\u0015kv but by nilpotence, we also have Nkv= 0: Hence\u0015kv= 0 andv(being an eigenvector) cannot vanish. Thus \u0015k= 0 and in turn \u0015=\n",
      "696 (212) 7. Let us think about the eigenvalue problem Mv=\u0015v. This has solutions when 0 = det\u00123\u0000\u0015\u00005 1\u00003\u0000\u0015\u0013 =\u00152\u00004)\u0015=\u00062: 336337 The associated eigenvalues solve the homogeneous systems (in augmented matrix form) \u00121\u000050 1\u000050\u0013 \u0018\u00121\u000050 0 0 0\u0013 and\u00125\u000050 1\u000010\u0013 \u0018\u00121\u000010 0 0 0\u0013 ; respectively, so are v2=\u00125 1\u0013 andv\u00002=\u00121 1\u0013 . HenceM12v2= 212v2and M12v\u00002= (\u00002)12v\u00002. Now,\u0012x y\u0013 =x\u0000y 4\u00125 1\u0013 \u0000x\u00005y 4\u00121 1\u0013 (this was obtained by solving the linear system av2+bv\u00002= foraandb). Thus M\u0012x y\u0013 =x\u0000y 4Mv2\u0000x\u00005y 4Mv\u00002 = 212\u0010x\u0000y 4v2\u0000x\u00005y 4v\u00002\u0011 = 212\u0012x y\u0013 : Thus M12=\u00124096 0 0 4096\u0013 : If you understand the above explanation, then you have a good understanding of diagonalization. A quicker route is simply to observe that M2=\u00124 0 0 4\u0013 .\n",
      "1049 (213) PM(\u0015) = (\u00001)2det\u0012a\u0000\u0015 b c d\u0000\u0015\u0013 = (\u0015\u0000a)(\u0015\u0000d)\u0000bc: Thus PM(M) = (M\u0000aI)(M\u0000dI)\u0000bcI =\u0012\u0012a b c d\u0013 \u0000\u0012a0 0a\u0013\u0013\u0012\u0012a b c d\u0013 \u0000\u0012d0 0d\u0013\u0013 \u0000\u0012bc 0 0bc\u0013 =\u00120b c d\u0000a\u0013\u0012a\u0000d b c 0\u0013 \u0000\u0012bc 0 0bc\u0013 = 0: Observe that any 2 \u00022 matrix is a zero of its own characteristic polynomial (in fact this holds for square matrices of any size ). Now ifA=P\u00001DP thenA2=P\u00001DPP\u00001DP =P\u00001D2P. Similarly Ak=P\u00001DkP. So for anymatrix polynomial we have An+c1An\u00001+\u0001\u0001\u0001cn\u00001A+cnI =P\u00001DnP+c1P\u00001Dn\u00001P+\u0001\u0001\u0001cn\u00001P\u00001DP+cnP\u00001P =P\u00001(Dn+c1Dn\u00001+\u0001\u0001\u0001cn\u00001D+cnI)P: 337338 Sample Second Midterm Thus we may conclude PA(A) =P\u00001PA(D)P. Now suppose D=0 BBB@\u001510\u0001\u0001\u0001 0 0\u00152 0 ......... 0\u0001\u0001\u0001\u0015n1 CCCA. Then PA(\u0015) = det(\u0015I\u0000A) = det(\u0015P\u00001IP\u0000P\u00001DP) = detP:det(\u0015I\u0000D):detP = det(\u0015I\u0000D) = det0 BBB@\u0015\u0000\u00151 0\u0001\u0001\u0001 0 0\u0015\u0000\u00152 0 ......... 0 0\u0001\u0001\u0001\u0015\u0000\u0015n1 CCCA = (\u0015\u0000\u00151)(\u0015\u0000\u00152):::(\u0015\u0000\u0015n): Thus we see that \u00151,\u00152;:::;\u0015nare the eigenvalues of M. Finally we compute PA(D) = (D\u0000\u00151)(D\u0000\u00152):::(D\u0000\u0015n) =0 BBB@0 0\u0001\u0001\u0001 0 0\u00152 0 ......... 0 0\u0001\u0001\u0001\u0015n1 CCCA0 BBB@\u001510\u0001\u0001\u0001 0 0 0 0 ......... 0 0\u0001\u0001\u0001\u0015n1 CCCA:::0 BBB@\u001510\u0001\u0001\u00010 0\u00152 0 ......... 0 0\u0001\u0001\u000101 CCCA= 0: We conclude the PM(M) =\n",
      "900 (214) 9. A subset of a vector space is called a subspace if it itself is a vector space, using the rules for vector addition and scalar multiplication inherited from the original vector space. (a) So long as U6=U[W6=Wthe answer is no. Take, for example, U to be thex-axis in R2andWto be they-axis. Then\u0000 1;0\u0001 2Uand\u0000 0;1\u0001 2W, but\u0000 1;0\u0001 +\u0000 0;1\u0001 =\u0000 1;1\u0001 =2U[W. SoU[Wis not additively closed and is not a vector space (and thus not a subspace). It is easy to draw the example described. (b) Here the answer is always yes. The proof is not di\u000ecult. Take a vector uandwsuch thatu2U\\W3w. This means that bothuandw are in bothUandW. But, since Uis a vector space, u+ wis also inU. Similarly, u+ w2W. Hence u+ w2U\\W. So closure holds inU\\Wand this set is a subspace by the subspace theorem. Here, a good picture to draw is two planes through the origin in R3 intersecting at a line (also through the origin). 338339\n",
      "153 (215) (i) We say that the vectors fv1;v2;:::vngare linearly independent if there exist noconstantsc1,c2;:::cn(not all vanishing) such that c1v1+c2v2+ \u0001\u0001\u0001+cnvn=\n",
      "133 (216) Alternatively, we can require that there is no non-trivial solution for scalars c1,c2;:::;cnto the linear system c1v1+c2v2+\u0001\u0001\u0001+ cnvn=\n",
      "544 (217) (ii) We say that these vectors span a vector space Vif the set spanfv1;v2;:::vng=fc1v1+c2v2+\u0001\u0001\u0001+cnvn:c1;c2;:::cn2Rg=V. (iii) We callfv1;v2;:::vnga basis forViffv1;v2;:::vngare linearly independent andspanfv1;v2;:::vng=V. Foru;v;w to be a basis for R3, we rstly need (the spanning requirement) that any vector0 @x y z1 Acan be written as a linear combination of u,vandw c10 @\u00001 \u00004 31 A+c20 @4 5 01 A+c30 @10 7 h+ 31 A=0 @x y z1 A: The linear independence requirement implies that when x=y=z= 0, the only solution to the above system is c1=c2=c3=\n",
      "332 (218) But the above system in matrix language reads 0 @\u00001 4 10 \u00004 5 7 3 0h+ 31 A0 @c1 c2 c31 A=0 @x y z1 A: Both requirements mean that the matrix on the left hand side must be invertible, so we examine its determinant det0 @\u00001 4 10 \u00004 5 7 3 0h+ 31 A=\u00004\u0001(\u00004\u0001(h+ 3)\u00007\u00013) + 5\u0001(\u00001\u0001(h+ 3)\u000010\u00013) = 11(h\u00003)\u0001 Hence we obtain a basis whenever h6=\n",
      "137 (219) 339340 Sample Second Midterm 340F Sample Final Exam Here are some worked problems typical for what you might expect on a nal examination.\n",
      "1348 (220) De ne the following terms: (a) An orthogonal matrix . (b) A basis for a vector space. (c) The span of a set of vectors. (d) The dimension of a vector space. (e) An eigenvector . (f) A subspace of a vector space. (g) The kernel of a linear transformation. (h) The nullity of a linear transformation. (i) The image of a linear transformation. (j) The rank of a linear transformation. (k) The characteristic polynomial of a square matrix. (l) An equivalence relation . (m) A homogeneous solution to a linear system of equations. (n) A particular solution to a linear system of equations. (o) The general solution to a linear system of equations. (p) The direct sum of a pair of subspaces of a vector space. 341342 Sample Final Exam (q) The orthogonal complement to a subspace of a vector space. 2.Kircho 's laws : Electrical circuits are easy to analyze using systems of equa- tions. The change in voltage (measured in Volts) around any loop due to batteriesj and resistors =n=n=n=n(given by the product of the current mea- sured in Amps and resistance measured in Ohms) equals zero. Also, the sum of currents entering any junction vanishes. Consider the circuit J Amps 3 Ohms60 Volts1 Ohm 2 Ohms 80 Volts 3 OhmsV Volts13 Amps I Amps Find all possible equations for the unknowns I,JandVand then solve for I,JandV. Give your answers with correct units.\n",
      "520 (221) Suppose Mis the matrix of a linear transformation L:U!V and the vector spaces UandVhave dimensions dimU=n; dimV=m; and m6=n: Also assume kerL=f0Ug: (a) How many rows does Mhave? (b) How many columns does Mhave? (c) Are the columns of Mlinearly independent? (d) What size matrix is MTM? (e) What size matrix is MMT? (f) IsMTMinvertible? (g) isMTMsymmetric? 342343 (h) IsMTMdiagonalizable? (i) DoesMTMhave a zero eigenvalue? (j) Suppose U=Vand kerL6=f0Ug. Find an eigenvalue of M. (k) Suppose U=Vand kerL6=f0Ug. Find detM.\n",
      "248 (222) Consider the system of equations x+y+z+w= 1 x+ 2y+ 2z+ 2w= 1 x+ 2y+ 3z+ 3w= 1 Express this system as a matrix equation MX =Vand then nd the solution set by computing an LUdecomposition for the matrix M(be sure to use back and forward substitution).\n",
      "422 (223) Compute the following determinants det\u00121 2 3 4\u0013 ;det0 @1 2 3 4 5 6 7 8 91 A;det0 BB@1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 161 CCA; det0 BBBB@1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 251 CCCCA: Now test your skills on det0 BBBBB@1 2 3 \u0001\u0001\u0001n n+ 1n+ 2n+ 3\u0001\u0001\u00012n 2n+ 1 2n+ 2 2n+ 3 3 n ......... n2\u0000n+ 1n2\u0000n+ 2n2\u0000n+ 3\u0001\u0001\u0001n21 CCCCCA: Make sure to jot down a few brief notes explaining any clever tricks you use.\n",
      "651 (224) For which values of adoes U= span8 < :0 @1 0 11 A;0 @1 2 \u000031 A;0 @a 1 01 A9 = ;=R3? 343344 Sample Final Exam For any special values of aat whichU6=R3, express the subspace Uas the span of the least number of vectors possible. Give the dimension of Ufor these cases and draw a picture showing Uinside R3. 7.Vandermonde determinant: Calculate the following determinants det\u00121x 1y\u0013 ;det0 @1x x2 1y y2 1z z21 A;det0 BB@1x x2x3 1y y2y3 1z z2z3 1w w2w31 CCA: Be sure to factorize you answers, if possible. Challenging: Compute the determinant det0 BBBBB@1x1(x1)2\u0001\u0001\u0001 (x1)n\u00001 1x2(x2)2\u0001\u0001\u0001 (x2)n\u00001 1x3(x3)2\u0001\u0001\u0001 (x3)n\u00001 ............... 1xn(xn)2\u0001\u0001\u0001(xn)n\u000011 CCCCCA:\n",
      "385 (225) (a) Do the vectors8 < :0 @1 2 31 A;0 @3 2 11 A;0 @1 0 01 A;0 @0 1 01 A;0 @0 0 11 A9 = ;form a basis for R3? Be sure to justify your answer. (b) Find a basis for R4that includes the vectors0 BB@1 2 3 41 CCAand0 BB@4 3 2 11 CCA. (c) Explain in words how to generalize your computation in part (b) to obtain a basis for Rnthat includes a given pair of (linearly independent) vectorsuandv.\n",
      "992 (226) Elite NASA engineers determine that if a satellite is placed in orbit starting at a pointO, it will return exactly to that same point after one orbit of the earth. Unfortunately, if there is a small mistake in the original location of the satellite, which the engineers label by a vector XinR3with origin1atO, 1This is a spy satellite. The exact location of O, the orientation of the coordinate axes inR3and the unit system employed by the engineers are CIA secrets. 344345 after one orbit the satellite will instead return to some other point Y2R3. The engineer's computations show that Yis related to Xby a matrix Y=0 BB@01 21 1 21 21 2 11 201 CCAX: (a) Find all eigenvalues of the above matrix. (b) Determine allpossible eigenvectors associated with each eigenvalue. Let us assume that the rule found by the engineers applies to all subsequent orbits. Discuss case by case, what will happen to the satellite if the initial mistake in its location is in a direction given by an eigenvector.\n",
      "1059 (227) In this problem the scalars in the vector spaces are bits (0 ;1 with 1+1 = 0). The spaceBkis the vector space of bit-valued, k-component column vectors. (a) Find a basis for B3. (b) Your answer to part (a) should be a list of vectors v1,v2;:::vn. What number did you nd for n? (c) How many elements are there in the setB3. (d) What is the dimension of the vector space B3. (e) Suppose L:B3!B=f0;1gis a linear transformation. Explain why specifyingL(v1),L(v2);:::;L (vn) completely determines L. (f) Use the notation of part (e) to list all linear transformations L:B3!B: How many di erent linear transformations did you nd? Compare your answer to part (c). (g) Suppose L1:B3!BandL2:B3!Bare linear transformations, and and are bits. De ne a new map ( L1+ L2) :B3!Bby ( L1+ L2)(v) = L1(v) + L2(v): Is this map a linear transformation? Explain. (h) Do you think the set of all linear transformations from B3toBis a vector space using the addition rule above? If you answer yes, give a basis for this vector space and state its dimension. 345346 Sample Final Exam\n",
      "3433 (228) A team of distinguished, post-doctoral engineers analyzes the design for a bridge across the English channel. They notice that the force on the center of the bridge when it is displaced by an amount X=0 @x y z1 Ais given by F=0 @\u0000x\u0000y \u0000x\u00002y\u0000z \u0000y\u0000z1 A: Moreover, having read Newton's Principi\u001a, they know that force is propor- tional to acceleration so that2 F=d2X dt2: Since the engineers are worried the bridge might start swaying in the heavy channel winds, they search for an oscillatory solution to this equation of the form3 X= cos(!t)0 @a b c1 A: (a) By plugging their proposed solution in the above equations the engi- neers nd an eigenvalue problem M0 @a b c1 A=\u0000!20 @a b c1 A: HereMis a 3\u00023 matrix. Which 3 \u00023 matrixMdid the engineers nd? Justify your answer. (b) Find the eigenvalues and eigenvectors of the matrix M. (c) The numberj!jis often called a characteristic frequency . What char- acteristic frequencies do you nd for the proposed bridge? (d) Find an orthogonal matrix Psuch thatMP =PD whereDis a diagonal matrix. Be sure to also state your result for D. 2The bridge is intended for French and English military vehicles, so the exact units, coordinate system and constant of proportionality are state secrets. 3Here,a;b;c and!are constants which we aim to calculate. 346347 (e) Is there a direction in which displacing the bridge yields no force? If so give a vector in that direction. Brie y evaluate the quality of this bridge design. 12.Conic Sections : The equation for the most general conic section is given by ax2+ 2bxy+dy2+ 2cx+ 2ey+f= 0: Our aim is to analyze the solutions to this equation using matrices. (a) Rewrite the above quadratic equation as one of the form XTMX +XTC+CTX+f= 0 relating an unknown column vector X=\u0012x y\u0013 , its transpose XT, a 2\u00022 matrixM, a constant column vector Cand the constant f. (b) Does your matrix Mobey any special properties? Find its eigenvalues. You may call your answers \u0015and\u0016for the rest of the problem to save writing. For the rest of this problem we will focus on central conics for which the matrix Mis invertible. (c) Your equation in part (a) above should be be quadratic in X. Recall that ifm6= 0, the quadratic equation mx2+ 2cx+f= 0 can be rewritten by completing the square m\u0010 x+c m\u00112 =c2 m\u0000f: Being very careful that you are now dealing with matrices, use the same trick to rewrite your answer to part (a) in the form YTMY =g: Make sure you give formulas for the new unknown column vector Y and constant gin terms of X,M,Candf. You need not multiply out any of the matrix expressions you nd. If all has gone well, you have found a way to shift coordinates for the original conic equation to a new coordinate system with its origin at the center of symmetry. Our next aim is to rotate the coordinate axes to produce a readily recognizable equation. 347348 Sample Final Exam (d) Why is the angle between vectors VandWis not changed when you replace them by PVandPW forPany orthogonal matrix? (e) Explain how to choose an orthogonal matrix Psuch thatMP =PD whereDis a diagonal matrix. (f) For the choice of Pabove, de ne our nal unknown vector ZbyY= PZ. Find an expression for YTMY in terms of Zand the eigenvalues ofM. (g) CallZ=\u0012z w\u0013 . What equation do zandwobey? (Hint, write your answer using \u0015,\u0016andg.) (h) Central conics are circles, ellipses, hyperbolae or a pair of straight lines. Give examples of values of ( \u0015;\u0016;g ) which produce each of these cases.\n",
      "662 (229) LetL:V!Wbe a linear transformation between nite-dimensional vector spacesVandW, and letMbe a matrix for L(with respect to some basis forVand some basis for W). We know that Lhas an inverse if and only if it is bijective, and we know a lot of ways to tell whether Mhas an inverse. In fact,Lhas an inverse if and only if Mhas an inverse: (a) Suppose that Lis bijective (i.e., one-to-one and onto). i. Show that dim V= rankL= dimW. ii. Show that 0 is not an eigenvalue of M. iii. Show that Mis an invertible matrix. (b) Now, suppose that Mis an invertible matrix. i. Show that 0 is not an eigenvalue of M. ii. Show that Lis injective. iii. Show that Lis surjective.\n",
      "639 (230) Captain Conundrum gives Queen Quandary a pair of newborn doves, male and female for her birthday. After one year, this pair of doves breed and produce a pair of dove eggs. One year later these eggs hatch yielding a new pair of doves while the original pair of doves breed again and an additional pair of eggs are laid. Captain Conundrum is very happy because now he will never need to buy the Queen a present ever again! Let us say that in year zero, the Queen has no doves. In year one she has one pair of doves, in year two she has two pairs of doves etc... CallFnthe number of pairs of doves in years n. For example, F0= 0,F1= 1 and F2=\n",
      "277 (231) Assume no doves die and that the same breeding pattern continues 348349 well into the future. Then F3= 2 because the eggs laid by the rst pair of doves in year two hatch. Notice also that in year three, two pairs of eggs are laid (by the rst and second pair of doves). Thus F4=\n",
      "706 (232) (a) Compute F5andF6. (b) Explain why (for any n\u00152) the following recursion relation holds Fn=Fn\u00001+Fn\u00002: (c) Let us introduce a column vector Xn=\u0012Fn Fn\u00001\u0013 . Compute X1andX2. Verify that these vectors obey the relationship X2=MX 1whereM=\u00121 1 1 0\u0013 : (d) Show that Xn+1=MXn. (e) Diagonalize M. (I.e., writeMas a product M=PDP\u00001whereDis diagonal.) (f) Find a simple expression for Mnin terms of P,DandP\u00001. (g) Show that Xn+1=MnX1. (h) The number '=1 +p 5 2 is called the golden ratio . Write the eigenvalues of Min terms of '. (i) Put your results from parts (c), (f) and (g) together (along with a short matrix computation) to nd the formula for the number of doves Fn in yearnexpressed in terms of ', 1\u0000'andn.\n",
      "124 (233) Use Gram{Schmidt to nd an orthonormal basis for span8 >>< >>:0 BB@1 1 1 11 CCA;0 BB@1 0 1 11 CCA;0 BB@0 0 1 21 CCA9 >>= >>;:\n",
      "497 (234) LetMbe the matrix of a linear transformation L:V!Win given bases forVandW. Fill in the blanks below with one of the following six vector spaces:V,W, kerL,\u0000 kerL\u0001?, imL,\u0000 imL\u0001?. 349350 Sample Final Exam (a) The columns of Mspan in the basis given for . (b) The rows of Mspan in the basis given for . Suppose M=0 BB@1 2 1 3 2 1\u00001 2 1 0 0\u00001 4 1\u00001 01 CCA is the matrix of Lin the basesfv1;v2;v3;v4gforVandfw1;w2;w3;w4g forW. Find bases for ker Land imL. Use the dimension formula to check your result.\n",
      "502 (235) Captain Conundrum collects the following data set yx 5\u00002 2\u00001 01 32 which he believes to be well-approximated by a parabola y=ax2+bx+c: (a) Write down a system of four linear equations for the unknown coe\u000e- cientsa,bandc. (b) Write the augmented matrix for this system of equations. (c) Find the reduced row echelon form for this augmented matrix. (d) Are there any solutions to this system? (e) Find the least squares solution to the system. (f) What value does Captain Conundrum predict for ywhenx= 2?\n",
      "998 (236) Suppose you have collected the following data for an experiment xy x1y1 x2y2 x3y3 and believe that the result is well modeled by a straight line y=mx+b: 350351 (a) Write down a linear system of equations you could use to nd the slope mand constant term b. (b) Arrange the unknowns ( m;b) in a column vector Xand write your answer to (a) as a matrix equation MX =V : Be sure to give explicit expressions for the matrix Mand column vector V. (c) For a generic data set, would you expect your system of equations to have a solution? Brie y explain your answer. (d) Calculate MTMand (MTM)\u00001(for the latter computation, state the condition required for the inverse to exist). (e) Compute the least squares solution for mandb. (f) The least squares method determines a vector Xthat minimizes the length of the vector V\u0000MX. Draw a rough sketch of the three data points in the ( x;y)-plane as well as their least squares t. Indicate how the components of V\u0000MX could be obtained from your picture. Solutions\n",
      "83 (237) You can nd the de nitions for all these terms by consulting the index of this book.\n",
      "437 (238) Both junctions give the same equation for the currents I+J+ 13 = 0: There are three voltage loops (one on the left, one on the right and one going around the outside of the circuit). Respectively, they give the equations 60\u0000I\u000080\u00003I= 0 80 + 2J\u0000V+ 3J= 0 60\u0000I+ 2J\u0000V+ 3J\u00003I= 0: (F.1) The above equations are easily solved (either using an augmented matrix and row reducing, or by substitution). The result is I=\u00005 Amps,J=\u00008 Amps,V= 40 Volts.\n",
      "184 (239) (a)m. 351352 Sample Final Exam (b)n. (c) Yes. (d)n\u0002n. (e)m\u0002m. (f) Yes. This relies on ker M= 0 because if MTMhad a non-trivial kernel, then there would be a non-zero solution XtoMTMX =\n",
      "59 (240) But then by multiplying on the left by XTwe see thatjjMXjj=\n",
      "390 (241) This in turn impliesMX = 0 which contradicts the triviality of the kernel of M. (g) Yes because\u0000 MTM\u0001T=MT(MT)T=MTM. (h) Yes, all symmetric matrices have a basis of eigenvectors. (i) No, because otherwise it would not be invertible. (j) Since the kernel of Lis non-trivial, Mmust have 0 as an eigenvalue. (k) SinceMhas a zero eigenvalue in this case, its determinant must vanish. I.e., detM=\n",
      "568 (242) 4. To begin with the system becomes 0 B@1 1 1 1 1 2 2 2 1 2 3 31 CA0 BBB@x y z w1 CCCA=0 B@1 1 11 CA Then M=0 B@1 1 1 1 1 2 2 2 1 2 3 31 CA=0 B@1 0 0 1 1 0 1 0 11 CA0 B@1 1 1 1 0 1 1 1 0 1 2 21 CA =0 B@1 0 0 1 1 0 1 1 11 CA0 B@1 1 1 1 0 1 1 1 0 0 1 11 CA=LU So nowMX =VbecomesLW =VwhereW=UX=0 @a b c1 A(say). Thus we solveLW=Vby forward substitution a= 1; a+b= 1; a+b+c= 1)a= 1;b= 0;c= 0: 352353 Now solveUX=Wby back substitution x+y+z+w= 1; y+z+w= 0; z+w= 0 )w=\u0016(arbitrary);z=\u0000\u0016;y= 0;x= 1: The solution set is8 >>< >>:0 BB@x y z y1 CCA=0 BB@1 0 \u0000\u0016 \u00161 CCA:\u00162R9 >>= >>;\n",
      "233 (243) First det\u00121 2 3 4\u0013 =\u00002: All the other determinants vanish because the rst three rows of each matrix are not independent. Indeed, 2 R2\u0000R1=R3in each case, so we can make row operations to get a row of zeros and thus a zero determinant.\n",
      "583 (244) IfUspans R3, then we must be able to express any vector X=0 @x y z1 A2R3 as X=c10 @1 0 11 A+c20 @1 2 \u000031 A+c30 @a 1 01 A=0 @1 1a 0 2 1 1\u00003 01 A0 @c1 c2 c31 A; for some coe\u000ecients c1,c2andc3. This is a linear system. We could solve forc1,c2andc3using an augmented matrix and row operations. However, since we know that dim R3= 3, ifUspans R3, it will also be a basis. Then the solution for c1,c2andc3would be unique. Hence, the 3 \u00023 matrix above must be invertible, so we examine its determinant det0 @1 1a 0 2 1 1\u00003 01 A= 1:(2:0\u00001:(\u00003)) + 1:(1:1\u0000a:2) = 4\u00002a: ThusUspans R3whenevera6=\n",
      "573 (245) When a= 2 we can write the third vector inUin terms of the preceding ones as 0 @2 1 01 A=3 20 @1 0 11 A+1 20 @1 2 \u000031 A: (You can obtain this result, or an equivalent one by studying the above linear system with X= 0, i.e., the associated homogeneous system. ) The two 353354 Sample Final Exam vectors0 @1 2 \u000031 Aand0 @2 1 01 Aare clearly linearly independent, so this is the least number of vectors spanning Ufor this value of a. Also we see that dimU= 2 in this case. Your picture should be a plane in R3though the origin containing the vectors0 @1 2 \u000031 Aand0 @2 1 01 A.\n",
      "1373 (246) det\u00121x 1y\u0013 =y\u0000x; det0 @1x x2 1y y2 1z z21 A= det0 @1x x2 0y\u0000x y2\u0000x2 0z\u0000x z2\u0000x21 A = (y\u0000x)(z2\u0000x2)\u0000(y2\u0000x2)(z\u0000x) = (y\u0000x)(z\u0000x)(z\u0000y): det0 BB@1x x2x3 1y y2y3 1z z2z3 1w w2w31 CCA= det0 BB@1x x2x3 0y\u0000x y2\u0000x2y3\u0000x3 0z\u0000x z2\u0000x2z3\u0000x3 0w\u0000x w2\u0000x2w3\u0000x31 CCA = det0 BB@1 0 0 0 0y\u0000x y (y\u0000x)y2(y\u0000x) 0z\u0000x z (z\u0000x)z2(z\u0000x) 0w\u0000x w (w\u0000x)w2(w\u0000x)1 CCA = (y\u0000x)(z\u0000x)(w\u0000x) det0 BB@1 0 0 0 0 1y y2 0 1z z2 0 1w w21 CCA = (y\u0000x)(z\u0000x)(w\u0000x) det0 @1y y2 1z z2 1w w21 A = (y\u0000x)(z\u0000x)(w\u0000x)(z\u0000y)(w\u0000y)(w\u0000z): From the 4\u00024 case above, you can see all the tricks required for a general Vandermonde matrix. First zero out the rst column by subtracting the rst row from all other rows (which leaves the determinant unchanged). 354355 Now zero out the top row by subtracting x1times the rst column from the second column, x1times the second column from the third column et cetra . Again these column operations do not change the determinant. Now factor outx2\u0000x1from the second row, x3\u0000x1from the third row, etc. This does change the determinant so we write these factors outside the remaining determinant, which is just the same problem but for the ( n\u00001)\u0002(n\u00001) case. Iterating the same procedure gives the result det0 BBBBB@1x1(x1)2\u0001\u0001\u0001 (x1)n\u00001 1x2(x2)2\u0001\u0001\u0001 (x2)n\u00001 1x3(x3)2\u0001\u0001\u0001 (x3)n\u00001 ............... 1xn(xn)2\u0001\u0001\u0001(xn)n\u000011 CCCCCA=Y i>j(xi\u0000xj): (HereQstands for a multiple product, just like \u0006 stands for a multiple sum.)\n",
      "1179 (247) (a) No, a basis for R3must have exactly three vectors. (b) We rst extend the original vectors by the standard basis for R4and then try to eliminate two of them by considering 0 BB@1 2 3 41 CCA+ 0 BB@4 3 2 11 CCA+ 0 BB@1 0 0 01 CCA+\u000e0 BB@0 1 0 01 CCA+\"0 BB@0 0 1 01 CCA+\u00110 BB@0 0 0 11 CCA= 0: So we study 0 BB@1 4 1 0 0 0 2 3 0 1 0 0 3 2 0 0 1 0 4 1 0 0 0 11 CCA\u00180 BB@1 4 1 0 0 0 0\u00005\u00002 1 0 0 0\u000010\u00003 0 1 0 0\u000015\u00004 0 0 11 CCA \u00180 BB@1 0\u00003 5\u00004 0 0 0 12 51 50 0 0 0 1 10 1 0 0 0 2 15 0 11 CCA\u00180 BB@1 0 0 23 50 0 1 0\u000019 5\u00002 50 0 0 1 10 1 0 0 0 0\u00005 2\u0000101 21 CCA From here we can keep row reducing to achieve RREF, but we can already see that the non-pivot variables will be \"and\u0011. Hence we can 355356 Sample Final Exam eject the last two vectors and obtain as our basis 8 >>< >>:0 BB@1 2 3 41 CCA;0 BB@4 3 2 11 CCA;0 BB@1 0 0 01 CCA;0 BB@0 1 0 01 CCA9 >>= >>;: Of course, this answer is far from unique! (c) The method is the same as above. Add the standard basis to fu;vg to obtain the linearly dependent set fu;v;e 1;:::;eng. Then put these vectors as the columns of a matrix and row reduce. The standard basis vectors in columns corresponding to the non-pivot variables can be removed.\n",
      "155 (248) (a) det0 BB@\u0015\u00001 2\u00001 \u00001 2\u0015\u00001 2\u00001 2 \u00001\u00001 2\u00151 CCA=\u0015\u0010 (\u0015\u00001 2\u0011 \u0015\u00001 4)+1 2\u0010 \u0000\u0015 2\u00001 2\u0011 \u0000\u0010 \u00001 4+\u0015\u0011 =\u00153\u00001 2\u00152\u00003 2\u0015=\u0015(\u0015+ 1)(\u0015\u00003 2): Hence the eigenvalues are 0 ;\u00001;3\n",
      "608 (249) (b) When\u0015= 0 we must solve the homogenous system 0 B@01 210 1 21 21 20 11 2001 CA\u00180 B@11 200 01 41 20 01 2101 CA\u00180 B@1 0\u000010 0 1 2 0 0 0 0 01 CA: So we nd the eigenvector0 @s \u00002s s1 Awheres6= 0 is arbitrary. For\u0015=\u00001 0 B@11 210 1 23 21 20 11 2101 CA\u00180 B@1 0 1 0 0 1 0 0 0 0 0 01 CA: So we nd the eigenvector0 @\u0000s 0 s1 Awheres6= 0 is arbitrary. 356357 Finally, for \u0015=3 20 B@\u00003 21 210 1 2\u000011 20 11 2\u00003 201 CA\u00180 B@11 2\u00003 20 0\u00005 45 40 05 4\u00005 401 CA\u00180 B@1 0\u000010 0 1\u000010 0 0 0 01 CA: So we nd the eigenvector0 @s s s1 Awheres6= 0 is arbitrary. If the mistake Xis in the direction of the eigenvector0 @1 \u00002 11 A, thenY=\n",
      "953 (250) I.e., the satellite returns to the origin O. For all subsequent orbits it will again return to the origin. NASA would be very pleased in this case. If the mistake Xis in the direction0 @\u00001 0 11 A, thenY=\u0000X. Hence the satellite will move to the point opposite to X. After next orbit will move back toX. It will continue this wobbling motion inde nitely. Since this is a stable situation, again, the elite engineers will pat themselves on the back. Finally, if the mistake Xis in the direction0 @1 1 11 A, the satellite will move to a pointY=3 2Xwhich is further away from the origin. The same will happen for all subsequent orbits, with the satellite moving a factor 3 =2 further away fromOeach orbit (in reality, after several orbits, the approximations used by the engineers in their calculations probably fail and a new computation will be needed). In this case, the satellite will be lost in outer space and the engineers will likely lose their jobs!\n",
      "69 (251) (a) A basis for B3is8 < :0 @1 0 01 A;0 @0 1 01 A;0 @0 0 11 A9 = ; (b)\n",
      "1453 (254) (e) Because the vectors fv1;v2;v3gare a basis any element v2B3can be written uniquely as v=b1v1+b2v2+b3v3for some triplet of bits0 @b1 b2 b31 A. 357358 Sample Final Exam Hence, to compute L(v) we use linearity of L L(v) =L(b1v1+b2v2+b3v3) =b1L(v1) +b2L(v2) +b3L(v3) =\u0000 L(v1)L(v2)L(v3)\u00010 @b1 b2 b31 A: (f) From the notation of the previous part, we see that we can list linear transformations L:B3!Bby writing out all possible bit-valued row vectors \u0000 0 0 0\u0001 ;\u0000 1 0 0\u0001 ;\u0000 0 1 0\u0001 ;\u0000 0 0 1\u0001 ;\u0000 1 1 0\u0001 ;\u0000 1 0 1\u0001 ;\u0000 0 1 1\u0001 ;\u0000 1 1 1\u0001 : There are 23= 8 di erent linear transformations L:B3!B, exactly the same as the number of elements in B3. (g) Yes, essentially just because L1andL2are linear transformations. In detail for any bits ( a;b) and vectors ( u;v) inB3it is easy to check the linearity property for ( L1+ L2) ( L1+ L2)(au+bv) = L1(au+bv) + L2(au+bv) = aL 1(u) + bL 1(v) + aL 1(u) + bL 1(v) =a( L1(u) + L2(v)) +b( L1(u) + L2(v)) =a( L1+ L2)(u) +b( L1+ L2)(v): Here the rst line used the de nition of ( L1+ L2), the second line depended on the linearity of L1andL2, the third line was just algebra and the fourth used the de nition of ( L1+ L2) again. (h) Yes. The easiest way to see this is the identi cation above of these maps with bit-valued column vectors. In that notation, a basis is n\u0000 1 0 0\u0001 ;\u0000 0 1 0\u0001 ;\u0000 0 0 1\u0001o : 358359 Since this (spanning) set has three (linearly independent) elements, the vector space of linear maps B3!Bhas dimension\n",
      "69 (255) This is an example of a general notion called the dual vector space .\n",
      "815 (256) (a)d2X dt2=d2cos(!t) dt20 @a b c1 A=\u0000!2cos(!t)0 @a b c1 A: Hence F= cos(!t)0 @\u0000a\u0000b \u0000a\u00002b\u0000c \u0000b\u0000c1 A= cos(!t)0 @\u00001\u00001 0 \u00001\u00002\u00001 0\u00001\u000011 A0 @a b c1 A =\u0000!2cos(!t)0 @a b c1 A; so M=0 @\u00001\u00001 0 \u00001\u00002\u00001 0\u00001\u000011 A: (b) det0 @\u0015+ 1 1 0 1\u0015+ 2 1 0 1 \u0015+ 11 A= (\u0015+ 1)\u0000 (\u0015+ 2)(\u0015+ 1)\u00001\u0001 \u0000(\u0015+ 1) = (\u0015+ 1)\u0000 (\u0015+ 2)(\u0015+ 1)\u00002\u0001 = (\u0015+ 1)\u0000 \u00152+ 3\u0015) =\u0015(\u0015+ 1)(\u0015+ 3) so the eigenvalues are \u0015= 0;\u00001;\u00003. For the eigenvectors, when \u0015= 0 we study: M\u00000:I=0 @\u00001\u00001 0 \u00001\u00002\u00001 0\u00001\u000011 A\u00180 @1 1 0 0\u00001\u00001 0\u00001\u000011 A\u00180 @1 0\u00001 0 1 1 0 0 01 A; so0 @1 \u00001 11 Ais an eigenvector. For\u0015=\u00001 M\u0000(\u00001):I=0 @0\u00001 0 \u00001\u00001\u00001 0\u00001 01 A\u00180 @1 0 1 0 1 0 0 0 01 A; 359360 Sample Final Exam so0 @\u00001 0 11 Ais an eigenvector. For\u0015=\u00003 M\u0000(\u00003):I=0 @2\u00001 0 \u00001 1\u00001 0\u00001 21 A\u00180 @1\u00001 1 0 1\u00002 0\u00001 21 A\u00180 @1 0\u00001 0 1\u00002 0 0 01 A; so0 @1 2 11 Ais an eigenvector. (c) The characteristic frequencies are 0 ;1;p\n",
      "332 (257) (d) The orthogonal change of basis matrix P=0 B@1p 3\u00001p 21p 6 \u00001p 302p 61p 31p 21p 61 CA It obeysMP =PDwhere D=0 @0 0 0 0\u00001 0 0 0\u000031 A: (e) Yes, the direction given by the eigenvector0 @1 \u00001 11 Abecause its eigen- value is zero. This is probably a bad design for a bridge because it can be displaced in this direction with no force!\n",
      "1967 (258) (a) If we call M=\u0012a b b d\u0013 , thenXTMX =ax2+ 2bxy+dy2. Similarly puttingC=\u0012c e\u0013 yieldsXTC+CTX= 2XC= 2cx+ 2ey. Thus 0 =ax2+ 2bxy+dy2+ 2cx+ 2ey+f =\u0000 x y\u0001\u0012a b b d\u0013\u0012x y\u0013 +\u0000 x y\u0001\u0012c e\u0013 +\u0000 c e\u0001\u0012x y\u0013 +f: 360361 (b) Yes, the matrix Mis symmetric, so it will have a basis of eigenvectors and is similar to a diagonal matrix of real eigenvalues. To nd the eigenvalues notice that det\u0012a\u0000\u0015 b b d\u0000\u0015\u0013 = (a\u0000\u0015)(d\u0000 \u0015)\u0000b2=\u0000 \u0015\u0000a+d 2\u00012\u0000b2\u0000\u0000a\u0000d 2\u00012. So the eigenvalues are \u0015=a+d 2+r b2+\u0000a\u0000d 2\u00012and\u0016=a+d 2\u0000r b2+\u0000a\u0000d 2\u00012: (c) The trick is to write XTMX +CTX+XTC= (XT+CTM\u00001)M(X+M\u00001C)\u0000CTM\u00001C; so that (XT+CTM\u00001)M(X+M\u00001C) =CTMC\u0000f: HenceY=X+M\u00001Candg=CTMC\u0000f. (d) The cosine of the angle between vectors VandWis given by VWp VV WW=VTWp VTV WTW: So replacing V!PVandW!PW will always give a factor PTP inside all the products, but PTP=Ifor orthogonal matrices. Hence none of the dot products in the above formula changes, so neither does the angle between VandW. (e) If we take the eigenvectors of M, normalize them ( i.e. divide them by their lengths), and put them in a matrix P(as columns) then P will be an orthogonal matrix. (If it happens that \u0015=\u0016, then we also need to make sure the eigenvectors spanning the two dimensional eigenspace corresponding to \u0015are orthogonal.) Then, since Mtimes the eigenvectors yields just the eigenvectors back again multiplied by their eigenvalues, it follows that MP =PDwhereDis the diagonal matrix made from eigenvalues. (f) IfY=PZ, thenYTMY =ZTPTMPZ =ZTPTPDZ =ZTDZ whereD=\u0012\u00150 0\u0016\u0013 . (g) Using part (f) and (c) we have \u0015z2+\u0016w2=g: 361362 Sample Final Exam (h) When\u0015=\u0016andg=\u0015=R2, we get the equation for a circle radius Rin the (z;w)-plane. When \u0015,\u0016andgare postive, we have the equation for an ellipse. Vanishing galong with\u0015and\u0016of opposite signs gives a pair of straight lines. When gis non-vanishing, but \u0015and\u0016have opposite signs, the result is a pair of hyperbol\u001a. These shapes all come from cutting a cone with a plane, and are therefore called conic sections.\n",
      "1500 (259) We show that Lis bijective if and only if Mis invertible. (a) We suppose that Lis bijective. i. SinceLis injective, its kernel consists of the zero vector alone. Hence L= dim kerL= 0: So by the Dimension Formula, dimV=L+ rankL= rankL: SinceLis surjective, L(V) =W:Thus rankL= dimL(V) = dimW: Thereby dimV= rankL= dimW: ii. Since dim V= dimW, the matrix Mis square so we can talk about its eigenvalues. Since Lis injective, its kernel is the zero vector alone. That is, the only solution to LX= 0 isX= 0V. ButLXis the same as MX, so the only solution to MX = 0 is X= 0V. SoMdoes not have zero as an eigenvalue. iii. SinceMX = 0 has no non-zero solutions, the matrix Mis invert- ible. (b) Now we suppose that Mis an invertible matrix. i. SinceMis invertible, the system MX = 0 has no non-zero solu- tions. ButLXis the same as MX, so the only solution to LX= 0 isX= 0V. SoLdoes not have zero as an eigenvalue. ii. SinceLX= 0 has no non-zero solutions, the kernel of Lis the zero vector alone. So Lis injective. iii. SinceMis invertible, we must have that dim V= dimW. By the Dimension Formula, we have dimV=L+ rankL 362363 and since ker L=f0Vgwe haveL= dim kerL= 0, so dimW= dimV= rankL= dimL(V): SinceL(V) is a subspace of Wwith the same dimension as W, it must be equal to W. To see why, pick a basis BofL(V). Each element ofBis a vector in W, so the elements of Bform a linearly independent set in W. Therefore Bis a basis of W, since the size ofBis equal to dim W. SoL(V) = spanB=W:SoLis surjective.\n",
      "540 (261) (b) The number of pairs of doves in any given year equals the number of the previous years plus those that hatch and there are as many of them as pairs of doves in the year before the previous year. (c)X1=\u0012F1 F0\u0013 =\u00121 0\u0013 andX2=\u0012F2 F1\u0013 =\u00121 1\u0013 . MX 1=\u00121 1 1 0\u0013\u00121 0\u0013 =\u00121 1\u0013 =X2: (d) We just need to use the recursion relationship of part (b) in the top slot ofXn+1: Xn+1=\u0012Fn+1 Fn\u0013 =\u0012Fn+Fn\u00001 Fn\u0013 =\u00121 1 1 0\u0013\u0012Fn Fn\u00001\u0013 =MXn: (e) Notice Mis symmetric so this is guaranteed to work. det\u00121\u0000\u00151 1\u0000\u0015\u0013 =\u0015(\u0015\u00001)\u00001 =\u0000 \u0015\u00001 2\u00012\u00005 4; so the eigenvalues are1\u0006p 5\n",
      "399 (262) Hence the eigenvectors are 1\u0006p 5 2 1! , respectively (notice that1+p 5 2+ 1 =1+p 5 2:1+p 5 2and1\u0000p 5 2+ 1 = 1\u0000p 5 2:1\u0000p 5 2). ThusM=PDP\u00001with D= 1+p 5 20 01\u0000p 5 2! andP= 1+p 5 21\u0000p 5 2 1 1! : (f)Mn= (PDP\u00001)n=PDP\u00001PDP\u00001:::PDP\u00001=PDnP\u00001. 363364 Sample Final Exam (g) Just use the matrix recursion relation of part (d) repeatedly: Xn+1=MXn=M2Xn\u00001=\u0001\u0001\u0001=MnX1: (h) The eigenvalues are '=1+p 5 2and 1\u0000'=1\u0000p 5\n",
      "223 (263) (i) Xn+1=\u0012Fn+1 Fn\u0013 =MnXn=PDnP\u00001X1 =P\u0012' 0 0 1\u0000'\u0013n 1p 5? \u00001p 5?!\u00121 0\u0013 =P\u0012'n0 0 (1\u0000')n\u0013 1p 5 \u00001p 5! = 1+p 5 21\u0000p 5 2 1 1! 'n p 5 \u0000(1\u0000')n p 5! = ? 'n\u0000(1\u0000')n p 5! : Hence Fn='n\u0000(1\u0000')n p 5: These are the famous Fibonacci numbers.\n",
      "356 (264) Call the three vectors u;vandw, respectively. Then v?=v\u0000uv uuu=v\u00003 4u=0 BBB@1 4 \u00003 4 1 4 1 41 CCCA; and w?=w\u0000uw uuu\u0000v?w v?v?v?=w\u00003 4u\u00003 4 3 4v?=0 BB@\u00001 0 0 11 CCA Dividing by lengths, an orthonormal basis for span fu;v;wgis 8 >>>>>< >>>>>:0 BBBBB@1 2 1 2 1 2 1 21 CCCCCA;0 BBBBB@p 3 6 \u0000p 3 2p 3 6p 3 61 CCCCCA;0 BBBB@\u0000p 2 2 0 0 p 2 21 CCCCA9 >>>>>= >>>>>;:\n",
      "466 (265) (a) The columns of Mspan imLin the basis given for W. 364365 (b) The rows of Mspan (kerL)? (c) First we put Min RREF: M=0 BB@1 2 1 3 2 1\u00001 2 1 0 0\u00001 4 1\u00001 01 CCA\u00180 BB@1 2 1 3 0\u00003\u00003\u00004 0\u00002\u00001\u00004 0\u00007\u00005\u0000121 CCA \u00180 BBB@1 0\u000011 3 0 1 14 3 0 0 1\u00004 3 0 0 2\u00008 31 CCCA\u00180 BBB@1 0 0\u00001 0 1 08 3 0 0 1\u00004 3 0 0 0 01 CCCA: Hence kerL= spanfv1\u00008 3v2+4 3v3+v4g and imL= spanfv1+ 2v2+v3+ 4v4;2v1+v2+v4;v1\u0000v2\u0000v4g: Thus dim ker L= 1 and dim im L= 3 so dim kerL+ dim imL= 1 + 3 = 4 = dim V :\n",
      "555 (266) (a)8 >>< >>:5 = 4a\u00002b+c 2 =a\u0000b+c 0 =a+b+c 3 = 4a+ 2b+c: (b,c,d) 0 BB@4\u00002 1 5 1\u00001 1 2 1 1 1 0 4 2 1 31 CCA\u00180 BB@1 1 1 0 0\u00006\u000035 0\u00002 0 2 0\u00002\u0000331 CCA\u00180 BB@1 0 1\u00001 0 1 0 1 0 0\u0000311 0 0\u00003 31 CCA The system has no solutions because c=\u00001 andc=\u000011 3is impossible. (e) Let M=0 BB@4\u00002 1 1\u00001 1 1 1 1 4 2 11 CCAandV=0 BB@5 2 0 31 CCA: 365366 Sample Final Exam Then MTM=0 @34 0 10 0 10 0 10 0 41 AandMTV=0 @34 \u00006 101 A: So 0 @34 0 10 34 0 10 0\u00006 10 0 4 101 A\u00180 @1 02 51 0 10 0\u00006 0 0\u000018 501 A\u00180 @1 0 0 1 0 1 0\u00003 5 0 0 1 01 A The least squares solution is a= 1,b=\u00003 5andc=\n",
      "50 (267) (b) The Captain predicts y(2) = 1:22\u00003 5:2 + 0 =14\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "problems = []\n",
    "with open(\"data-augment/src/LinAlgTxt1.pdf\", \"rb\") as f:\n",
    "    reader = PdfReader(f)\n",
    "    pages = [page for page in reader.pages]\n",
    "\n",
    "    for rng in [(29,36),(47,51),(60,62),(67,70),(79,82),(96,100),(108,110),(117,120),(128,132),\n",
    "                (145,149),(154,158),(165,168),(181,186),(192,194),(201,202),(209,212),(220,224),\n",
    "                (237,240),(247,252),(271,276),(298,302),(311,314),(320,366)]:\n",
    "\n",
    "        txt = \"\"\n",
    "        for i in range(rng[0], rng[1]):\n",
    "            page = re.sub(\"\\s+\", \" \", pages[i].extract_text())\n",
    "            txt += page\n",
    "\n",
    "        problems += re.split(\"\\s\\d+\\.\\s+\", txt)[1:]\n",
    "\n",
    "out = []\n",
    "for i, problem in enumerate(problems):\n",
    "    if len(problem) < 50: continue\n",
    "    out.append(problem)\n",
    "    print(len(problem), f\"({i+1}) {problem}\")\n",
    "\n",
    "## export\n",
    "df = pd.DataFrame()\n",
    "df[\"Question\"] = out\n",
    "df[\"label\"] = 6\n",
    "df.to_csv(\"data-augment/out/linalg1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce97dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1807 (2) The angle is a symmetric function of aandb: We have6(a;b) =6(b;a). The angle is not a ected by scaling each of the vectors by a positive scalar: We have, for any vectors aandb, and any positive numbers and , 6( a; b ) =6(a;b): Acute and obtuse angles. Angles are classi ed according to the sign of aTb. Supposeaandbare nonzero vectors of the same size. \u000fIf the angle is \u0019=2 = 90\u000e,i.e.,aTb= 0, the vectors are said to be orthogonal . We writea?bifaandbare orthogonal. (By convention, we also say that a zero vector is orthogonal to any vector.) \u000fIf the angle is zero, which means aTb=kakkbk, the vectors are aligned . Each vector is a positive multiple of the other. \u000fIf the angle is \u0019= 180\u000e, which means aTb=\u0000kakkbk, the vectors are anti- aligned . Each vector is a negative multiple of the other. \u000fIf6(a;b)<\u0019= 2 = 90\u000e, the vectors are said to make an acute angle . This is the same as aTb>0,i.e., the vectors have positive inner product. \u000fIf6(a;b)>\u0019= 2 = 90\u000e, the vectors are said to make an obtuse angle . This is the same as aTb<0,i.e., the vectors have negative inner product. These de nitions are illustrated in gure 3.6. Examples. \u000fSpherical distance. Supposeaandbare 3-vectors that represent two points that lie on a sphere of radius R(for example, locations on earth). The spherical distance between them, measured along the sphere, is given by R6(a;b). This is illustrated in gure 3.7. \u000fDocument similarity via angles. Ifn-vectorsxandyrepresent the word counts for two documents, their angle 6(x;y) can be used as a measure of document dissimilarity. (When using angle to measure document dissimilar- ity, either word counts or histograms can be used; they produce the same result.) As an example, table 3.2 gives the angles in degrees between the word histograms in the example at the end of x3.2.\n",
      "840 (3) We conclude that the vectors e1;l1;:::;ln\u00001are linearly independent, and therefore a basis. This means that any cash ow n-vectorccan be expressed as a linear combi- nation of ( i.e., replicated by) an initial payment and one period loans: c= 1e1+ 2l1+\u0001\u0001\u0001+ nln\u00001: It is possible to work out what the coe\u000ecients are (see exercise 5.3). The most interesting one is the rst coe\u000ecient 1=c1+c2 1 +r+\u0001\u0001\u0001+cn (1 +r)n\u00001; which is exactly the net present value (NPV) of the cash ow, with interest rate r. Thus we see that anycash ow can be replicated as an income in period 1 equal to its net present value, plus a linear combination of one-period loans at interest rater. Proof of independence-dimension inequality. The proof is by induction on the dimensionn. First consider a linearly independent collection a1;:::;akof 1-vectors. We must have a16=\n",
      "163 (4) This means that every element aiof the collection can be expressed as a multiple ai= (ai=a1)a1of the rst element a1. This contradicts linear independence unless k=\n",
      "292 (5) Next suppose n\u00152 and the independence-dimension inequality holds for di- mensionn\u00001. Leta1;:::;akbe a linearly independent list of n-vectors. We need to show that k\u0014n. We partition the vectors as ai=\u0014bi i\u0015 ; i= 1;:::;k; wherebiis an (n\u00001)-vector and iis a scalar. First suppose that 1=\u0001\u0001\u0001= k=\n",
      "397 (6) Then the vectors b1;:::;bkare linearly independent:Pk i=1 ibi= 0 holds if and only ifPk i=1 iai= 0, which is only possible for 1=\u0001\u0001\u0001= k= 0 because the vectors aiare linearly independent. The vectors b1;:::;bktherefore form a linearly independent collection of ( n\u00001)- vectors. By the induction hypothesis we have k\u0014n\u00001, so certainly k\u0014n. Next suppose that the scalars iare not all zero. Assume j6=\n",
      "284 (7) We de ne a collection of k\u00001 vectorsciof lengthn\u00001 as follows: ci=bi\u0000 i jbj; i= 1;:::;j\u00001; ci=bi+1\u0000 i+1 jbj; i=j;:::;k\u00001: Thesek\u00001 vectors are linearly independent: IfPk\u00001 i=1 ici= 0 then j\u00001X i=1 i\u0014bi i\u0015 + \u0014bj j\u0015 +kX i=j+1 i\u00001\u0014bi i\u0015 = 0 (5.2) with =\u00001 j(j\u00001X i=1 i i+kX i=j+1 i\u00001 i):\n",
      "3603 (8) It is also common to describe this collection of vectors using the transpose of that matrix. In this case, we give the vectors as an N\u0002n matrixX. Itsith row isxT i, the transpose of the ith vector. Its jth column gives the value of the jth entry (or feature) across the collection of Nvectors. When an author refers to a data matrix orfeature matrix , it can usually be determined from context (for example, its dimensions) whether they mean the data matrix organized by rows or columns. Symmetric matrix. A square matrix Aissymmetric ifA=AT,i.e.,Aij=Aji for alli;j. Symmetric matrices arise in several applications. For example, suppose thatAis the adjacency matrix of a graph or relation (see page 112). The matrix A is symmetric when the relation is symmetric, i.e., whenever ( i;j)2R, we also have (j;i)2R. An example is the friend relation on a set ofnpeople, where ( i;j)2R means that person iand person jare friends. (In this case the associated graph is called the `social network graph'.) 6.3.2 Matrix addition Two matrices of the same size can be added together. The result is another matrix of the same size, obtained by adding the corresponding elements of the two matrices. For example,2 40 4 7 0 3 13 5+2 41 2 2 3 0 43 5=2 41 6 9 3 3 53 5: Matrix subtraction is similar. As an example, \u00141 6 9 3\u0015 \u0000I=\u00140 6 9 2\u0015 : (This gives another example where we have to gure out the size of the identity matrix. Since we can only add or subtract matrices of the same size, Irefers to a 2\u00022 identity matrix.) Properties of matrix addition. The following important properties of matrix ad- dition can be veri ed directly from the de nition. We assume here that A,B, and Care matrices of the same size.6.3 Transpose, addition, and norm 117 \u000fCommutativity. A+B=B+A. \u000fAssociativity. (A+B) +C=A+ (B+C). We therefore write both as A+B+C. \u000fAddition with zero matrix. A+ 0 = 0 +A=A. Adding the zero matrix to a matrix has no e ect. \u000fTranspose of sum. (A+B)T=AT+BT. The transpose of a sum of two matrices is the sum of their transposes. 6.3.3 Scalar-matrix multiplication Scalar multiplication of matrices is de ned in a similar way as for vectors, and is done by multiplying every element of the matrix by the scalar. For example (\u00002)2 41 6 9 3 6 03 5=2 4\u00002\u000012 \u000018\u00006 \u000012 03 5: As with scalar-vector multiplication, the scalar can also appear on the right. Note that 0A= 0 (where the left-hand zero is the scalar zero, and the right-hand 0 is the zero matrix). Several useful properties of scalar multiplication follow directly from the de ni- tion. For example, ( A)T= (AT) for a scalar and a matrix A. IfAis a matrix and , are scalars, then ( + )A= A+ A; ( )A= ( A): It is useful to identify the symbols appearing in these two equations. The + symbol on the left of the left-hand equation is addition of scalars, while the + symbol on the right of the left-hand equation denotes matrix addition. On the left side of the right-hand equation we see scalar-scalar multiplication ( ) and scalar-matrix multiplication; on the right we see two cases of scalar-matrix multiplication. Finally, we mention that scalar-matrix multiplication has higher precedence than matrix addition, which means that we should carry out multiplication before addition (when there are no parentheses to x the order). So the right-hand side of the left equation above is to be interpreted as ( A) + ( A). 6.3.4 Matrix norm The norm of an m\u0002nmatrixA, denotedkAk, is the squareroot of the sum of the squares of its entries, kAk=vuutmX i=1nX j=1A2 ij: (6.3)118 6 Matrices This agrees with our de nition for vectors when Ais a vector, i.e.,n=\n",
      "512 (9) The norm of an m\u0002nmatrix is the norm of an mn-vector formed from the entries of the matrix (in any order). Like the vector norm, the matrix norm is a quantitative measure of the magnitude of a matrix. In some applications it is more natural to use the RMS values of the matrix entries, kAk=pmn, as a measure of matrix size. The RMS value of the matrix entries tells us the typical size of the entries, independent of the matrix dimensions. The matrix norm (6.3) satis es the properties of any norm, given on page\n",
      "1586 (10) For anym\u0002nmatrixA, we havekAk\u00150 (i.e., the norm is nonnegative), and kAk= 0 only ifA= 0 (de niteness). The matrix norm is nonnegative homogeneous: For any scalar andm\u0002nmatrixA, we havek Ak=j jkAk. Finally, for any two m\u0002nmatricesAandB, we have the triangle inequality, kA+Bk\u0014kAk+kBk: (The plus symbol on the left-hand side is matrix addition, and the plus symbol on the right-hand side is addition of numbers.) The matrix norm allows us to de ne the distance between two matrices as kA\u0000Bk. As with vectors, we can say that one matrix is close to, or near, another one if their distance is small. (What quali es as small depends on the application.) In this book we will only use the matrix norm (6.3). Several other norms of a matrix are commonly used, but are beyond the scope of this book. In contexts where other norms of a matrix are used, the norm (6.3) is called the Frobenius norm , after the mathematician Ferdinand Georg Frobenius, and is usually denoted with a subscript, as kAkF. One simple property of the matrix norm is kAk=kATk,i.e., the norm of a matrix is the same as the norm of its transpose. Another one is kAk2=ka1k2+\u0001\u0001\u0001+kank2; wherea1;:::;anare the columns of A. In other words: The squared norm of a matrix is the sum of the squared norms of its columns. 6.4 Matrix-vector multiplication IfAis anm\u0002nmatrix and xis ann-vector, then the matrix-vector product y=Ax is them-vectorywith elements yi=nX k=1Aikxk=Ai1x1+\u0001\u0001\u0001+Ainxn; i= 1;:::;m: (6.4) As a simple example, we have \u00140 2\u00001 \u00002 1 1\u00152 42 1 \u000013 5=\u0014(0)(2) + (2)(1) + ( \u00001)(\u00001) (\u00002)(2) + (1)(1) + (1)( \u00001)\u0015 =\u00143 \u00004\u0015 :\n",
      "94 (11) For this source, the ow vector x= (0:6;0:3;0:6;\u00000:1;\u00000:3) satis es ow conservation, i.e.,Ax+s=\n",
      "281 (12) This ow can be explained in words: The unit external ow entering node 1 splits three ways, with 0 :6 owing up, 0:3 owing right, and 0 :1 owing diagonally up (on edge 4). The upward ow on edge 1 passes through node 2, where ow is conserved, and proceeds right on edge 3 towards node\n",
      "106 (13) The rightward ow on edge 2 passes through node 4, where ow is conserved, and proceeds up on edge 5 to node\n",
      "1892 (14) The one unit of excess ow arriving at node 3 is removed as external ow. Node potentials. A graph is also useful when we focus on the values of some quantity at each graph vertex or node. Let vbe ann-vector, often interpreted as a potential , withvithe potential value at node i. We can give a simple interpretation to the matrix-vector product u=ATv. Them-vectoru=ATvgives the potential di erences across the edges: uj=vl\u0000vk, where edge jgoes from node kto nodel. Dirichlet energy. When them-vectorATvis small, it means that the potential di erences across the edges are small. Another way to say this is that the potentials of connected vertices are near each other. A quantitative measure of this is the function of vgiven by D(v) =kATvk2: This function arises in many applications, and is called the Dirichlet energy (or Laplacian quadratic form ) associated with the graph. It can be expressed as D(v) =X edges (k;l)(vl\u0000vk)2; which is the sum of the squares of the potential di erences of vacross all edges in the graph. The Dirichlet energy is small when the potential di erences across the edges of the graph are small, i.e., nodes that are connected by edges have similar potential values. The Dirichlet energy is used as a measure the non-smoothness (roughness) of a set of node potentials on a graph. A set of node potentials with small Dirichlet energy can be thought of as smoothly varying across the graph. Conversely, a set of potentials with large Dirichlet energy can be thought of as non-smooth or rough. The Dirichlet energy will arise as a measure of roughness in several applications we will encounter later. As a simple example, consider the potential vector v= (1;\u00001;2;\u00001) for the graph shown in gure 7.2. For this set of potentials, the potential di erences across the edges are relatively large, with ATv= (\u00002;\u00002;3;\u00001;\u00003), and the associated Dirichlet energy is kATvk2=\n",
      "1227 (15) Now consider the potential vector v= (1;2;2;1). The associated edge potential di erences are ATv= (1;0;0;\u00001;\u00001), and the Dirichlet energy has the much smaller value kATvk2= 3.136 7 Matrix examples 1 2 3 n1 2 3 nâˆ’1 Figure 7.4 Chain graph. 0 20 40 60 80 1000123 kak 0 20 40 60 80 1000123 kbk Figure 7.5 Two vectors of length 100, with Dirichlet energy D(a) = 1:14 and D(b) = 8:99. Chain graph. The incidence matrix and the Dirichlet energy function have a particularly simple form for the chain graph shown in gure 7.4, with nvertices andn\u00001 edges. The n\u0002(n\u00001) incidence matrix is the transpose of the di erence matrixDdescribed on page 119, in (6.5). The Dirichlet energy is then D(v) =kDvk2= (v2\u0000v1)2+\u0001\u0001\u0001+ (vn\u0000vn\u00001)2; the sum of squares of the di erences between consecutive entries of the n-vectorv. This is used as a measure of the non-smoothness of the vector v, considered as a time series. Figure 7.5 shows an example. 7.4 Convolution The convolution of ann-vectoraand anm-vectorbis the (n+m\u00001)-vector denotedc=a\u0003b, with entries ck=X i+j=k+1aibj; k = 1;:::;n +m\u00001; (7.2) where the subscript in the sum means that we should sum over all values of iand jin their index ranges 1 ;:::;n and 1;:::;m , for which the sum i+jisk+\n",
      "6255 (17) In other words, superposition holds for a\u000ene combinations of vectors. (For linear functions, superposition holds for any linear combinations of vectors.) The matrix Aand the vector bin the representation of an a\u000ene function as f(x) =Ax+bare unique. These parameters can be obtained by evaluating fat the vectors 0, e1;:::;en, whereekis thekth unit vector in Rn. We have A=\u0002f(e1)\u0000f(0)f(e2)\u0000f(0)\u0001\u0001\u0001f(en)\u0000f(0)\u0003 ; b =f(0): Just like a\u000ene scalar-valued functions, a\u000ene vector-valued functions are often called linear, even though they are linear only when the vector bis zero. 8.2 Linear function models Many functions or relations between variables that arise in natural science, engi- neering, and social sciences can be approximated as linear or a\u000ene functions. In these cases we refer to the linear function relating the two sets of variables as a model or an approximation , to remind us that the relation is only an approximation, and not exact. We give a few examples here. \u000fPrice elasticity of demand. Considerngoods or services with prices given by then-vectorp, and demands for the goods given by the n-vectord. A change in prices will induce a change in demands. We let \u000epricebe then-vector that gives the fractional change in the prices, i.e.,\u000eprice i = (pnew i\u0000pi)=pi, wherepnewis then-vector of new (changed) prices. We let \u000edembe then- vector that gives the fractional change in the product demands, i.e.,\u000edem i= (dnew i\u0000di)=di, wherednewis then-vector of new demands. A linear demand elasticity model relates these vectors as \u000edem=Ed\u000eprice, whereEdis then\u0002n demand elasticity matrix . For example, suppose Ed 11=\u00000:4 andEd 21= 0:2. This means that a 1% increase in the price of the rst good, with other prices kept the same, will cause demand for the rst good to drop by 0 :4%, and demand for the second good to increase by 0 :2%. (In this example, the second good is acting as a partial substitute for the rst good.) \u000fElastic deformation. Consider a steel structure like a bridge or the structural frame of a building. Let fbe ann-vector that gives the forces applied to the structure at nspeci c places (and in nspeci c directions), sometimes called a loading . The structure will deform slightly due to the loading. Let dbe anm-vector that gives the displacements (in speci c directions) of m points in the structure, due to the load, e.g., the amount of sag at a speci c point on a bridge. For small displacements, the relation between displacement and loading is well approximated as linear: d=Cf, whereCis them\u0002n compliance matrix . The units of the entries of Care m/N.8.2 Linear function models 151 8.2.1 Taylor approximation Supposef:Rn!Rmis di erentiable, i.e., has partial derivatives, and zis an n-vector. The rst-order Taylor approximation of fnearzis given by ^f(x)i=fi(z) +@fi @x1(z)(x1\u0000z1) +\u0001\u0001\u0001+@fi @xn(z)(xn\u0000zn) =fi(z) +rfi(z)T(x\u0000z); fori= 1;:::;m . (This is just the rst-order Taylor approximation of each of the scalar-valued functions fi, described inx2.2.) Forxnearz,^f(x) is a very good approximation of f(x). We can express this approximation in compact notation, using matrix-vector multiplication, as ^f(x) =f(z) +Df(z)(x\u0000z); (8.3) where them\u0002nmatrixDf(z) is the derivative orJacobian matrix offatz (seexC.1). Its components are the partial derivatives of f, Df(z)ij=@fi @xj(z); i= 1;:::;m; j = 1;:::;n; evaluated at the point z. The rows of the Jacobian are rfi(z)T, fori= 1;:::;m . The Jacobian matrix is named for the mathematician Carl Gustav Jacob Jacobi. As in the scalar-valued case, Taylor approximation is sometimes written with a second argument as ^f(x;z) to show the point zaround which the approximation is made. Evidently the Taylor series approximation ^fis an a\u000ene function of x. (It is often called a linear approximation of f, even though it is not, in general, a linear function.) 8.2.2 Regression model Recall the regression model (2.7) ^y=xT +v; (8.4) where then-vectorxis a feature vector for some object, is ann-vector of weights, vis a constant (the o set), and ^ yis the (scalar) value of the regression model prediction. Now suppose we have a set of Nobjects (also called samples orexamples ), with feature vectors x(1);:::;x(N). The regression model predictions associated with the examples are given by ^y(i)= (x(i))T +v; i = 1;:::;N: These numbers usually correspond to predictions of the value of the outputs or responses. If in addition to the example feature vectors x(i)we are also given the152 8 Linear equations actual value of the associated response variables, y(1);:::;y(N), then our prediction errors orresiduals are r(i)=y(i)\u0000^y(i); i= 1;:::;N: (Some authors de ne the prediction errors as ^ y(i)\u0000y(i).) We can express this using compact matrix-vector notation. We form the n\u0002N feature matrix Xwith columns x(1);:::;x(N). We letyddenote theN-vector whose entries are the actual values of the response for the Nexamples. (The superscript `d' stands for `data'.) We let ^ yddenote theN-vector of regression model predictions for theNexamples, and we let rddenote the N-vector of residuals or prediction errors. We can then express the regression model predictions for this data set in matrix-vector form as ^yd=XT +v1: The vector of Nprediction errors for the examples is given by rd=yd\u0000^yd=yd\u0000XT \u0000v1: We can include the o set vin the regression model by including an additional feature equal to one as the rst entry of each feature vector: ^yd=\u0014 1T X\u0015T\u0014v \u0015 =~XT~ ; where ~Xis the new feature matrix, with a new rst row of ones, and ~ = (v; ) is the vector of regression model parameters. This is often written without the tildes, as ^yd=XT , by simply including the feature one as the rst feature. The equation above shows that the N-vector of predictions for the Nexamples is a linear function of the model parameters ( v; ). TheN-vector of prediction errors is an a\u000ene function of the model parameters. 8.3 Systems of linear equations Consider a set (also called a system) of mlinear equations in nvariables or un- knownsx1;:::;xn: A11x1+A12x2+\u0001\u0001\u0001+A1nxn=b1 A21x1+A22x2+\u0001\u0001\u0001+A2nxn=b2 ::: Am1x1+Am2x2+\u0001\u0001\u0001+Amnxn=bm: The numbers Aijare called the coe\u000ecients in the linear equations, and the numbers biare called the right-hand sides (since by tradition, they appear on the right-hand\n",
      "390 (18) (United States Census Bu- reau, census.gov). it a real number. In any case, our model certainly is not accurate at the level of individual people. Also, note that the model does not track people 100 and older. The distribution of ages in the US in 2010 is shown in gure 9.1. The birth rate is given by a 100-vector b, wherebiis the average number of births per person with age i\u00001,i= 1;:::;\n",
      "1022 (19) (This is half the average number of births per woman with age i\u00001, assuming equal numbers of men and women in the population.) Of course biis approximately zero for i<13 andi>50. The approximate birth rates for the US in 2010 are shown in gure 9.2. The death rate is given by a 100-vector d, wherediis the portion of those aged i\u00001 who will die this year. The death rates for the US in 2010 are shown in gure 9.3. To derive the dynamics equation (9.1), we nd xt+1in terms of xt, taking into account only births and deaths, and not immigration. The number of 0-year olds next year is the total number of births this year: (xt+1)1=bTxt: The number of i-year olds next year is the number of ( i\u00001)-year-olds this year, minus those who die: (xt+1)i+1= (1\u0000di)(xt)i; i= 1;:::; 99: We can assemble these equations into the time-invariant linear dynamical system xt+1=Axt; t= 1;2;:::; (9.4)166 9 Linear dynamical systems 010 20 30 40 50 60 70 80 90100024 AgeBirth rate (%) Figure 9.2 Approximate birth rate versus age in the US in\n",
      "234 (20) The gure is based on statistics for age groups of ve years (hence, the piecewise-constant shape) and assumes an equal number of men and women in each age group. (Martin J.A., Hamilton B.E., Ventura S.J. et al. , Births: Final data for\n",
      "163 (22) National Center for Health Statistics, 2012.) 010 20 30 40 50 60 70 80 901000102030 AgeDeath rate (%) Figure 9.3 Death rate versus age, for ages 0{99, in the US in\n",
      "100 (23) (Centers for Disease Control and Prevention, National Center for Health Statistics, wonder.cdc.gov.)\n",
      "229 (24) The property extends to higher powers of A. If`is a positive integer, then thei;jelement ofA`is the number of paths of length `from vertex jto vertexi. This can be proved by induction on `. We have already shown the result for `=\n",
      "690 (25) Assume that it is true that the elements of A`give the paths of length `between the di erent vertices. Consider the expression for the i;jelement ofA`+1: (A`+1)ij=nX k=1Aik(A`)kj: Thekth term in the sum is equal to the number of paths of length `fromjtokif there is an edge from ktoi, and is equal to zero otherwise. Therefore it is equal to the number of paths of length `+ 1 fromjtoithat end with the edge ( k;i), i.e., of the form ( j;:::;k;i ). By summing over all kwe obtain the total number of paths of length `+ 1 from vertex jtoi.188 10 Matrix multiplication 010 20 30 40 50 60 70 80 9010000.511.5 AgeFactor Figure 10.2 Contribution factor per age in 2010 to the total population in\n",
      "1165 (26) The value for age i\u00001 is theith component of the row vector 1TA10. This can be veri ed in the example. The third power of Ais A3=2 666641 1 1 1 2 2 0 2 3 1 2 1 1 2 2 1 0 1 1 0 0 1 0 0 13 77775: The (A3)24= 3 paths of length three from vertex 4 to vertex 2 are (4 ;3;3;2), (4;5;3;2), (4;5;1;2). Linear dynamical system. Consider a time-invariant linear dynamical system, described by xt+1=Axt. We have xt+2=Axt+1=A(Axt) =A2xt. Continuing this argument, we have xt+`=A`xt; for`= 1;2;:::. In a linear dynamical system, we can interpret A`as the matrix that propagates the state forward `time steps. For example, in a population dynamics model, A`is the matrix that maps the current population distribution into the population distribution `periods in the future, taking into account births, deaths, and the births and deaths of children, and so on. The total population `periods in the future is given by 1T(A`xt), which we can write as ( 1TA`)xt. The row vector 1TA`has an interesting interpretation: Itsith entry is the contribution to the total population in `periods due to each person with current age i\u00001. It is plotted in gure 10.2 for the US data given inx9.2.\n",
      "3622 (27) Compute QTb. 3.Back substitution. Solve the triangular equation Rx=QTbusing back substi- tution. The rst step requires 2 n3 ops (seex5.4), the second step requires 2 n2 ops, and the third step requires n2 ops. The total number of ops is then 2n3+ 3n2\u00192n3; so the order is n3, cubic in the number of variables, which is the same as the number of equations. In the complexity analysis above, we found that the rst step, the QR factor- ization, dominates the other two; that is, the cost of the other two is negligible in comparison to the cost of the rst step. This has some interesting practical implications, which we discuss below. Factor-solve methods. Algorithm 11.2 is similar to many methods for solving a set of linear equations and is sometimes referred to as a factor-solve scheme. A factor-solve scheme consists of two steps. In the rst (factor) step the coe\u000ecient matrix is factored as a product of matrices with special properties. In the second (solve) step one or more linear equations that involve the factors in the factorization are solved. (In algorithm 11.2, the solve step consists of steps 2 and 3.) The complexity of the solve step is smaller than the complexity of the factor step, and in many cases, it is negligible by comparison. This is the case in algorithm 11.2, where the factor step has order n3and the solve step has order n2. Factor-solve methods with multiple right-hand sides. Now suppose that we must solve several sets of linear equations, Ax1=b1; :::; Ax k=bk;11.3 Solving linear equations 209 all with the same coe\u000ecient matrix A, but di erent right-hand sides. We can express this as the matrix equation AX=B, whereXis then\u0002kmatrix with columnsx1;:::;xk, andBis then\u0002kmatrix with columns b1;:::;bk(see page 180). AssumingAis invertible, the solution of AX=BisX=A\u00001B. A na \u0010ve way to solve the kproblemsAxi=bi(or in matrix notation, compute X=A\u00001B) is to apply algorithm 11.2 ktimes, which costs 2 kn3 ops. A more e\u000ecient method exploits the fact that Ais the same matrix in each problem, so we can re-use the matrix factorization in step 1 and only need to repeat steps 2 and 3 to compute ^ xk=R\u00001QTbkforl= 1;:::;k . (This is sometimes referred to asfactorization caching , since we save or cache the factorization after carrying it out, for later use.) The cost of this method is 2 n3+ 3kn2 ops, or approximately 2n3 ops ifk n. The (surprising) conclusion is that we can solve multiple sets of linear equations, with the same coe\u000ecient matrix A, at essentially the same cost as solving oneset of linear equations. Backslash notation. In several software packages for manipulating matrices, Anb is taken to mean the solution of Ax=b,i.e.,A\u00001b, whenAis invertible. This backslash notation is extended to matrix right-hand sides: AnB, withBann\u0002k matrix, denotes A\u00001B, the solution of the matrix equation AX=B. (The compu- tation is implemented as described above, by factoring Ajust once, and carrying outkback substitutions.) This backslash notation is not standard mathematical notation, however, so we will not use it in this book. Computing the matrix inverse. We can now describe a method to compute the inverseB=A\u00001of an (invertible) n\u0002nmatrixA. We rst compute the QR factorization of A, soA\u00001=R\u00001QT. We can write this as RB=QT, which, written out by columns is Rbi= ~qi; i= 1;:::;n; wherebiis theith column of Band ~qiis theith column of QT. We can solve these equations using back substitution, to get the columns of the inverse B. Algorithm 11.3 Computing the inverse via QR factorization given ann\u0002ninvertible matrix A. 1.QR factorization. Compute the QR factorization A=QR.\n",
      "4238 (28) Fori= 1;:::;n , Solve the triangular equation Rbi= ~qiusing back substitution. The complexity of this method is 2 n3 ops (for the QR factorization) and n3for nback substitutions, each of which costs n2 ops. So we can compute the matrix inverse in around 3 n3 ops. This gives an alternative method for solving the square set of linear equations Ax=b: We rst compute the inverse matrix A\u00001, and then the matrix-vector productx= (A\u00001)b. This method has a higher op count than directly solving210 11 Matrix inverses the equations using algorithm 11.2 (3 n3versus 2n3), so algorithm 11.2 is the usual method of choice. While the matrix inverse appears in many formulas (such as the solution of a set of linear equations), it is computed far less often. Sparse linear equations. Systems of linear equations with sparse coe\u000ecient ma- trix arise in many applications. By exploiting the sparsity of the coe\u000ecient matrix, these linear equations can be solved far more e\u000eciently than by using the generic algorithm 11.2. One method is to use the same basic algorithm 11.2, replacing the QR factorization with a variant that handles sparse matrices (see page 190). The memory usage and complexity of these methods depends in a complicated way on the sparsity pattern of the coe\u000ecient matrix. In order, the memory usage is typically a modest multiple of nnz(A) +n, the number of scalars required to specify the problem data Aandb, which is typically much smaller than n2+n, the number of scalars required to store Aandbif they are not sparse. The op count for solving sparse linear equations is also typically closer in order to nnz(A) than n3, the order when the matrix Ais not sparse. 11.4 Examples Polynomial interpolation. The 4-vector cgives the coe\u000ecients of a cubic polyno- mial, p(x) =c1+c2x+c3x2+c4x3 (see pages 154 and 120). We seek the coe\u000ecients that satisfy p(\u00001:1) =b1; p (\u00000:4) =b2; p (0:2) =b3; p (0:8) =b4: We can express this as the system of 4 equations in 4 variables Ac=b, where A=2 6641\u00001:1 (\u00001:1)2(\u00001:1)3 1\u00000:4 (\u00000:4)2(\u00000:4)3 1 0:2 (0:2)2(0:2)3 1 0:8 (0:8)2(0:8)33 775; which is a speci c Vandermonde matrix (see (6.7)). The unique solution is c= A\u00001b, where A\u00001=2 664\u00000:5784 1:9841\u00002:1368 0:7310 0:3470 0:1984\u00001:4957 0:9503 0:1388\u00001:8651 1:6239 0:1023 \u00000:0370 0:3492 0:7521\u00000:06433 775 (to 4 decimal places). This is illustrated in gure 11.1, which shows the two cu- bic polynomials that interpolate the two sets of points shown as lled circles and squares, respectively. The columns of A\u00001are interesting: They give the coe\u000ecients of a polynomial that evaluates to 0 at three of the points, and 1 at the other point. For example, the11.4 Examples 211 âˆ’1.5âˆ’1âˆ’0.5 0 0.5 1xp(x) Figure 11.1 Cubic interpolants through two sets of points, shown as circles and squares. rst column of A\u00001, which isA\u00001e1, gives the coe\u000ecients of the polynomial that has value 1 at\u00001:1, and value 0 at \u00000:4, 0:2, and 0:8. The four polynomials with coe\u000ecients given by the columns of A\u00001are called the Lagrange polynomials asso- ciated with the points \u00001:1,\u00000:4, 0:2, 0:8. These are plotted in gure 11.2. (The Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange, whose name will re-appear in several other contexts.) The rows of A\u00001are also interesting: The ith row shows how the values b1, . . . ,b4, the polynomial values at the points \u00001:1,\u00000:4, 0:2, 0:8, map into the ith coe\u000ecient of the polynomial, ci. For example, we see that the coe\u000ecient c4is not very sensitive to the value of b1(since (A\u00001)41is small). We can also see that for each increase of one in b4, the coe\u000ecient c2increases by around 0 :95. Balancing chemical reactions. (See page 154 for background.) We consider the problem of balancing the chemical reaction a1Cr2O2\u0000 7+a2Fe2++a3H+\u0000!b1Cr3++b2Fe3++b3H2O; where the superscript gives the charge of each reactant and product. There are 4 atoms (Cr, O, Fe, H) and charge to balance. The reactant and product matrices are (using the order just listed) R=2 666642 0 0 7 0 0 0 1 0 0 0 1 \u00002 2 13 77775; P =2 666641 0 0 0 0 1 0 1 0 0 0 2 3 3 03 77775:212 11 Matrix inverses âˆ’1 0 101 xp(x) âˆ’1 0 101 xp(x) âˆ’1 0 101 xp(x) âˆ’1 0 101 xp(x) Figure 11.2 Lagrange polynomials associated with the points \u00001:1,\u00000:4, 0:2, 0:8.\n",
      "631 (29) Since Ahas linearly independent columns, we conclude that x\u0000^x= 0, i.e.,x= ^x. So the only xwithkAx\u0000bk2= kA^x\u0000bk2isx= ^x; for allx6= ^x, we havekAx\u0000bk2>kA^x\u0000bk2. Row form. The formula for the least squares approximate solution can be ex- pressed in a useful form in terms of the rows ~ aT iof the matrix A. ^x= (ATA)\u00001ATb= mX i=1~ai~aT i!\u00001 mX i=1bi~ai! : (12.8) In this formula we express the n\u0002nGram matrix ATAas a sum of mouter products, and the n-vectorATbas a sum of m n-vectors.12.3 Solving least squares problems 231 a1 a2Ë†rb AË†x Figure 12.2 Illustration of orthogonality principle for a least squares problem of sizem= 3,n=\n",
      "614 (30) The optimal residual ^ ris orthogonal to any linear combination of a1anda2, the two columns of A. Orthogonality principle. The pointA^xis the linear combination of the columns ofAthat is closest to b. The optimal residual is ^ r=A^x\u0000b. The optimal residual satis es a property that is sometimes called the orthogonality principle : It is orthogonal to the columns of A, and therefore, it is orthogonal to any linear combination of the columns of A. In other words, for any n-vectorz, we have (Az)?^r: (12.9) We can derive the orthogonality principle from the normal equations, which can be expressed as AT(A^x\u0000b) =\n",
      "168 (31) For any n-vectorz, we have (Az)T^r= (Az)T(A^x\u0000b) =zTAT(A^x\u0000b) = 0: The orthogonality principle is illustrated in gure 12.2, for a least squares prob- lem withm= 3 andn=\n",
      "1023 (32) The shaded plane is the set of all linear combinations z1a1+z2a2ofa1anda2, the two columns of A. The point A^xis the closest point in the plane to b. The optimal residual ^ ris shown as the vector from btoA^x. This vector is orthogonal to any point in the shaded plane. 12.3 Solving least squares problems We can use the QR factorization to compute the least squares approximate so- lution (12.5). Let A=QRbe the QR factorization of A(which exists by our assumption (12.2) that its columns are linearly independent). We have already seen that the pseudo-inverse Aycan be expressed as Ay=R\u00001QT, so we have ^x=R\u00001QTb: (12.10) To compute ^ xwe rst multiply bbyQT; then we compute R\u00001(QTb) using back substitution. This is summarized in the following algorithm, which computes the least squares approximate solution ^ x, givenAandb.232 12 Least squares Algorithm 12.1 Least squares via QR factorization given anm\u0002nmatrixAwith linearly independent columns and an m-vectorb. 1.QR factorization. Compute the QR factorization A=QR.\n",
      "7163 (33) Compute QTb. 3.Back substitution. Solve the triangular equation R^x=QTb. Comparison to solving a square system of linear equations. Recall that the solution of the square invertible system of linear equations Ax=bisx=A\u00001b. We can express xusing the QR factorization of Aasx=R\u00001QTb(see (11.4)). This equation is formally identical to (12.10). The only di erence is that in (12.10), A andQneed not be square, and R\u00001QTbis the least squares approximate solution, which is not (in general) a solution of Ax=b. Indeed, algorithm 12.1 is formally the same as algorithm 11.2, the QR fac- torization method for solving linear equations. (The only di erence is that in algorithm 12.1, AandQcan be tall.) WhenAis square, solving the linear equations Ax=band the least squares problem of minimizing kAx\u0000bk2are the same, and algorithm 11.2 and algo- rithm 12.1 are the same. So we can think of algorithm 12.1 as a generalization of algorithm 11.2, which solves the equation Ax=bwhenAis square, and computes the least squares approximate solution when Ais tall. Backslash notation. Several software packages for manipulating matrices extend the backslash operator (see page 209) to mean the least squares approximate solu- tion of an over-determined set of linear equations. In these packages Anbis taken to mean the solution A\u00001bofAx=bwhenAis square and invertible, and the least squares approximate solution AybwhenAis tall and has linearly indepen- dent columns. (We remind the reader that this backslash notation is not standard mathematical notation.) Complexity. The complexity of the rst step of algorithm 12.1 is 2 mn2 ops. The second step involves a matrix-vector multiplication, which takes 2 mn ops. The third step requires n2 ops. The total number of ops is 2mn2+ 2mn+n2\u00192mn2; neglecting the second and third terms, which are smaller than the rst by factors ofnand 2m, respectively. The order of the algorithm is mn2. The complexity is linear in the row dimension of Aand quadratic in the number of variables. Sparse least squares. Least squares problems with sparse Aarise in several appli- cations and can be solved more e\u000eciently, for example by using a QR factorization tailored for sparse matrices (see page 190) in the generic algorithm 12.1.12.3 Solving least squares problems 233 Another simple approach for exploiting sparsity of Ais to solve the normal equationsATA^x=ATbby solving a larger (but sparse) system of equations, \u0014 0AT A I\u0015\u0014^x ^y\u0015 =\u00140 b\u0015 : (12.11) This is a square set of m+nlinear equations. Its coe\u000ecient matrix is sparse when A is sparse. If (^ x;^y) satis es these equations, it is easy to see that ^ xsatis es (12.11); conversely, if ^ xsatis es the normal equations, (^ x;^y) satis es (12.11) with ^ y= b\u0000A^x. Any method for solving a sparse system of linear equations can be used to solve (12.11). Matrix least squares. A simple extension of the least squares problem is to choose then\u0002kmatrixXso as to minimize kAX\u0000Bk2. HereAis anm\u0002nmatrix and Bis anm\u0002kmatrix, and the norm is the matrix norm. This is sometimes called thematrix least squares problem . Whenk= 1,xandbare vectors, and the matrix least squares problem reduces to the usual least squares problem. The matrix least squares problem is in fact nothing but a set of kordinary least squares problems. To see this, we note that kAX\u0000Bk2=kAx1\u0000b1k2+\u0001\u0001\u0001+kAxk\u0000bkk2; wherexjis thejth column of Xandbjis thejth column of B. (Here we use the property that the square of the matrix norm is the sum of the squared norms of the columns of the matrix.) So the objective is a sum of kterms, with each term depending on only one column of X. It follows that we can choose the columns xj independently, each one by minimizing its associated term kAxj\u0000bjk2. Assuming thatAhas linearly independent columns, the solution is ^ xj=Aybj. The solution of the matrix least squares problem is therefore ^X=\u0002 ^x1\u0001\u0001\u0001 ^xk\u0003 =\u0002 Ayb1\u0001\u0001\u0001Aybk\u0003 =Ay\u0002b1\u0001\u0001\u0001bk\u0003 =AyB: (12.12) The very simple solution ^X=AyBof the matrix least squares problem agrees with the solution of the ordinary least squares problem when k= 1 (as it must). Many software packages for linear algebra use the backslash operator AnBto denoteAyB, but this is not standard mathematical notation. The matrix least squares problem can be solved e\u000eciently by exploiting the fact that algorithm 12.1 is another example of a factor-solve algorithm. To compute ^X=AyBwe carry out the QR factorization of Aonce; we carry out steps 2 and 3 of algorithm 12.1 for each of the kcolumns ofB. The total cost is 2 mn2+k(2mn+n2) ops. When kis small compared to nthis is roughly 2 mn2 ops, the same cost as solving a single least squares problem ( i.e., one with a vector right-hand side).234 12 Least squares 12.4 Examples Advertising purchases. We havemdemographic groups or audiences that we want to advertise to, with a target number of impressions or views for each group, which we give as a vector vdes. (The entries are positive.) To reach these audiences, we purchase advertising in ndi erent channels (say, di erent web publishers, radio, print, . . . ), in amounts that we give as an n-vectors. (The entries of sare non- negative, which we ignore.) The m\u0002nmatrixRgives the number of impressions in each group per dollar spending in the channels: Rijis the number of impres- sions in group iper dollar spent on advertising in channel j. (These entries are estimated, and are nonnegative.) The jth column of Rgives the e ectiveness or reach (in impressions per dollar) for channel j. Theith row ofRshows which media demographic group iis exposed to. The total number of impressions in each demographic group is the m-vectorv, which is given by v=Rs. The goal is to ndsso thatv=Rs\u0019vdes. We can do this using least squares, by choosing sto minimizekRs\u0000vdesk2. (We are not guaranteed that the resulting channel spend vector will be nonnegative.) This least squares formulation does not take into account the total cost of the advertising; we will see in chapter 16 how this can be done. We consider a simple numerical example, with n= 3 channels and m= 10 demographic groups, and matrix R=2 6666666666666640:97 1:86 0:41 1:23 2:18 0:53 0:80 1:24 0:62 1:29 0:98 0:51 1:10 1:23 0:69 0:67 0:34 0:54 0:87 0:26 0:62 1:10 0:16 0:48 1:92 0:22 0:71 1:29 0:12 0:623 777777777777775; with units of 1000 views per dollar. The entries of Rrange over an 18:1 range, so the 3 channels are quite di erent in terms of their audience reach; see gure 12.3. We takevdes= (103)1,i.e., our goal is to reach one million customers in each of the 10 demographic groups. Least squares gives the advertising budget allocation ^s= (62;100;1443); which achieves a views vector with RMS error 132, or 13 :2% of the target values. The views vector is shown in gure 12.4. Illumination. A set ofnlamps illuminates an area that we divide into mregions or pixels. We let lidenote the lighting level in region i, so them-vectorlgives the illumination levels across all regions. We let pidenote the power at which lamp ioperates, so the n-vectorpgives the set of lamp powers. (The lamp powers are nonnegative and also must not exceed a maximum allowed power, but we ignore these issues here.)\n",
      "615 (34) Another example is an original feature that takes on the values 1 ;2;:::; 7, representing the day of the week. Such features are called categorical in statistics, since they specify which category the example is in, and not some real number. Expanding a categorical feature with lvalues means replacing it with a set of l\u00001 new features, each of which is Boolean, and simply records whether or not the original feature has the associated value. (When all these features are zero, it means the original feature had the default value.) As an example, suppose the original feature x1takes on only the values \u00001, 0, and\n",
      "235 (35) Using the feature value 0 as the default feature value, we replace x1with the two mapped features f1(x) =\u001a1x1=\u00001 0 otherwise ;f2(x) =\u001a1x1= 1 0 otherwise : In words,f1(x) tells us ifx1has the value\u00001, andf2(x) tells us ifx1has the value\n",
      "1700 (36) (We do not need a new feature for the default value x1= 0; this corresponds to f1(x) =f2(x) = 0.) This feature mapping is shown in table 13.3. Expanding a categorical feature with lvalues into l\u00001 features that encode whether the feature has one of the (non-default) values is sometimes called one-hot encoding , because for any data example, only one of the new feature values is one, and the others are zero. (When the original feature has the default value, all the new features are zero.)13.3 Feature engineering 271 x1f1(x)f2(x) \u00001 1 0 0 0 0 1 0 1 Table 13.3 The original categorical feature x1takes on only the three values listed in the rst column. This feature is replaced with (expanded into) the two features f1(x) andf2(x) shown in the second and third columns. There is no need to expand an original feature that is Boolean ( i.e., takes on two values). If the original Boolean feature is encoded with the values 0 and 1, and 0 is taken as the default value, then the one new feature value will be the same as the original feature value. As an example of expanding categoricals, consider a model that is used to predict house prices based on various features that include the number of bedrooms, that ranges from 1 to 5 (say). In the basic regression model, we use the number of bedrooms directly as a feature. In the basic model there is one parameter value that corresponds to value per bedroom; we multiply this parameter by the number of bedrooms to get the contribution to our price prediction. In this model, the price prediction increases (or decreases) by the same amount when we change the number of bedrooms from 1 to 2 as it does when we change the number of bedrooms from 4 to\n",
      "868 (37) If we expand this categorical feature, using 2 bedrooms as the default, we have 4 Boolean features that correspond to a house having 1, 3, 4, and 5 bedrooms. We then have 4 parameters in our model, which assign di erent amounts to add to our prediction for houses with 1, 3, 4, and 5 bedrooms, respectively. This more exible model can capture the idea that a change from 1 to 2 bedrooms is di erent from a change from 4 to 5 bedrooms. Generalized additive model. We introduce new features that are nonlinear func- tions of the original features, such as, for each xi, the functions min fxi+a;0g and maxfxi\u0000b;0g, whereaandbare parameters. These new features are readily interpreted: minfxi+a;0gis the amount by which feature xiis below\u0000a, and maxfxi\u0000b;0gis the amount by which feature xiis aboveb. A common choice, assuming that xihas already been standardized, is a=b=\n",
      "1495 (38) This leads to the predictor ^y= 1(x1) +\u0001\u0001\u0001+ n(xn); (13.5) where iis the piecewise-linear function i(xi) =\u0012n+iminfxi+a;0g+\u0012ixi+\u00122n+imaxfxi\u0000b;0g; (13.6) which has kink or knot points at the values \u0000aand +b. The model (13.5) has 3 n parameters, corresponding to the original features, and the two additional features per original feature. The prediction ^ yis a sum of functions of the original features, and is called a generalized additive model . (More complex versions add more than two additional functions of each original feature.)272 13 Least squares data tting âˆ’1 1 âˆ’0.50.5 x1Ïˆ1(x1) âˆ’1 1 âˆ’0.50.5 x2Ïˆ2(x2) Figure 13.14 The functions iin (13.6) for n= 2,a=b= 1, and\u00121= 0:5, \u00122=\u00000:4,\u00123= 0:3,\u00124=\u00000:2,\u00125=\u00000:3,\u00126= 0:2. Consider an example with n= 2 original features. Our prediction ^ yis a sum of two piecewise-linear functions, each depending on one of the original features. Figure 13.14 shows an example. In this example model, we can say that increasing x1increases our prediction ^ y; but for high values of x1(i.e., above 1) the increase in the prediction is less pronounced, and for low values ( i.e., below\u00001), it is more pronounced. Products and interactions. New features can be developed from pairs of original features, for example, their product. From the original features we can add xixj, fori;j= 1;:::;n ,i\u0014j. Products are used to model interactions among the features. Product features are easily interpretable when the original features are Boolean, i.e., take the values 0 or\n",
      "5700 (39) Thus xi= 1 means that feature iis present or has occurred, and the new product feature xixjhas the value 1 exactly when both feature iandjhave occurred. Strati ed models. In a strati ed model , we have several di erent sub-models, and choose the one to use depending on the values of the regressors. For example, instead of treating gender as a regressor in a single model of some medical outcome, we build two di erent sub-models, one for male patients and one for female patients. In this case we choose the sub-model to use based on one of the original features, gender. As a more general example, we can carry out clustering of the original feature vectors, and t a separate model within each cluster. To evaluate ^ yfor a newx, we rst determine which cluster xis in, and then use the associated model. Whether or not a strati ed model is a good idea is checked using out-of-sample validation.13.3 Feature engineering 273 13.3.3 Advanced feature generation methods Custom mappings. In many applications custom mappings of the raw data are used as additional features, in addition to the original features given. For example in a model meant to predict an asset's future price using prior prices, we might also use the highest and lowest prices over the last week. Another well known example in nancial models is the price-to-earnings ratio, constructed from the price and (last) earnings features. In document analysis applications word count features are typically replaced with term frequency inverse document frequency (TFIDF) values, which scale the raw count values by a function of the frequency with which the word appears across the given set of documents, usually in such a way that uncommon words are given more weight. (There are many variations on the particular scaling function to use. Which one to use in a given application can be determined by out-of-sample or cross-validation.) Predictions from other models. In many applications there are existing models for the data. A common trick is to use the predictions of these models as features in your model. In this case you can describe your model as one that combines or blends the raw data available with predictions made from one or more existing models to create a new prediction. Distance to cluster representatives. We can build new features from a clustering of the data into kgroups. One simple method uses the cluster representatives z1;:::;zk, and gives knew features, given by f(x) =e\u0000kx\u0000zik2=\u001b2, where\u001bis a parameter. Random features. The new features are given by a nonlinear function of a random linear combination of the original features. To add Knew features of this type, we rst generate a random K\u0002nmatrixR. We then generate new features as (Rx)+orjRxj, where (\u0001)+andj\u0001jare applied elementwise to the vector Rx. (Other nonlinear functions can be used as well.) This approach to generating new features is quite counter-intuitive, since you would imagine that feature engineering should be done using detailed knowledge of, and intuition about, the particular application. Nevertheless this method can be very e ective in some applications. Neural network features. Aneural network computes transformed features using compositions of linear transformations interspersed with nonlinear mappings such as the absolute value. This architecture was originally inspired by biology, as a crude model of how human and animal brains work. The ideas behind neural networks are very old, but their use has accelerated over the last few years due to a combination of new techniques, greatly increased computing power, and access to large amounts of data. Neural networks can nd good feature mappings directly from the data, provided there is a very large amount of data available.274 13 Least squares data tting 13.3.4 Summary The discussion above makes it clear that there is much art in choosing features to use in a model. But it is important to keep several things in mind when creating new features: \u000fTry simple models rst. Start with a constant, then a simple regression model, and so on. You can compare more sophisticated models against these. \u000fCompare competing candidate models using validation. Adding new features will always reduce the RMS error on the training data, but the important question is whether or not it substantially reduces the RMS error on the test or validation data sets. (We add the quali er `substantially' here because a small reduction in test set error is not meaningful.) \u000fAdding new features can easily lead to over- t. (This will show up when validating the model.) The most straightforward way to avoid over- t is to keep the model simple. We mention here that another approach to avoiding over- t, called regularization (covered in chapter 15), can be very e ective when combined with feature engineering. 13.3.5 House price prediction In this section we use feature engineering to develop a more complicated regres- sion model for the house sales data, illustrating some of the methods described above. As mentioned in x2.3, the data set contains records of 774 house sales in the Sacramento area. For our more complex model we will use four base attributes or original features: \u000fx1is the area of the house (in 1000 square feet), \u000fx2is the number of bedrooms, \u000fx3is equal to one if the property is a condominium, and zero otherwise, \u000fx4is the ve-digit ZIP code. Only the rst two attributes were used in the simple regression model ^y= 1x1+ 2x2+v given inx2.3. In that model, we do not carry out any feature engineering or modi cation. Feature engineering. Here we examine a more complicated model, with 8 basis functions, ^y=8X i=1\u0012ifi(x): These basis functions are described below.\n",
      "739 (40) The density of the object we are imaging is shown in gure 15.9. In this object the density of each pixel is either 0 or 1 (shown as white or black, respectively). We reconstruct or estimate the object density from the 4000 (noisy) line integral measurements by solving the regularized least squares problem minimizekAx\u0000yk2+\u0015kDxk2; wherekDxk2is the sum of squares of the di erences of the pixel values from their neighbors. Figure 15.10 shows the results for six di erent values of \u0015. We can see that for small \u0015the reconstruction is relatively sharp, but su ers from noise. For large\u0015the noise in the reconstruction is smaller, but it is too smooth. 15.4 Regularized data tting We consider least squares data tting, as described in chapter\n",
      "3844 (41) In x13.2 we con- sidered the issue of over- tting, where the model performs poorly on new, unseen data, which occurs when the model is too complicated for the given data set. The remedy is to keep the model simple, e.g., by tting with a polynomial of not too high a degree. Regularization is another way to avoid over- tting, di erent from simply choos- ing a model that is simple ( i.e., does not have too many basis functions). Regular- ization is also called de-tuning ,shrinkage , or ridge regression , for reasons we will explain below.326 15 Multi-objective least squares Figure 15.8 The square region at the center of the picture is surrounded by 100 points shown as circles. 40 lines (beams) emanate from each point. (The lines are shown for two points only.) This gives a total of 4000 lines that intersect the region. . Figure 15.9 Density of object used in the tomography example.15.4 Regularized data tting 327 \u0015= 10\u00002\u0015= 10\u00001 \u0015= 1 \u0015= 5 \u0015= 10 \u0015= 100 Figure 15.10 Regularized least squares reconstruction for six values of the regularization parameter.328 15 Multi-objective least squares Motivation. To motivate regularization, consider the model ^f(x) =\u00121f1(x) +\u0001\u0001\u0001+\u0012pfp(x): (15.6) We can interpret \u0012ias the amount by which our prediction depends on fi(x), so if\u0012iis large, our prediction will be very sensitive to changes or variations in the value offi(x), such as those we might expect in new, unseen data. This suggests that we should prefer that \u0012ibe small, so our model is not too sensitive. There is one exception here: if fi(x) is constant (for example, the number one), then we should not worry about the size of \u0012i, sincefi(x) never varies. But we would like all the others to be small, if possible. This suggests the bi-criterion least squares problem with primary objective ky\u0000A\u0012k2, the sum of squares of the prediction errors, and secondary objective k\u00122:pk2, assuming that f1is the constant function one. Thus we should minimize ky\u0000A\u0012k2+\u0015k\u00122:pk2; (15.7) where\u0015>0 is called the regularization parameter . For the regression model, this weighted objective can be expressed as ky\u0000XT \u0000v1k2+\u0015k k2: Here we penalize being large (because this leads to sensitivity of the model), but not the o set v. Choosing to minimize this weighted objective is called ridge regression . E ect of regularization. The e ect of the regularization is to accept a worse value of sum square t ( ky\u0000A\u0012k2) in return for a smaller value of k\u00122:pk2, which measures the size of the parameters (except \u00121, which is associated with the constant basis function). This explains the name shrinkage: The parameters are smaller than they would be without regularization, i.e., they are shrunk. The term de-tuned suggests that with regularization, the model is not excessively `tuned' to the training data (which would lead to over- t). Regularization path. We get a di erent model for every choice of \u0015. The way the parameters change with \u0015is called the regularization path . Whenpis small enough (say, less than 15 or so) the parameter values can be plotted, with \u0015on the horizontal axis. Usually only 30 or 50 values of \u0015are considered, typically spaced logarithmically over a large range (see page 314). An appropriate value of \u0015can be chosen via out-of-sample or cross-validation. As\u0015increases, the RMS t on the training data worsens (increases). But (as with model order) the test set RMS prediction error typically decreases as \u0015increases, and then, when \u0015gets too big, it increases. A good choice of regularization pa- rameter is one which approximately minimizes the test set RMS prediction error. When multiple values of \u0015approximately minimize the RMS error, common prac- tice is to take the largest value of \u0015. The idea here is to use the model of minimum sensitivity, as measured by k k2, among those that make good predictions on the test set.\n",
      "84 (42) After that it applies zero force, so the mass stays where it is, at rest at position\n",
      "4811 (43) The superscript `bb' refers to bang-bang , which means that a large force is applied to get the mass moving (the rst `bang') and another large force (the second `bang') is then applied to slow it to zero velocity. The force and position versus time for this choice of fare shown in gure 16.3. Now we ask, what is the smallest force sequence that can achieve v n= 0, p n= 1, where smallest is measured by the sum of squares of the applied forces, kfk2=f2 1+\u0001\u0001\u0001+f2 10? This problem can be posed as a least norm problem, minimizekfk2 subject to\u0014 1 1\u0001\u0001\u0001 1 1 19=2 17=2\u0001\u0001\u0001 3=2 1=2\u0015 f=\u0014 0 1\u0015 ; with variable f. The solution fln, and the resulting position, are shown in g- ure 16.4. The norm square of the least norm solution flnis 0.0121; in contrast, the norm square of the bang-bang force sequence is 2, a factor of 165 times larger. (Note the very di erent vertical axis scales in gures 16.4 and 16.3.)344 16 Constrained least squares 0 2 4 6 8 10âˆ’0.0500.05 TimeForce 0 2 4 6 8 1000.51 TimePosition Figure 16.4 Left: The smallest force sequence flnthat transfers the mass over a unit distance in 10 steps. Right: The resulting position of the mass p(t). 16.2 Solution Optimality conditions via Lagrange multipliers. We will use the method of La- grange multipliers (developed by the mathematician Joseph-Louis Lagrange, and summarized inxC.3) to solve the constrained least squares problem (16.1). Later we give an independent veri cation, that does not rely on calculus or Lagrange multipliers, that the solution we derive is correct. We rst write the constrained least squares problem with the constraints given as a list ofpscalar equality constraints: minimizekAx\u0000bk2 subject to cT ix=di; i= 1;:::;p; wherecT iare the rows of C. We form the Lagrangian function L(x;z) =kAx\u0000bk2+z1(cT 1x\u0000d1) +\u0001\u0001\u0001+zp(cT px\u0000dp); wherezis thep-vector of Lagrange multipliers . The method of Lagrange multipliers tells us that if ^ xis a solution of the constrained least squares problem, then there is a set of Lagrange multipliers ^ zthat satisfy @L @xi(^x;^z) = 0; i= 1;:::;n;@L @zi(^x;^z) = 0; i= 1;:::;p: (16.3) These are the optimality conditions for the constrained least squares problem. Any solution of the constrained least squares problem must satisfy them. We will now see that the optimality conditions can be expressed as a set of linear equations. The second set of equations in the optimality conditions can be written as @L @zi(^x;^z) =cT i^x\u0000di= 0; i= 1;:::;p;16.2 Solution 345 which states that ^ xsatis es the equality constraints C^x=d(which we already knew). The rst set of equations, however, is more informative. Expanding the objectivekAx\u0000bk2as a sum of terms involving the entries of x(as was done on page 229) and taking the partial derivative of Lwith respect to xiwe obtain @L @xi(^x;^z) = 2nX j=1(ATA)ij^xj\u00002(ATb)i+pX j=1^zj(cj)i= 0: These equations can be written in compact matrix-vector form as 2(ATA)^x\u00002ATb+CT^z= 0: Combining this set of linear equations with the feasibility conditions C^x=d, we can write the optimality conditions (16.3) as one set of n+plinear equations in the variables (^ x;^z): \u0014 2ATA CT C 0\u0015\u0014^x ^z\u0015 =\u0014 2ATb d\u0015 : (16.4) These equations are called the KKT equations for the constrained least squares problem. (KKT are the initials of the last names of William Karush, Harold Kuhn, and Albert Tucker, the three researchers who derived the optimality con- ditions for a more general form of constrained optimization problem.) The KKT equations (16.4) are an extension of the normal equations (12.4) for a least squares problem with no constraints. So we have reduced the constrained least squares problem to the problem of solving a (square) set of n+plinear equations in n+p variables (^x;^z). Invertibility of KKT matrix. The (n+p)\u0002(n+p) coe\u000ecient matrix in (16.4) is called the KKT matrix . It is invertible if and only if Chas linearly independent rows, and\u0014A C\u0015 has linearly independent columns . (16.5) The rst condition requires that Cis wide (or square), i.e., that there are fewer constraints than variables. The second condition depends on both AandC, and it can be satis ed even when the columns of Aare linearly dependent. The con- dition (16.5) is the generalization of our assumption (12.2) for unconstrained least squares ( i.e., thatAhas linearly independent columns). Before proceeding, let us verify that the KKT matrix is invertible if and only if (16.5) holds. First suppose that the KKT matrix is not invertible. This means that there is a nonzero vector (\u0016 x;\u0016z) with \u0014 2ATA CT C 0\u0015\u0014 \u0016x \u0016z\u0015 = 0: Multiply the top block equation 2 ATA\u0016x+CT\u0016z= 0 on the left by \u0016 xTto get 2kA\u0016xk2+ \u0016xTCT\u0016z= 0:346 16 Constrained least squares The second block equation, C\u0016x= 0, implies (by taking the transpose) \u0016 xTCT= 0, so the equation above becomes 2 kA\u0016xk2= 0, i.e.,A\u0016x=\n",
      "137 (44) We also have C\u0016x= 0, so \u0014A C\u0015 \u0016x= 0: Since the matrix on the left has linearly independent columns (by assumption), we conclude that \u0016 x=\n",
      "81 (46) But by our assumption that the columns of CTare linearly independent, we have \u0016z=\n",
      "170 (47) So (\u0016x;\u0016z) = 0, which is a contradiction. The converse is also true. First suppose that the rows of Care linearly depen- dent. Then there is a nonzero vector \u0016 zwithCT\u0016z=\n",
      "1209 (48) Then \u0014 2ATA CT C 0\u0015\u00140 \u0016z\u0015 = 0; which shows the KKT matrix is not invertible. Now suppose that the stacked matrix in (16.5) has linearly dependent columns, which means there is a nonzero vector \u0016xfor which \u0014A C\u0015 \u0016x= 0: Direct calculation shows that \u0014 2ATA CT C 0\u0015\u0014 \u0016x 0\u0015 = 0; which shows that the KKT matrix is not invertible. When the conditions (16.5) hold, the constrained least squares problem (16.1) has the (unique) solution ^ x, given by \u0014^x ^z\u0015 =\u0014 2ATA CT C 0\u0015\u00001\u0014 2ATb d\u0015 : (16.6) (This formula also gives us ^ z, the set of Lagrange multipliers.) From (16.6), we observe that the solution ^ xis a linear function of ( b;d). Direct veri cation of constrained least squares solution. We will now show directly, without using calculus, that the solution ^ xgiven in (16.6) is the unique vector that minimizes kAx\u0000bk2over allxthat satisfy the constraints Cx=d, when the conditions (16.5) hold. Let ^ xand ^zdenote the vectors given in (16.6), so they satisfy 2ATA^x+CT^z= 2ATb; C ^x=d: Suppose that x6= ^xis any vector that satis es Cx=d. We will show that kAx\u0000bk2>kA^x\u0000bk2. We proceed in the same way as for the least squares problem: kAx\u0000bk2=k(Ax\u0000A^x) + (A^x\u0000b)k2 =kAx\u0000A^xk2+kA^x\u0000bk2+ 2(Ax\u0000A^x)T(A^x\u0000b):\n",
      "870 (49) We rst consider the Boolean problem of recognizing the digit zero. We use linear features, i.e., ~f(x) =xT +v; wherexis the 493-vector of pixel intensities. To determine the parameters vand we solve the nonlinear least squares problem minimizeNX i=1( ((x(i))T +v)\u0000y(i))2+\u0015k k2; (18.18) where is the sigmoid function (18.16) and \u0015is a positive regularization parameter. (This\u0015is the regularization parameter in the classi cation problem; it has no relation to the trust parameter \u0015(k)in the iterates of the Levenberg{Marquardt algorithm.) Figure 18.17 shows the classi cation error on the training and test sets as a function of the regularization parameter \u0015. For\u0015= 100, the classi cation errors on the training and test sets are about 0 :7%. This is less than half the 1 :6% error of the Boolean least squares classi er that used the same features, discussed in chapter\n",
      "1281 (50) This improvement in performance, by more than a factor of two, comes from minimizing an objective that is closer to what we want ( i.e., the number of prediction errors on the training set) than the surrogate linear least squares objective. The confusion matrices for the training set and test set are given in table 18.1. Figure 18.18 shows the distribution of the values of ~f(x(i)) for the two classes of the data set.18.5 Nonlinear least squares classi cation 405 Prediction Outcome ^ y= +1 ^y=\u00001 Total y= +1 5627 296 5923 y=\u00001 148 53929 54077 All 5775 54225 60000Prediction Outcome ^ y= +1 ^y=\u00001 Total y= +1 945 35 980 y=\u00001 40 8980 9020 All 985 9015 10000 Table 18.1 Confusion matrices for a Boolean classi er to recognize the digit zero. The table on the left is for the training set. The table on the right is for the test set. âˆ’10 âˆ’5 0 500.020.040.060.08 Ëœf(x(i))FractionPositive Negative Figure 18.18 The distribution of the values of ~f(x(i)) used in the Boolean classi er (14.1) for recognizing the digit zero. The function ~fwas computed by solving the nonlinear least squares problem (18.17).406 18 Nonlinear least squares 0 2 4 6 8 10100101102 IterationClassification error (%)Train Test Figure 18.19 Training and test error versus Levenberg{Marquardt iteration for\u0015=\n",
      "1536 (51) Convergence of Levenberg{Marquardt algorithm. The Levenberg{Marquardt algorithm is used to compute the parameters in the nonlinear least squares classi er. In this example the algorithm takes several tens of iterations to converge, i.e., until the stopping criterion for the nonlinear least squares problem is satis ed. But in this application we are more interested in the performance of the classi er, and not minimizing the objective of the nonlinear least squares problem. Figure 18.19 shows the classi cation error of the classi er (on the training and test data sets) with parameter \u0012(k), thekth iterate of the Levenberg{Marquardt algorithm. We can see that the classi cation errors reach their nal values of 0 :7% after just a few iterations. This phenomenon is very typical in nonlinear data tting problems. Well before convergence, the Levenberg{Marquardt algorithm nds model parameters that are just as good (as judged by test error) as the parameters obtained when the algorithm converges. Feature engineering. After adding the 5000 random features used in chapter 14, we obtain the training and test classi cation errors shown in gure 18.20. The error on the training set is zero for small \u0015. For\u0015= 1000, the error on the test set is 0:24%, with the confusion matrix in table 18.2. The distribution of ~f(x(i)) on the training set in gure 18.21 shows why the training error is zero. Figure 18.22 shows the classi cation errors versus Levenberg{Marquardt iter- ation, if we start the Levenberg{Marquardt algorithm with = 0,v=\n",
      "179 (52) (This implies that the values computed in the rst iteration are the coe\u000ecients of the linear least squares classi er.) The error on the training set is exactly zero at itera- tion\n",
      "1011 (53) The error on the test set is almost equal to its nal value after one iteration.18.5 Nonlinear least squares classi cation 407 10âˆ’210010210410600.20.40.6 Î»Classification error (%)Train Test Figure 18.20 Boolean classi cation error in percent versus \u0015, after adding 5000 random features. Prediction Outcome ^ y= +1 ^y=\u00001 Total y= +1 967 13 980 y=\u00001 11 9009 9020 All 978 9022 10000 Table 18.2 Confusion matrix on the test set for the Boolean classi er to recognize the digit zero after addition of 5000 new features.408 18 Nonlinear least squares âˆ’10 âˆ’5 0 500.020.040.06 Ëœf(x(i))FractionPositive Negative Figure 18.21 The distribution of the values of ~f(x(i)) used in the Boolean classi er (14.1) for recognizing the digit zero, after addition of 5000 new features. The function ~fwas computed by solving the nonlinear least squares problem (18.17). 0 2 4 6 8 1010âˆ’310âˆ’210âˆ’1100101102 IterationClassification error (%)Train Test Figure 18.22 Training and test error versus Levenberg{Marquardt iteration for\u0015= 1000.\n"
     ]
    }
   ],
   "source": [
    "## linear algebra text #2\n",
    "\n",
    "problems = []\n",
    "with open(\"data-augment/src/LinAlgTxt2.pdf\", \"rb\") as f:\n",
    "    reader = PdfReader(f)\n",
    "    pages = [page for page in reader.pages]\n",
    "\n",
    "    for rng in [(25, 28), (42, 44), (64, 68), (87, 88), (103, 104), (124, 128), (144, 146), (159, 162),\n",
    "                (174, 176), (191, 198), (217, 222), (239, 244), (279, 284), (305, 308), (334, 338), (352, 356),\n",
    "                (378, 380), (412, 418), (434, 436)]:\n",
    "\n",
    "        txt = \"\"\n",
    "        for i in range(rng[0], rng[1]):\n",
    "            page = re.sub(\"\\s+\", \" \", pages[i].extract_text())\n",
    "            txt += page\n",
    "\n",
    "        problems += re.split(\"\\s\\d+\\.\\s+\", txt)[1:]\n",
    "\n",
    "out = []\n",
    "for i, problem in enumerate(problems):\n",
    "    if len(problem) < 50: continue\n",
    "    out.append(problem)\n",
    "    print(len(problem), f\"({i+1}) {problem}\")\n",
    "\n",
    "## export\n",
    "df = pd.DataFrame()\n",
    "df[\"Question\"] = out\n",
    "df[\"label\"] = 6\n",
    "df.to_csv(\"data-augment/out/linalg2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
